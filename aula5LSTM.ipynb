{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1 - preparar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fazer o download da sequencias do valor das ações da google GOOGL stock data (fonte yahoo.com)\n",
    "dataset:\n",
    "http://chart.finance.yahoo.com/table.csv?s=GOOGL&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv\n",
    "A função get_stock_data é generica para ir buscar dados à yahoo.com\n",
    "trata-se uma tabela com: ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "Vamos śo utilizar os campos ['Open','High','Close']\n",
    "'''\n",
    "def get_stock_data(stock_name, normalized=0,file_name=None):\n",
    "    if not file_name:\n",
    "        file_name = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "    stocks = pd.read_csv(file_name, header=0, names=col_names) #fica numa especie de tabela exactamente como estava no csv (1350 linhas,7 colunas)\n",
    "    df = pd.DataFrame(stocks) #neste caso não vai fazer nada\n",
    "    date_split = df['Date'].str.split('-').str #não vai servir para nada\n",
    "    df['Year'], df['Month'], df['Day'] = date_split #não vai servir para nada\n",
    "    df[\"Volume\"] = df[\"Volume\"] / 10000 #não vai servir para nada\n",
    "    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) #vou só ficar com as colunas 1,2,4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GOOGL_stock_dataset():\n",
    "    stock_name = 'GOOGL'\n",
    "    return get_stock_data(stock_name, 0, 'table.csv')\n",
    "\n",
    "def pre_processar_GOOGL_stock_dataset(df):\n",
    "    df['High'] = df['High'] / 100\n",
    "    df['Open'] = df['Open'] / 100\n",
    "    df['Close'] = df['Close'] / 100\n",
    "    return df\n",
    "\n",
    "# Visualizar os top registos da tabela\n",
    "def visualize_GOOGL():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    print('### Antes do pré-processamento ###')\n",
    "    print(df.head()) #mostra só os primeiros 5 registos\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print('### Após o pré-processamento ###')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - tam_sequencia): #numero de registos - tamanho da sequencia\n",
    "        res.append(mat_dados[i: i + tam_sequencia])\n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes (janela deslizante ao longo da serie)\n",
    "    qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    train = res[:qt_casos_treino, :]\n",
    "    x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2 - Definir a topologia da rede (arquitectura do modelo) e compilar '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(janela):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(janela, 3), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(janela, 3), return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]- predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i],predic[i], diff[i], racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_utilizando_GOOGL_data():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    #model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    #print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em \n",
    "    #um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    ''' \n",
    "    MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "    o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Antes do pré-processamento ###\n",
      "         Open        High       Close\n",
      "0  929.000000  935.900024  924.520020\n",
      "1  890.000000  893.380005  891.440002\n",
      "2  891.390015  892.989990  889.140015\n",
      "3  882.260010  892.250000  888.840027\n",
      "4  868.440002  879.960022  878.929993\n",
      "### Após o pré-processamento ###\n",
      "     Open    High   Close\n",
      "0  9.2900  9.3590  9.2452\n",
      "1  8.9000  8.9338  8.9144\n",
      "2  8.9139  8.9299  8.8914\n",
      "3  8.8226  8.9225  8.8884\n",
      "4  8.6844  8.7996  8.7893\n",
      "df (1350, 3)\n",
      "X_train (1194, 22, 3)\n",
      "y_train (1194,)\n",
      "X_test (133, 22, 3)\n",
      "y_test (133,)\n",
      "Train on 1074 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "1074/1074 [==============================] - 2s 2ms/step - loss: 54.5862 - acc: 0.0000e+00 - val_loss: 57.5806 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 54.1077 - acc: 0.0000e+00 - val_loss: 56.8931 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 53.4246 - acc: 0.0000e+00 - val_loss: 55.9843 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 52.5148 - acc: 0.0000e+00 - val_loss: 54.7946 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 51.3353 - acc: 0.0000e+00 - val_loss: 53.2089 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 49.7616 - acc: 0.0000e+00 - val_loss: 51.1320 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 47.7345 - acc: 0.0000e+00 - val_loss: 48.6266 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1074/1074 [==============================] - 0s 218us/step - loss: 45.3466 - acc: 0.0000e+00 - val_loss: 45.9199 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 42.7738 - acc: 0.0000e+00 - val_loss: 43.0800 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 40.0657 - acc: 0.0000e+00 - val_loss: 39.9972 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 37.1440 - acc: 0.0000e+00 - val_loss: 36.7110 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 34.0917 - acc: 0.0000e+00 - val_loss: 33.4331 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 31.0237 - acc: 0.0000e+00 - val_loss: 30.0678 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1074/1074 [==============================] - 0s 187us/step - loss: 27.8965 - acc: 0.0000e+00 - val_loss: 26.6381 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1074/1074 [==============================] - 0s 186us/step - loss: 24.7474 - acc: 0.0000e+00 - val_loss: 23.2048 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 21.6226 - acc: 0.0000e+00 - val_loss: 19.8326 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 18.6046 - acc: 0.0000e+00 - val_loss: 16.5861 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 15.7189 - acc: 0.0000e+00 - val_loss: 13.5515 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 13.0630 - acc: 0.0000e+00 - val_loss: 10.7905 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 10.7044 - acc: 0.0000e+00 - val_loss: 8.3367 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 8.6650 - acc: 0.0000e+00 - val_loss: 6.2186 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 6.9499 - acc: 0.0000e+00 - val_loss: 4.4752 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 5.6081 - acc: 0.0000e+00 - val_loss: 3.0926 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 4.5873 - acc: 0.0000e+00 - val_loss: 2.0403 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.8878 - acc: 0.0000e+00 - val_loss: 1.2903 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.4518 - acc: 0.0000e+00 - val_loss: 0.7893 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.2134 - acc: 0.0000e+00 - val_loss: 0.4813 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1180 - acc: 0.0000e+00 - val_loss: 0.3098 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1180 - acc: 0.0000e+00 - val_loss: 0.2239 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1429 - acc: 0.0000e+00 - val_loss: 0.1881 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1826 - acc: 0.0000e+00 - val_loss: 0.1758 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.2020 - acc: 0.0000e+00 - val_loss: 0.1750 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.2003 - acc: 0.0000e+00 - val_loss: 0.1807 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1864 - acc: 0.0000e+00 - val_loss: 0.1910 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1693 - acc: 0.0000e+00 - val_loss: 0.2052 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1519 - acc: 0.0000e+00 - val_loss: 0.2251 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1344 - acc: 0.0000e+00 - val_loss: 0.2515 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1219 - acc: 0.0000e+00 - val_loss: 0.2811 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1139 - acc: 0.0000e+00 - val_loss: 0.3106 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3414 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3701 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3860 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3973 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1074/1074 [==============================] - 0s 216us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4064 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.4062 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1074/1074 [==============================] - 0s 216us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3958 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3743 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3574 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3469 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3387 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3305 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3230 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.2896 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1127 - acc: 0.0000e+00 - val_loss: 0.2729 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1170 - acc: 0.0000e+00 - val_loss: 0.2633 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1203 - acc: 0.0000e+00 - val_loss: 0.2586 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1215 - acc: 0.0000e+00 - val_loss: 0.2608 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1204 - acc: 0.0000e+00 - val_loss: 0.2681 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2751 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1163 - acc: 0.0000e+00 - val_loss: 0.2780 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1154 - acc: 0.0000e+00 - val_loss: 0.2873 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3236 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "1074/1074 [==============================] - 0s 216us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3398 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3524 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3515 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "1074/1074 [==============================] - 0s 215us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3576 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3609 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3578 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3514 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3345 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3215 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3131 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3017 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2963 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.2984 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3225 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3423 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3564 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3652 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "1074/1074 [==============================] - 0s 218us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3746 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3842 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3913 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3921 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3863 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3797 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3625 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3495 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3526 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3579 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3694 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3770 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3744 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3586 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "1074/1074 [==============================] - 0s 260us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3238 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "1074/1074 [==============================] - 0s 266us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3308 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 0s 261us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3409 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "1074/1074 [==============================] - 0s 258us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "1074/1074 [==============================] - 0s 260us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3391 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "1074/1074 [==============================] - 0s 265us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "1074/1074 [==============================] - 0s 259us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3173 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "1074/1074 [==============================] - 0s 263us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3311 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "1074/1074 [==============================] - 0s 258us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3369 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "1074/1074 [==============================] - 0s 258us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3388 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "1074/1074 [==============================] - 0s 256us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3413 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3369 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3527 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3608 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3686 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3707 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3675 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3406 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3241 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3141 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3103 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3224 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3310 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3368 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3331 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3165 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3067 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3092 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3197 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3326 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3330 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3217 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3155 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3223 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3303 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3463 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3551 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3492 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3633 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3533 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3420 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3222 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.2955 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.2832 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2849 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.3004 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3223 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3522 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3759 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3849 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3720 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3450 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3218 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3201 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3310 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3436 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3483 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3541 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3658 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3583 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3360 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3420 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3539 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3655 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3692 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3505 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1078 - acc: 0.0000e+00 - val_loss: 0.3118 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2917 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.2988 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3203 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3469 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3645 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "1074/1074 [==============================] - 0s 250us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3505 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3766 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "1074/1074 [==============================] - 0s 245us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3911 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "1074/1074 [==============================] - 0s 253us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "1074/1074 [==============================] - 0s 260us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3751 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3532 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3301 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "1074/1074 [==============================] - 0s 249us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3146 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "1074/1074 [==============================] - 0s 249us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.2967 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.2925 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.2995 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.2962 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.2928 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3290 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3516 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3758 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3791 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3338 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3165 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3163 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3240 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3434 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3699 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3917 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.4190 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "1074/1074 [==============================] - 0s 215us/step - loss: 3.1160 - acc: 0.0000e+00 - val_loss: 0.4413 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1185 - acc: 0.0000e+00 - val_loss: 0.4343 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1169 - acc: 0.0000e+00 - val_loss: 0.4167 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1141 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3315 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3302 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3485 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3488 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3433 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3360 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3241 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3149 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3153 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3208 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3506 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "1074/1074 [==============================] - 0s 218us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3724 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3771 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3724 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3518 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3415 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3700 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1075 - acc: 0.0000e+00 - val_loss: 0.4241 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1164 - acc: 0.0000e+00 - val_loss: 0.4736 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1263 - acc: 0.0000e+00 - val_loss: 0.5041 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1342 - acc: 0.0000e+00 - val_loss: 0.5140 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1343 - acc: 0.0000e+00 - val_loss: 0.4756 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1233 - acc: 0.0000e+00 - val_loss: 0.4075 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3435 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.2963 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.2812 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "1074/1074 [==============================] - 0s 260us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2856 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "1074/1074 [==============================] - 0s 263us/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.2949 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "1074/1074 [==============================] - 0s 278us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.3092 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "1074/1074 [==============================] - 0s 271us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3242 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "1074/1074 [==============================] - 0s 270us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3313 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "1074/1074 [==============================] - 0s 281us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3245 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "1074/1074 [==============================] - 0s 275us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3063 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "1074/1074 [==============================] - 0s 299us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.2955 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "1074/1074 [==============================] - 0s 281us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3107 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "1074/1074 [==============================] - 0s 279us/step - loss: 3.1070 - acc: 0.0000e+00 - val_loss: 0.3512 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "1074/1074 [==============================] - 0s 281us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3920 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "1074/1074 [==============================] - 0s 325us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.4124 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "1074/1074 [==============================] - 0s 333us/step - loss: 3.1139 - acc: 0.0000e+00 - val_loss: 0.4169 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "1074/1074 [==============================] - 1s 499us/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.3932 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "1074/1074 [==============================] - 0s 437us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3645 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "1074/1074 [==============================] - 0s 458us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3423 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "1074/1074 [==============================] - 0s 442us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "1074/1074 [==============================] - 0s 445us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3145 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "1074/1074 [==============================] - 0s 430us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3181 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "1074/1074 [==============================] - 1s 478us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3295 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "1074/1074 [==============================] - 1s 470us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3411 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "1074/1074 [==============================] - 0s 330us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3457 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "1074/1074 [==============================] - 0s 315us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3149 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "1074/1074 [==============================] - 0s 308us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3286 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "1074/1074 [==============================] - 0s 307us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3597 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "1074/1074 [==============================] - 0s 304us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3815 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "1074/1074 [==============================] - 0s 330us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3879 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "1074/1074 [==============================] - 0s 320us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3767 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "1074/1074 [==============================] - 0s 331us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "1074/1074 [==============================] - 0s 320us/step - loss: 3.1078 - acc: 0.0000e+00 - val_loss: 0.3085 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "1074/1074 [==============================] - 0s 323us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.2915 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "1074/1074 [==============================] - 0s 327us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.2942 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.2942 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "1074/1074 [==============================] - 0s 287us/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.3048 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "1074/1074 [==============================] - 0s 285us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3245 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "1074/1074 [==============================] - 0s 283us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "1074/1074 [==============================] - 0s 277us/step - loss: 3.1071 - acc: 0.0000e+00 - val_loss: 0.3700 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "1074/1074 [==============================] - 0s 289us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.4116 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "1074/1074 [==============================] - 0s 295us/step - loss: 3.1149 - acc: 0.0000e+00 - val_loss: 0.4200 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "1074/1074 [==============================] - 0s 282us/step - loss: 3.1144 - acc: 0.0000e+00 - val_loss: 0.4049 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "1074/1074 [==============================] - 0s 288us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3887 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "1074/1074 [==============================] - 0s 269us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3812 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "1074/1074 [==============================] - 0s 261us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3696 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "1074/1074 [==============================] - 0s 272us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3429 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "1074/1074 [==============================] - 0s 285us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3131 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3039 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3039 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2995 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3330 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3591 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3804 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "1074/1074 [==============================] - 0s 249us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3954 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.4046 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.4166 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1139 - acc: 0.0000e+00 - val_loss: 0.4023 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3841 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3670 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3657 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3699 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3726 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3744 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3634 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3472 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3553 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3861 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.4090 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1139 - acc: 0.0000e+00 - val_loss: 0.4125 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.3857 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3484 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3214 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3075 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3163 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "1074/1074 [==============================] - 0s 216us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3327 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3479 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3462 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "1074/1074 [==============================] - 0s 283us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3487 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3588 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3608 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3591 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "1074/1074 [==============================] - 0s 254us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3737 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "1074/1074 [==============================] - 0s 255us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3827 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "1074/1074 [==============================] - 0s 249us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3477 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "1074/1074 [==============================] - 0s 263us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3531 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3689 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "1074/1074 [==============================] - 0s 247us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "1074/1074 [==============================] - 0s 250us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3361 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3357 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3521 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3825 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3946 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3751 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3484 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3258 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3126 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3245 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3364 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3398 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3445 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3418 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3561 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3677 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3709 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3733 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3981 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.4434 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1191 - acc: 0.0000e+00 - val_loss: 0.4648 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1237 - acc: 0.0000e+00 - val_loss: 0.4519 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "1074/1074 [==============================] - 0s 273us/step - loss: 3.1188 - acc: 0.0000e+00 - val_loss: 0.3941 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3464 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "1074/1074 [==============================] - 0s 264us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3149 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "1074/1074 [==============================] - 0s 246us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.2880 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "1074/1074 [==============================] - 0s 247us/step - loss: 3.1144 - acc: 0.0000e+00 - val_loss: 0.2786 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1155 - acc: 0.0000e+00 - val_loss: 0.2949 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3190 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3581 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "1074/1074 [==============================] - 0s 256us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3967 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "1074/1074 [==============================] - 0s 261us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.4366 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.4468 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1194 - acc: 0.0000e+00 - val_loss: 0.4335 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1163 - acc: 0.0000e+00 - val_loss: 0.3963 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2864 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1140 - acc: 0.0000e+00 - val_loss: 0.2647 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1211 - acc: 0.0000e+00 - val_loss: 0.2662 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1163 - acc: 0.0000e+00 - val_loss: 0.3091 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.4112 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.4307 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1164 - acc: 0.0000e+00 - val_loss: 0.4248 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1155 - acc: 0.0000e+00 - val_loss: 0.4131 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "1074/1074 [==============================] - 0s 217us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3682 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3600 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3775 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3699 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3555 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3774 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3912 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3760 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3507 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3502 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3728 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.4133 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "1074/1074 [==============================] - 0s 257us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.4307 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1167 - acc: 0.0000e+00 - val_loss: 0.4359 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1171 - acc: 0.0000e+00 - val_loss: 0.4179 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1148 - acc: 0.0000e+00 - val_loss: 0.3931 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3711 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3386 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3261 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3289 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3295 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "1074/1074 [==============================] - 0s 289us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3218 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "1074/1074 [==============================] - 0s 311us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3163 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "1074/1074 [==============================] - 0s 307us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3040 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "1074/1074 [==============================] - 0s 305us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3091 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3097 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "1074/1074 [==============================] - 0s 306us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3207 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "1074/1074 [==============================] - 0s 304us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3247 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "1074/1074 [==============================] - 0s 309us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3140 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2816 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1148 - acc: 0.0000e+00 - val_loss: 0.2629 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "1074/1074 [==============================] - 0s 306us/step - loss: 3.1202 - acc: 0.0000e+00 - val_loss: 0.2647 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "1074/1074 [==============================] - 0s 281us/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2989 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "1074/1074 [==============================] - 0s 269us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "1074/1074 [==============================] - 0s 271us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3382 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "1074/1074 [==============================] - 0s 304us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.2974 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "1074/1074 [==============================] - 0s 284us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2874 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "1074/1074 [==============================] - 0s 274us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.2984 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "1074/1074 [==============================] - 0s 272us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "1074/1074 [==============================] - 0s 273us/step - loss: 3.1073 - acc: 0.0000e+00 - val_loss: 0.3719 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "1074/1074 [==============================] - 0s 265us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3958 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "1074/1074 [==============================] - 0s 278us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3535 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "1074/1074 [==============================] - 0s 272us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3114 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "1074/1074 [==============================] - 0s 268us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3015 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "1074/1074 [==============================] - 0s 264us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3040 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "1074/1074 [==============================] - 0s 265us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.2970 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.2900 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "1074/1074 [==============================] - 0s 257us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.2769 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 3.1171 - acc: 0.0000e+00 - val_loss: 0.2754 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.3028 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3338 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3617 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "1074/1074 [==============================] - 0s 270us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3634 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "1074/1074 [==============================] - 0s 256us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3613 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3272 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.2915 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.2965 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "1074/1074 [==============================] - 0s 250us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3659 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "1074/1074 [==============================] - 0s 266us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3782 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3813 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "1074/1074 [==============================] - 0s 266us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3742 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "1074/1074 [==============================] - 0s 288us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3788 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "1074/1074 [==============================] - 0s 335us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3689 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "1074/1074 [==============================] - 0s 323us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3467 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "1074/1074 [==============================] - 0s 289us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3209 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "1074/1074 [==============================] - 0s 290us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3133 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "1074/1074 [==============================] - 0s 284us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3195 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "1074/1074 [==============================] - 0s 294us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3254 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "1074/1074 [==============================] - 0s 287us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "1074/1074 [==============================] - 0s 302us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3564 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "1074/1074 [==============================] - 0s 297us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3606 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "1074/1074 [==============================] - 0s 272us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "1074/1074 [==============================] - 0s 321us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3279 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "1074/1074 [==============================] - 1s 482us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3227 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "1074/1074 [==============================] - 0s 428us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3001 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "1074/1074 [==============================] - 0s 423us/step - loss: 3.1131 - acc: 0.0000e+00 - val_loss: 0.3044 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "1074/1074 [==============================] - 1s 475us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3384 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "1074/1074 [==============================] - 0s 423us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3769 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "1074/1074 [==============================] - 0s 436us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3987 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "1074/1074 [==============================] - 0s 429us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.4003 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "1074/1074 [==============================] - 0s 442us/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.3659 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "1074/1074 [==============================] - 0s 321us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3132 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "1074/1074 [==============================] - 0s 383us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2858 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "1074/1074 [==============================] - 0s 314us/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.2854 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "1074/1074 [==============================] - 0s 361us/step - loss: 3.1141 - acc: 0.0000e+00 - val_loss: 0.2809 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "1074/1074 [==============================] - 0s 338us/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.2882 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.3077 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "1074/1074 [==============================] - 0s 336us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3147 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "1074/1074 [==============================] - 0s 331us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3361 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "1074/1074 [==============================] - 0s 315us/step - loss: 3.1078 - acc: 0.0000e+00 - val_loss: 0.3576 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "1074/1074 [==============================] - 0s 331us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3736 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "1074/1074 [==============================] - 0s 273us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3842 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "1074/1074 [==============================] - 0s 268us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3853 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "1074/1074 [==============================] - 0s 265us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3811 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3544 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "1074/1074 [==============================] - 0s 264us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3145 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "1074/1074 [==============================] - 0s 278us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.2962 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "1074/1074 [==============================] - 0s 275us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3053 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "1074/1074 [==============================] - 0s 267us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2949 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "1074/1074 [==============================] - 0s 262us/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.2904 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "1074/1074 [==============================] - 0s 264us/step - loss: 3.1135 - acc: 0.0000e+00 - val_loss: 0.2934 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "1074/1074 [==============================] - 0s 265us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.3080 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "1074/1074 [==============================] - 0s 276us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3513 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "1074/1074 [==============================] - 0s 288us/step - loss: 3.1073 - acc: 0.0000e+00 - val_loss: 0.4097 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 2.84 MSE (1.69 RMSE)\n",
      "Test Score: 1.41 MSE (1.19 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 8.068400 ---> Previsão: 7.111453 Diff: 0.956948 Racio: 0.134564\n",
      "valor: 8.214900 ---> Previsão: 7.111453 Diff: 1.103447 Racio: 0.155165\n",
      "valor: 8.268400 ---> Previsão: 7.111453 Diff: 1.156948 Racio: 0.162688\n",
      "valor: 8.216300 ---> Previsão: 7.111453 Diff: 1.104847 Racio: 0.155362\n",
      "valor: 8.240600 ---> Previsão: 7.111453 Diff: 1.129147 Racio: 0.158779\n",
      "valor: 8.357400 ---> Previsão: 7.111453 Diff: 1.245947 Racio: 0.175203\n",
      "valor: 8.285500 ---> Previsão: 7.111453 Diff: 1.174047 Racio: 0.165092\n",
      "valor: 8.221000 ---> Previsão: 7.111453 Diff: 1.109547 Racio: 0.156023\n",
      "valor: 8.173500 ---> Previsão: 7.111453 Diff: 1.062047 Racio: 0.149343\n",
      "valor: 8.195600 ---> Previsão: 7.111453 Diff: 1.084147 Racio: 0.152451\n",
      "valor: 8.099000 ---> Previsão: 7.111453 Diff: 0.987548 Racio: 0.138867\n",
      "valor: 8.054800 ---> Previsão: 7.111453 Diff: 0.943347 Racio: 0.132652\n",
      "valor: 7.884200 ---> Previsão: 7.111453 Diff: 0.772747 Racio: 0.108662\n",
      "valor: 7.821900 ---> Previsão: 7.111453 Diff: 0.710447 Racio: 0.099902\n",
      "valor: 7.811000 ---> Previsão: 7.111453 Diff: 0.699547 Racio: 0.098369\n",
      "valor: 8.020300 ---> Previsão: 7.111453 Diff: 0.908848 Racio: 0.127801\n",
      "valor: 8.119800 ---> Previsão: 7.111453 Diff: 1.008347 Racio: 0.141792\n",
      "valor: 8.055900 ---> Previsão: 7.111453 Diff: 0.944448 Racio: 0.132807\n",
      "valor: 7.802900 ---> Previsão: 7.111453 Diff: 0.691447 Racio: 0.097230\n",
      "valor: 7.717500 ---> Previsão: 7.111453 Diff: 0.606047 Racio: 0.085221\n",
      "valor: 7.532200 ---> Previsão: 7.111453 Diff: 0.420747 Racio: 0.059165\n",
      "valor: 7.751600 ---> Previsão: 7.111453 Diff: 0.640147 Racio: 0.090016\n",
      "valor: 7.799800 ---> Previsão: 7.111453 Diff: 0.688347 Racio: 0.096794\n",
      "valor: 7.861600 ---> Previsão: 7.111453 Diff: 0.750147 Racio: 0.105484\n",
      "valor: 7.759700 ---> Previsão: 7.111453 Diff: 0.648247 Racio: 0.091155\n",
      "valor: 7.848000 ---> Previsão: 7.111453 Diff: 0.736547 Racio: 0.103572\n",
      "valor: 7.850000 ---> Previsão: 7.111453 Diff: 0.738547 Racio: 0.103853\n",
      "valor: 7.790000 ---> Previsão: 7.111453 Diff: 0.678547 Racio: 0.095416\n",
      "valor: 7.802300 ---> Previsão: 7.111453 Diff: 0.690847 Racio: 0.097146\n",
      "valor: 7.857900 ---> Previsão: 7.111453 Diff: 0.746447 Racio: 0.104964\n",
      "valor: 7.894400 ---> Previsão: 7.111453 Diff: 0.782947 Racio: 0.110097\n",
      "valor: 7.758800 ---> Previsão: 7.111453 Diff: 0.647347 Racio: 0.091029\n",
      "valor: 7.643300 ---> Previsão: 7.111453 Diff: 0.531848 Racio: 0.074787\n",
      "valor: 7.644600 ---> Previsão: 7.111453 Diff: 0.533148 Racio: 0.074970\n",
      "valor: 7.782200 ---> Previsão: 7.111453 Diff: 0.670747 Racio: 0.094319\n",
      "valor: 7.761800 ---> Previsão: 7.111453 Diff: 0.650347 Racio: 0.091451\n",
      "valor: 7.914700 ---> Previsão: 7.111453 Diff: 0.803247 Racio: 0.112951\n",
      "valor: 7.951700 ---> Previsão: 7.111453 Diff: 0.840247 Racio: 0.118154\n",
      "valor: 8.094500 ---> Previsão: 7.111453 Diff: 0.983048 Racio: 0.138234\n",
      "valor: 8.079000 ---> Previsão: 7.111453 Diff: 0.967548 Racio: 0.136055\n",
      "valor: 8.153400 ---> Previsão: 7.111453 Diff: 1.041948 Racio: 0.146517\n",
      "valor: 8.178900 ---> Previsão: 7.111453 Diff: 1.067448 Racio: 0.150103\n",
      "valor: 8.156500 ---> Previsão: 7.111453 Diff: 1.045048 Racio: 0.146953\n",
      "valor: 8.098400 ---> Previsão: 7.111453 Diff: 0.986948 Racio: 0.138783\n",
      "valor: 8.125000 ---> Previsão: 7.111453 Diff: 1.013547 Racio: 0.142523\n",
      "valor: 8.152000 ---> Previsão: 7.111453 Diff: 1.040548 Racio: 0.146320\n",
      "valor: 8.122000 ---> Previsão: 7.111453 Diff: 1.010548 Racio: 0.142101\n",
      "valor: 8.096800 ---> Previsão: 7.111453 Diff: 0.985347 Racio: 0.138558\n",
      "valor: 8.078000 ---> Previsão: 7.111453 Diff: 0.966547 Racio: 0.135914\n",
      "valor: 8.099300 ---> Previsão: 7.111453 Diff: 0.987847 Racio: 0.138909\n",
      "valor: 8.045700 ---> Previsão: 7.111453 Diff: 0.934247 Racio: 0.131372\n",
      "valor: 8.028800 ---> Previsão: 7.111453 Diff: 0.917347 Racio: 0.128996\n",
      "valor: 7.924500 ---> Previsão: 7.111453 Diff: 0.813048 Racio: 0.114329\n",
      "valor: 8.080100 ---> Previsão: 7.111453 Diff: 0.968648 Racio: 0.136210\n",
      "valor: 8.077700 ---> Previsão: 7.111453 Diff: 0.966248 Racio: 0.135872\n",
      "valor: 8.130200 ---> Previsão: 7.111453 Diff: 1.018748 Racio: 0.143255\n",
      "valor: 8.252100 ---> Previsão: 7.111453 Diff: 1.140648 Racio: 0.160396\n",
      "valor: 8.271800 ---> Previsão: 7.111453 Diff: 1.160347 Racio: 0.163166\n",
      "valor: 8.260100 ---> Previsão: 7.111453 Diff: 1.148648 Racio: 0.161521\n",
      "valor: 8.298600 ---> Previsão: 7.111453 Diff: 1.187147 Racio: 0.166935\n",
      "valor: 8.295300 ---> Previsão: 7.111453 Diff: 1.183848 Racio: 0.166471\n",
      "valor: 8.309400 ---> Previsão: 7.111453 Diff: 1.197947 Racio: 0.168453\n",
      "valor: 8.274600 ---> Previsão: 7.111453 Diff: 1.163148 Racio: 0.163560\n",
      "valor: 8.290200 ---> Previsão: 7.111453 Diff: 1.178748 Racio: 0.165753\n",
      "valor: 8.243700 ---> Previsão: 7.111453 Diff: 1.132247 Racio: 0.159215\n",
      "valor: 8.281700 ---> Previsão: 7.111453 Diff: 1.170247 Racio: 0.164558\n",
      "valor: 8.444300 ---> Previsão: 7.111453 Diff: 1.332847 Racio: 0.187423\n",
      "valor: 8.495300 ---> Previsão: 7.111453 Diff: 1.383848 Racio: 0.194594\n",
      "valor: 8.584500 ---> Previsão: 7.111453 Diff: 1.473048 Racio: 0.207137\n",
      "valor: 8.569800 ---> Previsão: 7.111453 Diff: 1.458347 Racio: 0.205070\n",
      "valor: 8.450300 ---> Previsão: 7.111453 Diff: 1.338848 Racio: 0.188266\n",
      "valor: 8.238300 ---> Previsão: 7.111453 Diff: 1.126848 Racio: 0.158455\n",
      "valor: 8.201900 ---> Previsão: 7.111453 Diff: 1.090447 Racio: 0.153337\n",
      "valor: 8.152400 ---> Previsão: 7.111453 Diff: 1.040947 Racio: 0.146376\n",
      "valor: 8.182600 ---> Previsão: 7.111453 Diff: 1.071148 Racio: 0.150623\n",
      "valor: 8.201300 ---> Previsão: 7.111453 Diff: 1.089847 Racio: 0.153252\n",
      "valor: 8.216200 ---> Previsão: 7.111453 Diff: 1.104747 Racio: 0.155348\n",
      "valor: 8.292300 ---> Previsão: 7.111453 Diff: 1.180847 Racio: 0.166049\n",
      "valor: 8.298800 ---> Previsão: 7.111453 Diff: 1.187347 Racio: 0.166963\n",
      "valor: 8.300600 ---> Previsão: 7.111453 Diff: 1.189147 Racio: 0.167216\n",
      "valor: 8.348500 ---> Previsão: 7.111453 Diff: 1.237047 Racio: 0.173951\n",
      "valor: 8.389600 ---> Previsão: 7.111453 Diff: 1.278148 Racio: 0.179731\n",
      "valor: 8.400300 ---> Previsão: 7.111453 Diff: 1.288848 Racio: 0.181236\n",
      "valor: 8.373200 ---> Previsão: 7.111453 Diff: 1.261747 Racio: 0.177425\n",
      "valor: 8.421700 ---> Previsão: 7.111453 Diff: 1.310247 Racio: 0.184245\n",
      "valor: 8.465500 ---> Previsão: 7.111453 Diff: 1.354047 Racio: 0.190404\n",
      "valor: 8.492700 ---> Previsão: 7.111453 Diff: 1.381248 Racio: 0.194229\n",
      "valor: 8.513600 ---> Previsão: 7.111453 Diff: 1.402147 Racio: 0.197167\n",
      "valor: 8.510000 ---> Previsão: 7.111453 Diff: 1.398547 Racio: 0.196661\n",
      "valor: 8.478100 ---> Previsão: 7.111453 Diff: 1.366647 Racio: 0.192176\n",
      "valor: 8.496700 ---> Previsão: 7.111453 Diff: 1.385247 Racio: 0.194791\n",
      "valor: 8.449300 ---> Previsão: 7.111453 Diff: 1.337847 Racio: 0.188126\n",
      "valor: 8.567500 ---> Previsão: 7.111453 Diff: 1.456047 Racio: 0.204747\n",
      "valor: 8.498500 ---> Previsão: 7.111453 Diff: 1.387047 Racio: 0.195044\n",
      "valor: 8.490800 ---> Previsão: 7.111453 Diff: 1.379348 Racio: 0.193961\n",
      "valor: 8.472700 ---> Previsão: 7.111453 Diff: 1.361248 Racio: 0.191416\n",
      "valor: 8.511500 ---> Previsão: 7.111453 Diff: 1.400048 Racio: 0.196872\n",
      "valor: 8.536400 ---> Previsão: 7.111453 Diff: 1.424948 Racio: 0.200374\n",
      "valor: 8.578400 ---> Previsão: 7.111453 Diff: 1.466948 Racio: 0.206280\n",
      "valor: 8.614100 ---> Previsão: 7.111453 Diff: 1.502647 Racio: 0.211300\n",
      "valor: 8.645800 ---> Previsão: 7.111453 Diff: 1.534348 Racio: 0.215757\n",
      "valor: 8.659100 ---> Previsão: 7.111453 Diff: 1.547647 Racio: 0.217627\n",
      "valor: 8.683900 ---> Previsão: 7.111453 Diff: 1.572448 Racio: 0.221115\n",
      "valor: 8.700000 ---> Previsão: 7.111453 Diff: 1.588547 Racio: 0.223379\n",
      "valor: 8.723700 ---> Previsão: 7.111453 Diff: 1.612247 Racio: 0.226711\n",
      "valor: 8.679100 ---> Previsão: 7.111453 Diff: 1.567647 Racio: 0.220440\n",
      "valor: 8.501400 ---> Previsão: 7.111453 Diff: 1.389948 Racio: 0.195452\n",
      "valor: 8.498000 ---> Previsão: 7.111453 Diff: 1.386547 Racio: 0.194974\n",
      "valor: 8.396500 ---> Previsão: 7.111453 Diff: 1.285048 Racio: 0.180701\n",
      "valor: 8.351400 ---> Previsão: 7.111453 Diff: 1.239948 Racio: 0.174359\n",
      "valor: 8.385100 ---> Previsão: 7.111453 Diff: 1.273648 Racio: 0.179098\n",
      "valor: 8.406300 ---> Previsão: 7.111453 Diff: 1.294847 Racio: 0.182079\n",
      "valor: 8.498700 ---> Previsão: 7.111453 Diff: 1.387247 Racio: 0.195072\n",
      "valor: 8.494800 ---> Previsão: 7.111453 Diff: 1.383347 Racio: 0.194524\n",
      "valor: 8.478000 ---> Previsão: 7.111453 Diff: 1.366547 Racio: 0.192161\n",
      "valor: 8.567500 ---> Previsão: 7.111453 Diff: 1.456047 Racio: 0.204747\n",
      "valor: 8.525700 ---> Previsão: 7.111453 Diff: 1.414247 Racio: 0.198869\n",
      "valor: 8.489100 ---> Previsão: 7.111453 Diff: 1.377647 Racio: 0.193722\n",
      "valor: 8.451000 ---> Previsão: 7.111453 Diff: 1.339547 Racio: 0.188365\n",
      "valor: 8.421000 ---> Previsão: 7.111453 Diff: 1.309547 Racio: 0.184146\n",
      "valor: 8.417000 ---> Previsão: 7.111453 Diff: 1.305548 Racio: 0.183584\n",
      "valor: 8.398800 ---> Previsão: 7.111453 Diff: 1.287347 Racio: 0.181025\n",
      "valor: 8.414600 ---> Previsão: 7.111453 Diff: 1.303148 Racio: 0.183246\n",
      "valor: 8.401800 ---> Previsão: 7.111453 Diff: 1.290347 Racio: 0.181446\n",
      "valor: 8.551300 ---> Previsão: 7.111453 Diff: 1.439847 Racio: 0.202469\n",
      "valor: 8.539900 ---> Previsão: 7.111453 Diff: 1.428447 Racio: 0.200866\n",
      "valor: 8.565100 ---> Previsão: 7.111453 Diff: 1.453648 Racio: 0.204409\n",
      "valor: 8.600800 ---> Previsão: 7.111453 Diff: 1.489348 Racio: 0.209429\n",
      "valor: 8.589500 ---> Previsão: 7.111453 Diff: 1.478048 Racio: 0.207840\n",
      "valor: 8.789300 ---> Previsão: 7.111453 Diff: 1.677847 Racio: 0.235936\n",
      "valor: 8.888400 ---> Previsão: 7.111453 Diff: 1.776948 Racio: 0.249871\n",
      "valor: 8.891400 ---> Previsão: 7.111453 Diff: 1.779948 Racio: 0.250293\n",
      "valor: 8.914400 ---> Previsão: 7.111453 Diff: 1.802947 Racio: 0.253527\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f956a0a1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    visualize_GOOGL()\n",
    "    LSTM_utilizando_GOOGL_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
