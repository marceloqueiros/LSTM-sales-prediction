{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1 - preparar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fazer o download da sequencias do valor das ações da google GOOGL stock data (fonte yahoo.com)\n",
    "dataset:\n",
    "http://chart.finance.yahoo.com/table.csv?s=GOOGL&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv\n",
    "A função get_stock_data é generica para ir buscar dados à yahoo.com\n",
    "trata-se uma tabela com: ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "Vamos śo utilizar os campos ['Open','High','Close']\n",
    "'''\n",
    "def get_stock_data(stock_name, normalized=0,file_name=None):\n",
    "    if not file_name:\n",
    "        file_name = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "    stocks = pd.read_csv(file_name, header=0, names=col_names) #fica numa especie de tabela exactamente como estava no csv (1350 linhas,7 colunas)\n",
    "    df = pd.DataFrame(stocks) #neste caso não vai fazer nada\n",
    "    date_split = df['Date'].str.split('-').str #não vai servir para nada\n",
    "    df['Year'], df['Month'], df['Day'] = date_split #não vai servir para nada\n",
    "    df[\"Volume\"] = df[\"Volume\"] / 10000 #não vai servir para nada\n",
    "    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) #vou só ficar com as colunas 1,2,4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_GOOGL_stock_dataset():\n",
    "    stock_name = 'GOOGL'\n",
    "    return get_stock_data(stock_name, 0, 'table.csv')\n",
    "\n",
    "def pre_processar_GOOGL_stock_dataset(df):\n",
    "    df['High'] = df['High'] / 100\n",
    "    df['Open'] = df['Open'] / 100\n",
    "    df['Close'] = df['Close'] / 100\n",
    "    return df\n",
    "\n",
    "# Visualizar os top registos da tabela\n",
    "def visualize_GOOGL():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    print('### Antes do pré-processamento ###')\n",
    "    print(df.head()) #mostra só os primeiros 5 registos\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print('### Após o pré-processamento ###')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - tam_sequencia): #numero de registos - tamanho da sequencia\n",
    "        res.append(mat_dados[i: i + tam_sequencia])\n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes (janela deslizante ao longo da serie)\n",
    "    qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    train = res[:qt_casos_treino, :]\n",
    "    x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2 - Definir a topologia da rede (arquitectura do modelo) e compilar '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(janela):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(janela, 3), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(janela, 3), return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]- predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i],predic[i], diff[i], racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_utilizando_GOOGL_data():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    #model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    #print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em \n",
    "    #um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    ''' \n",
    "    MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "    o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194, 22, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Antes do pré-processamento ###\n",
      "         Open        High       Close\n",
      "0  929.000000  935.900024  924.520020\n",
      "1  890.000000  893.380005  891.440002\n",
      "2  891.390015  892.989990  889.140015\n",
      "3  882.260010  892.250000  888.840027\n",
      "4  868.440002  879.960022  878.929993\n",
      "### Após o pré-processamento ###\n",
      "     Open    High   Close\n",
      "0  9.2900  9.3590  9.2452\n",
      "1  8.9000  8.9338  8.9144\n",
      "2  8.9139  8.9299  8.8914\n",
      "3  8.8226  8.9225  8.8884\n",
      "4  8.6844  8.7996  8.7893\n",
      "df (1350, 3)\n",
      "X_train (1194, 22, 3)\n",
      "y_train (1194,)\n",
      "X_test (133, 22, 3)\n",
      "y_test (133,)\n",
      "Train on 1074 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "1074/1074 [==============================] - 2s 2ms/step - loss: 54.6266 - acc: 0.0000e+00 - val_loss: 57.6716 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 54.2142 - acc: 0.0000e+00 - val_loss: 57.1725 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 53.7332 - acc: 0.0000e+00 - val_loss: 56.5840 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 53.1533 - acc: 0.0000e+00 - val_loss: 55.8294 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 52.3968 - acc: 0.0000e+00 - val_loss: 54.8007 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 51.3722 - acc: 0.0000e+00 - val_loss: 53.4331 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 50.0400 - acc: 0.0000e+00 - val_loss: 51.8248 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 48.5042 - acc: 0.0000e+00 - val_loss: 50.0456 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 46.8176 - acc: 0.0000e+00 - val_loss: 48.2066 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 45.0435 - acc: 0.0000e+00 - val_loss: 46.1755 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 43.0980 - acc: 0.0000e+00 - val_loss: 43.9466 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 40.9892 - acc: 0.0000e+00 - val_loss: 41.5694 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 38.7378 - acc: 0.0000e+00 - val_loss: 39.0427 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 36.3501 - acc: 0.0000e+00 - val_loss: 36.3453 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 33.8126 - acc: 0.0000e+00 - val_loss: 33.5168 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 31.1898 - acc: 0.0000e+00 - val_loss: 30.6241 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 28.4968 - acc: 0.0000e+00 - val_loss: 27.6803 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 25.7883 - acc: 0.0000e+00 - val_loss: 24.7167 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 23.0848 - acc: 0.0000e+00 - val_loss: 21.7815 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 20.4209 - acc: 0.0000e+00 - val_loss: 18.9195 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 17.8478 - acc: 0.0000e+00 - val_loss: 16.1677 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 15.4251 - acc: 0.0000e+00 - val_loss: 13.5726 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 13.1460 - acc: 0.0000e+00 - val_loss: 11.1853 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 11.0920 - acc: 0.0000e+00 - val_loss: 9.0182 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 9.2656 - acc: 0.0000e+00 - val_loss: 7.1083 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 7.7089 - acc: 0.0000e+00 - val_loss: 5.4570 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 6.3911 - acc: 0.0000e+00 - val_loss: 4.0788 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 5.3355 - acc: 0.0000e+00 - val_loss: 2.9650 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 4.5185 - acc: 0.0000e+00 - val_loss: 2.0993 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.9387 - acc: 0.0000e+00 - val_loss: 1.4456 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.5385 - acc: 0.0000e+00 - val_loss: 0.9793 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.2987 - acc: 0.0000e+00 - val_loss: 0.6635 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1728 - acc: 0.0000e+00 - val_loss: 0.4633 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1217 - acc: 0.0000e+00 - val_loss: 0.3459 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.2795 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1202 - acc: 0.0000e+00 - val_loss: 0.2404 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1318 - acc: 0.0000e+00 - val_loss: 0.2194 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1439 - acc: 0.0000e+00 - val_loss: 0.2125 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1483 - acc: 0.0000e+00 - val_loss: 0.2129 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1474 - acc: 0.0000e+00 - val_loss: 0.2182 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1430 - acc: 0.0000e+00 - val_loss: 0.2270 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1366 - acc: 0.0000e+00 - val_loss: 0.2341 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1320 - acc: 0.0000e+00 - val_loss: 0.2412 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1282 - acc: 0.0000e+00 - val_loss: 0.2508 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1242 - acc: 0.0000e+00 - val_loss: 0.2598 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1210 - acc: 0.0000e+00 - val_loss: 0.2651 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1191 - acc: 0.0000e+00 - val_loss: 0.2702 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.2732 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1169 - acc: 0.0000e+00 - val_loss: 0.2742 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1163 - acc: 0.0000e+00 - val_loss: 0.2794 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1151 - acc: 0.0000e+00 - val_loss: 0.2872 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.2951 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3016 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3097 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3183 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3226 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3279 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3348 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3341 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3315 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3351 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3404 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3314 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3235 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3114 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.2997 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2951 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.2971 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3161 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3300 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3353 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3389 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3447 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3490 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3479 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3443 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3381 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3265 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3205 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3190 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3177 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3155 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3227 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3398 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3494 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3455 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3428 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3445 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3606 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3675 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3801 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3884 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3830 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3773 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3718 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3596 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3406 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3525 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3469 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3361 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3057 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2985 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.2993 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2979 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3014 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3079 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3125 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3191 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3282 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3493 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3607 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3722 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3733 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3456 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3255 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3172 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3040 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3039 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3139 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3186 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3171 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3164 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3209 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3273 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3465 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3589 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3556 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3316 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3182 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3110 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3031 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3074 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3176 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3346 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3346 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3215 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3098 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3161 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3363 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3610 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3721 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3760 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3759 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3511 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3295 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3100 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2927 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2910 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3873 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3569 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3063 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3014 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.2975 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.2944 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.2902 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2913 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.3032 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3688 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3665 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3611 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3498 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3492 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3349 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3366 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3431 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3535 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3609 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3513 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3358 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3426 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3432 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3478 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3502 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3276 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3270 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3642 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3882 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3745 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3493 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3372 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3423 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3604 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3687 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3674 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3583 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3494 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3283 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3046 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2854 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1153 - acc: 0.0000e+00 - val_loss: 0.2760 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1153 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3095 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3914 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3997 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.4029 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.4032 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.3973 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3858 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "1074/1074 [==============================] - 0s 246us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3644 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3241 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3243 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3464 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3656 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "1074/1074 [==============================] - 0s 246us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3807 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3761 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3694 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3744 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3595 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3545 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3707 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3641 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3309 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3031 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2877 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "1074/1074 [==============================] - 0s 245us/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.2849 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2929 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.2999 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3055 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3231 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3576 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3486 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3369 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3198 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.2947 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.2752 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1178 - acc: 0.0000e+00 - val_loss: 0.2712 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1167 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3261 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3700 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3947 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.4049 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3875 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3725 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3402 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3334 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3185 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3165 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3446 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3568 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3585 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3735 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3805 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3599 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3521 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3380 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3325 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3399 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3531 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3735 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3742 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3596 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3415 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3175 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3038 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.2989 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3140 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3597 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3704 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3624 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3669 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3636 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3523 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3309 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3417 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3619 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3910 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4114 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1135 - acc: 0.0000e+00 - val_loss: 0.4134 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.4191 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "1074/1074 [==============================] - 0s 243us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4175 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "1074/1074 [==============================] - 0s 239us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.3999 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3803 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3387 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3050 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.2907 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.2914 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3081 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3251 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3515 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3682 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3573 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "1074/1074 [==============================] - 0s 240us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3320 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "1074/1074 [==============================] - 0s 250us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3252 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3383 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3069 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2917 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.2730 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1171 - acc: 0.0000e+00 - val_loss: 0.2745 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1159 - acc: 0.0000e+00 - val_loss: 0.2961 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1072 - acc: 0.0000e+00 - val_loss: 0.3579 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3846 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3889 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3999 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3959 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3702 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3623 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3554 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3450 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3374 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3247 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3211 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3153 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3073 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3471 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3746 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3950 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3932 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3668 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3504 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3433 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3397 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3567 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3696 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3603 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3365 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.2841 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1148 - acc: 0.0000e+00 - val_loss: 0.2801 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2900 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3027 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3366 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3251 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3028 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3021 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3033 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3335 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3445 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3301 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "1074/1074 [==============================] - 0s 241us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3323 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3681 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "1074/1074 [==============================] - 0s 237us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3836 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3983 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.4154 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "1074/1074 [==============================] - 0s 238us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4293 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "1074/1074 [==============================] - 0s 234us/step - loss: 3.1162 - acc: 0.0000e+00 - val_loss: 0.4231 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3351 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3034 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1143 - acc: 0.0000e+00 - val_loss: 0.2823 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "1074/1074 [==============================] - 0s 244us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.2943 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "1074/1074 [==============================] - 0s 247us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3243 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3647 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3783 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3887 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3964 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3915 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3283 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.2868 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1168 - acc: 0.0000e+00 - val_loss: 0.2601 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1208 - acc: 0.0000e+00 - val_loss: 0.2656 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2783 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3614 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3714 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3401 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3382 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3547 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3585 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3621 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3763 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3510 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3394 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3306 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3414 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3407 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3480 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3721 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3803 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3880 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3881 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3326 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3075 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3027 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3189 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3830 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.4024 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4048 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3678 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3179 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.2824 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1160 - acc: 0.0000e+00 - val_loss: 0.2677 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.2878 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3287 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3835 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.4124 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "1074/1074 [==============================] - 0s 235us/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.4159 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1140 - acc: 0.0000e+00 - val_loss: 0.3935 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2831 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1155 - acc: 0.0000e+00 - val_loss: 0.2728 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1165 - acc: 0.0000e+00 - val_loss: 0.2874 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.3181 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3558 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.4136 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.4318 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.4282 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.3867 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 2.83 MSE (1.68 RMSE)\n",
      "Test Score: 1.28 MSE (1.13 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 8.068400 ---> Previsão: 7.170642 Diff: 0.897758 Racio: 0.125199\n",
      "valor: 8.214900 ---> Previsão: 7.170642 Diff: 1.044258 Racio: 0.145630\n",
      "valor: 8.268400 ---> Previsão: 7.170642 Diff: 1.097758 Racio: 0.153091\n",
      "valor: 8.216300 ---> Previsão: 7.170642 Diff: 1.045658 Racio: 0.145825\n",
      "valor: 8.240600 ---> Previsão: 7.170642 Diff: 1.069958 Racio: 0.149214\n",
      "valor: 8.357400 ---> Previsão: 7.170642 Diff: 1.186758 Racio: 0.165502\n",
      "valor: 8.285500 ---> Previsão: 7.170642 Diff: 1.114858 Racio: 0.155475\n",
      "valor: 8.221000 ---> Previsão: 7.170642 Diff: 1.050358 Racio: 0.146480\n",
      "valor: 8.173500 ---> Previsão: 7.170642 Diff: 1.002858 Racio: 0.139856\n",
      "valor: 8.195600 ---> Previsão: 7.170642 Diff: 1.024958 Racio: 0.142938\n",
      "valor: 8.099000 ---> Previsão: 7.170642 Diff: 0.928358 Racio: 0.129467\n",
      "valor: 8.054800 ---> Previsão: 7.170642 Diff: 0.884158 Racio: 0.123302\n",
      "valor: 7.884200 ---> Previsão: 7.170642 Diff: 0.713558 Racio: 0.099511\n",
      "valor: 7.821900 ---> Previsão: 7.170642 Diff: 0.651258 Racio: 0.090823\n",
      "valor: 7.811000 ---> Previsão: 7.170642 Diff: 0.640358 Racio: 0.089303\n",
      "valor: 8.020300 ---> Previsão: 7.170642 Diff: 0.849658 Racio: 0.118491\n",
      "valor: 8.119800 ---> Previsão: 7.170642 Diff: 0.949158 Racio: 0.132367\n",
      "valor: 8.055900 ---> Previsão: 7.170642 Diff: 0.885258 Racio: 0.123456\n",
      "valor: 7.802900 ---> Previsão: 7.170642 Diff: 0.632258 Racio: 0.088173\n",
      "valor: 7.717500 ---> Previsão: 7.170642 Diff: 0.546858 Racio: 0.076263\n",
      "valor: 7.532200 ---> Previsão: 7.170642 Diff: 0.361558 Racio: 0.050422\n",
      "valor: 7.751600 ---> Previsão: 7.170642 Diff: 0.580958 Racio: 0.081019\n",
      "valor: 7.799800 ---> Previsão: 7.170642 Diff: 0.629158 Racio: 0.087741\n",
      "valor: 7.861600 ---> Previsão: 7.170642 Diff: 0.690958 Racio: 0.096359\n",
      "valor: 7.759700 ---> Previsão: 7.170642 Diff: 0.589058 Racio: 0.082149\n",
      "valor: 7.848000 ---> Previsão: 7.170642 Diff: 0.677358 Racio: 0.094463\n",
      "valor: 7.850000 ---> Previsão: 7.170642 Diff: 0.679358 Racio: 0.094742\n",
      "valor: 7.790000 ---> Previsão: 7.170642 Diff: 0.619358 Racio: 0.086374\n",
      "valor: 7.802300 ---> Previsão: 7.170642 Diff: 0.631658 Racio: 0.088089\n",
      "valor: 7.857900 ---> Previsão: 7.170642 Diff: 0.687258 Racio: 0.095843\n",
      "valor: 7.894400 ---> Previsão: 7.170642 Diff: 0.723758 Racio: 0.100934\n",
      "valor: 7.758800 ---> Previsão: 7.170642 Diff: 0.588158 Racio: 0.082023\n",
      "valor: 7.643300 ---> Previsão: 7.170642 Diff: 0.472658 Racio: 0.065916\n",
      "valor: 7.644600 ---> Previsão: 7.170642 Diff: 0.473958 Racio: 0.066097\n",
      "valor: 7.782200 ---> Previsão: 7.170642 Diff: 0.611558 Racio: 0.085286\n",
      "valor: 7.761800 ---> Previsão: 7.170642 Diff: 0.591158 Racio: 0.082441\n",
      "valor: 7.914700 ---> Previsão: 7.170642 Diff: 0.744058 Racio: 0.103764\n",
      "valor: 7.951700 ---> Previsão: 7.170642 Diff: 0.781058 Racio: 0.108924\n",
      "valor: 8.094500 ---> Previsão: 7.170642 Diff: 0.923858 Racio: 0.128839\n",
      "valor: 8.079000 ---> Previsão: 7.170642 Diff: 0.908358 Racio: 0.126677\n",
      "valor: 8.153400 ---> Previsão: 7.170642 Diff: 0.982758 Racio: 0.137053\n",
      "valor: 8.178900 ---> Previsão: 7.170642 Diff: 1.008258 Racio: 0.140609\n",
      "valor: 8.156500 ---> Previsão: 7.170642 Diff: 0.985858 Racio: 0.137485\n",
      "valor: 8.098400 ---> Previsão: 7.170642 Diff: 0.927758 Racio: 0.129383\n",
      "valor: 8.125000 ---> Previsão: 7.170642 Diff: 0.954358 Racio: 0.133092\n",
      "valor: 8.152000 ---> Previsão: 7.170642 Diff: 0.981358 Racio: 0.136858\n",
      "valor: 8.122000 ---> Previsão: 7.170642 Diff: 0.951358 Racio: 0.132674\n",
      "valor: 8.096800 ---> Previsão: 7.170642 Diff: 0.926158 Racio: 0.129160\n",
      "valor: 8.078000 ---> Previsão: 7.170642 Diff: 0.907358 Racio: 0.126538\n",
      "valor: 8.099300 ---> Previsão: 7.170642 Diff: 0.928658 Racio: 0.129508\n",
      "valor: 8.045700 ---> Previsão: 7.170642 Diff: 0.875058 Racio: 0.122033\n",
      "valor: 8.028800 ---> Previsão: 7.170642 Diff: 0.858158 Racio: 0.119677\n",
      "valor: 7.924500 ---> Previsão: 7.170642 Diff: 0.753858 Racio: 0.105131\n",
      "valor: 8.080100 ---> Previsão: 7.170642 Diff: 0.909458 Racio: 0.126831\n",
      "valor: 8.077700 ---> Previsão: 7.170642 Diff: 0.907058 Racio: 0.126496\n",
      "valor: 8.130200 ---> Previsão: 7.170642 Diff: 0.959558 Racio: 0.133818\n",
      "valor: 8.252100 ---> Previsão: 7.170642 Diff: 1.081458 Racio: 0.150818\n",
      "valor: 8.271800 ---> Previsão: 7.170642 Diff: 1.101158 Racio: 0.153565\n",
      "valor: 8.260100 ---> Previsão: 7.170642 Diff: 1.089458 Racio: 0.151933\n",
      "valor: 8.298600 ---> Previsão: 7.170642 Diff: 1.127958 Racio: 0.157302\n",
      "valor: 8.295300 ---> Previsão: 7.170642 Diff: 1.124658 Racio: 0.156842\n",
      "valor: 8.309400 ---> Previsão: 7.170642 Diff: 1.138758 Racio: 0.158808\n",
      "valor: 8.274600 ---> Previsão: 7.170642 Diff: 1.103958 Racio: 0.153955\n",
      "valor: 8.290200 ---> Previsão: 7.170642 Diff: 1.119558 Racio: 0.156131\n",
      "valor: 8.243700 ---> Previsão: 7.170642 Diff: 1.073058 Racio: 0.149646\n",
      "valor: 8.281700 ---> Previsão: 7.170642 Diff: 1.111058 Racio: 0.154945\n",
      "valor: 8.444300 ---> Previsão: 7.170642 Diff: 1.273658 Racio: 0.177621\n",
      "valor: 8.495300 ---> Previsão: 7.170642 Diff: 1.324658 Racio: 0.184734\n",
      "valor: 8.584500 ---> Previsão: 7.170642 Diff: 1.413858 Racio: 0.197173\n",
      "valor: 8.569800 ---> Previsão: 7.170642 Diff: 1.399158 Racio: 0.195123\n",
      "valor: 8.450300 ---> Previsão: 7.170642 Diff: 1.279658 Racio: 0.178458\n",
      "valor: 8.238300 ---> Previsão: 7.170642 Diff: 1.067658 Racio: 0.148893\n",
      "valor: 8.201900 ---> Previsão: 7.170642 Diff: 1.031258 Racio: 0.143817\n",
      "valor: 8.152400 ---> Previsão: 7.170642 Diff: 0.981758 Racio: 0.136914\n",
      "valor: 8.182600 ---> Previsão: 7.170642 Diff: 1.011958 Racio: 0.141125\n",
      "valor: 8.201300 ---> Previsão: 7.170642 Diff: 1.030658 Racio: 0.143733\n",
      "valor: 8.216200 ---> Previsão: 7.170642 Diff: 1.045558 Racio: 0.145811\n",
      "valor: 8.292300 ---> Previsão: 7.170642 Diff: 1.121658 Racio: 0.156424\n",
      "valor: 8.298800 ---> Previsão: 7.170642 Diff: 1.128158 Racio: 0.157330\n",
      "valor: 8.300600 ---> Previsão: 7.170642 Diff: 1.129958 Racio: 0.157581\n",
      "valor: 8.348500 ---> Previsão: 7.170642 Diff: 1.177858 Racio: 0.164261\n",
      "valor: 8.389600 ---> Previsão: 7.170642 Diff: 1.218958 Racio: 0.169993\n",
      "valor: 8.400300 ---> Previsão: 7.170642 Diff: 1.229658 Racio: 0.171485\n",
      "valor: 8.373200 ---> Previsão: 7.170642 Diff: 1.202558 Racio: 0.167706\n",
      "valor: 8.421700 ---> Previsão: 7.170642 Diff: 1.251058 Racio: 0.174469\n",
      "valor: 8.465500 ---> Previsão: 7.170642 Diff: 1.294858 Racio: 0.180578\n",
      "valor: 8.492700 ---> Previsão: 7.170642 Diff: 1.322058 Racio: 0.184371\n",
      "valor: 8.513600 ---> Previsão: 7.170642 Diff: 1.342958 Racio: 0.187286\n",
      "valor: 8.510000 ---> Previsão: 7.170642 Diff: 1.339358 Racio: 0.186784\n",
      "valor: 8.478100 ---> Previsão: 7.170642 Diff: 1.307458 Racio: 0.182335\n",
      "valor: 8.496700 ---> Previsão: 7.170642 Diff: 1.326058 Racio: 0.184929\n",
      "valor: 8.449300 ---> Previsão: 7.170642 Diff: 1.278658 Racio: 0.178318\n",
      "valor: 8.567500 ---> Previsão: 7.170642 Diff: 1.396858 Racio: 0.194802\n",
      "valor: 8.498500 ---> Previsão: 7.170642 Diff: 1.327858 Racio: 0.185180\n",
      "valor: 8.490800 ---> Previsão: 7.170642 Diff: 1.320158 Racio: 0.184106\n",
      "valor: 8.472700 ---> Previsão: 7.170642 Diff: 1.302058 Racio: 0.181582\n",
      "valor: 8.511500 ---> Previsão: 7.170642 Diff: 1.340858 Racio: 0.186993\n",
      "valor: 8.536400 ---> Previsão: 7.170642 Diff: 1.365758 Racio: 0.190465\n",
      "valor: 8.578400 ---> Previsão: 7.170642 Diff: 1.407758 Racio: 0.196323\n",
      "valor: 8.614100 ---> Previsão: 7.170642 Diff: 1.443458 Racio: 0.201301\n",
      "valor: 8.645800 ---> Previsão: 7.170642 Diff: 1.475158 Racio: 0.205722\n",
      "valor: 8.659100 ---> Previsão: 7.170642 Diff: 1.488458 Racio: 0.207577\n",
      "valor: 8.683900 ---> Previsão: 7.170642 Diff: 1.513258 Racio: 0.211035\n",
      "valor: 8.700000 ---> Previsão: 7.170642 Diff: 1.529358 Racio: 0.213281\n",
      "valor: 8.723700 ---> Previsão: 7.170642 Diff: 1.553058 Racio: 0.216586\n",
      "valor: 8.679100 ---> Previsão: 7.170642 Diff: 1.508458 Racio: 0.210366\n",
      "valor: 8.501400 ---> Previsão: 7.170642 Diff: 1.330758 Racio: 0.185584\n",
      "valor: 8.498000 ---> Previsão: 7.170642 Diff: 1.327358 Racio: 0.185110\n",
      "valor: 8.396500 ---> Previsão: 7.170642 Diff: 1.225858 Racio: 0.170955\n",
      "valor: 8.351400 ---> Previsão: 7.170642 Diff: 1.180758 Racio: 0.164666\n",
      "valor: 8.385100 ---> Previsão: 7.170642 Diff: 1.214458 Racio: 0.169365\n",
      "valor: 8.406300 ---> Previsão: 7.170642 Diff: 1.235658 Racio: 0.172322\n",
      "valor: 8.498700 ---> Previsão: 7.170642 Diff: 1.328058 Racio: 0.185208\n",
      "valor: 8.494800 ---> Previsão: 7.170642 Diff: 1.324158 Racio: 0.184664\n",
      "valor: 8.478000 ---> Previsão: 7.170642 Diff: 1.307358 Racio: 0.182321\n",
      "valor: 8.567500 ---> Previsão: 7.170642 Diff: 1.396858 Racio: 0.194802\n",
      "valor: 8.525700 ---> Previsão: 7.170642 Diff: 1.355058 Racio: 0.188973\n",
      "valor: 8.489100 ---> Previsão: 7.170642 Diff: 1.318458 Racio: 0.183869\n",
      "valor: 8.451000 ---> Previsão: 7.170642 Diff: 1.280358 Racio: 0.178556\n",
      "valor: 8.421000 ---> Previsão: 7.170642 Diff: 1.250358 Racio: 0.174372\n",
      "valor: 8.417000 ---> Previsão: 7.170642 Diff: 1.246358 Racio: 0.173814\n",
      "valor: 8.398800 ---> Previsão: 7.170642 Diff: 1.228158 Racio: 0.171276\n",
      "valor: 8.414600 ---> Previsão: 7.170642 Diff: 1.243958 Racio: 0.173479\n",
      "valor: 8.401800 ---> Previsão: 7.170642 Diff: 1.231158 Racio: 0.171694\n",
      "valor: 8.551300 ---> Previsão: 7.170642 Diff: 1.380658 Racio: 0.192543\n",
      "valor: 8.539900 ---> Previsão: 7.170642 Diff: 1.369258 Racio: 0.190953\n",
      "valor: 8.565100 ---> Previsão: 7.170642 Diff: 1.394458 Racio: 0.194468\n",
      "valor: 8.600800 ---> Previsão: 7.170642 Diff: 1.430158 Racio: 0.199446\n",
      "valor: 8.589500 ---> Previsão: 7.170642 Diff: 1.418858 Racio: 0.197870\n",
      "valor: 8.789300 ---> Previsão: 7.170642 Diff: 1.618658 Racio: 0.225734\n",
      "valor: 8.888400 ---> Previsão: 7.170642 Diff: 1.717758 Racio: 0.239554\n",
      "valor: 8.891400 ---> Previsão: 7.170642 Diff: 1.720758 Racio: 0.239973\n",
      "valor: 8.914400 ---> Previsão: 7.170642 Diff: 1.743758 Racio: 0.243180\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x151a5128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    visualize_GOOGL()\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    #model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    #print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em \n",
    "    #um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    ''' \n",
    "    MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "    o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           67584     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 118,049\n",
      "Trainable params: 118,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
