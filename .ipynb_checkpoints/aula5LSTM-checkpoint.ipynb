{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/programs/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1 - preparar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fazer o download da sequencias do valor das ações da google GOOGL stock data (fonte yahoo.com)\n",
    "dataset:\n",
    "http://chart.finance.yahoo.com/table.csv?s=GOOGL&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv\n",
    "A função get_stock_data é generica para ir buscar dados à yahoo.com\n",
    "trata-se uma tabela com: ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "Vamos śo utilizar os campos ['Open','High','Close']\n",
    "'''\n",
    "def get_stock_data(stock_name, normalized=0,file_name=None):\n",
    "    if not file_name:\n",
    "        file_name = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "    stocks = pd.read_csv(file_name, header=0, names=col_names) #fica numa especie de tabela exactamente como estava no csv (1350 linhas,7 colunas)\n",
    "    df = pd.DataFrame(stocks) #neste caso não vai fazer nada\n",
    "    date_split = df['Date'].str.split('-').str #não vai servir para nada\n",
    "    df['Year'], df['Month'], df['Day'] = date_split #não vai servir para nada\n",
    "    df[\"Volume\"] = df[\"Volume\"] / 10000 #não vai servir para nada\n",
    "    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) #vou só ficar com as colunas 1,2,4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_GOOGL_stock_dataset():\n",
    "    stock_name = 'GOOGL'\n",
    "    return get_stock_data(stock_name, 0, 'table.csv')\n",
    "\n",
    "def pre_processar_GOOGL_stock_dataset(df):\n",
    "    df['High'] = df['High'] / 100\n",
    "    df['Open'] = df['Open'] / 100\n",
    "    df['Close'] = df['Close'] / 100\n",
    "    return df\n",
    "\n",
    "# Visualizar os top registos da tabela\n",
    "def visualize_GOOGL():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    print('### Antes do pré-processamento ###')\n",
    "    print(df.head()) #mostra só os primeiros 5 registos\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print('### Após o pré-processamento ###')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - tam_sequencia): #numero de registos - tamanho da sequencia\n",
    "        res.append(mat_dados[i: i + tam_sequencia])\n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes (janela deslizante ao longo da serie)\n",
    "    qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    train = res[:qt_casos_treino, :]\n",
    "    x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2 - Definir a topologia da rede (arquitectura do modelo) e compilar '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(janela):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(janela, 3), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(janela, 3), return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]- predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i],predic[i], diff[i], racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_utilizando_GOOGL_data():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    #model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    #print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em \n",
    "    #um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    ''' \n",
    "    MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "    o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Antes do pré-processamento ###\n",
      "         Open        High       Close\n",
      "0  929.000000  935.900024  924.520020\n",
      "1  890.000000  893.380005  891.440002\n",
      "2  891.390015  892.989990  889.140015\n",
      "3  882.260010  892.250000  888.840027\n",
      "4  868.440002  879.960022  878.929993\n",
      "### Após o pré-processamento ###\n",
      "     Open    High   Close\n",
      "0  9.2900  9.3590  9.2452\n",
      "1  8.9000  8.9338  8.9144\n",
      "2  8.9139  8.9299  8.8914\n",
      "3  8.8226  8.9225  8.8884\n",
      "4  8.6844  8.7996  8.7893\n",
      "df (1350, 3)\n",
      "X_train (1194, 22, 3)\n",
      "y_train (1194,)\n",
      "X_test (133, 22, 3)\n",
      "y_test (133,)\n",
      "Train on 1074 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "1074/1074 [==============================] - 2s 2ms/step - loss: 54.6266 - acc: 0.0000e+00 - val_loss: 57.6716 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 54.2142 - acc: 0.0000e+00 - val_loss: 57.1725 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 53.7332 - acc: 0.0000e+00 - val_loss: 56.5840 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 53.1533 - acc: 0.0000e+00 - val_loss: 55.8294 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 52.3968 - acc: 0.0000e+00 - val_loss: 54.8007 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 51.3722 - acc: 0.0000e+00 - val_loss: 53.4331 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 50.0400 - acc: 0.0000e+00 - val_loss: 51.8248 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 48.5042 - acc: 0.0000e+00 - val_loss: 50.0456 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 46.8176 - acc: 0.0000e+00 - val_loss: 48.2066 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 45.0435 - acc: 0.0000e+00 - val_loss: 46.1755 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 43.0980 - acc: 0.0000e+00 - val_loss: 43.9466 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 40.9892 - acc: 0.0000e+00 - val_loss: 41.5694 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 38.7378 - acc: 0.0000e+00 - val_loss: 39.0427 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 36.3501 - acc: 0.0000e+00 - val_loss: 36.3453 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 33.8126 - acc: 0.0000e+00 - val_loss: 33.5168 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 31.1898 - acc: 0.0000e+00 - val_loss: 30.6241 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 28.4968 - acc: 0.0000e+00 - val_loss: 27.6803 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 25.7883 - acc: 0.0000e+00 - val_loss: 24.7167 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 23.0848 - acc: 0.0000e+00 - val_loss: 21.7815 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 20.4209 - acc: 0.0000e+00 - val_loss: 18.9195 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 17.8478 - acc: 0.0000e+00 - val_loss: 16.1677 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 15.4251 - acc: 0.0000e+00 - val_loss: 13.5726 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 13.1460 - acc: 0.0000e+00 - val_loss: 11.1853 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 11.0920 - acc: 0.0000e+00 - val_loss: 9.0182 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 9.2656 - acc: 0.0000e+00 - val_loss: 7.1083 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 7.7089 - acc: 0.0000e+00 - val_loss: 5.4570 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 6.3911 - acc: 0.0000e+00 - val_loss: 4.0788 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 5.3355 - acc: 0.0000e+00 - val_loss: 2.9650 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 4.5185 - acc: 0.0000e+00 - val_loss: 2.0993 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.9387 - acc: 0.0000e+00 - val_loss: 1.4456 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.5385 - acc: 0.0000e+00 - val_loss: 0.9793 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.2987 - acc: 0.0000e+00 - val_loss: 0.6635 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1728 - acc: 0.0000e+00 - val_loss: 0.4633 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1217 - acc: 0.0000e+00 - val_loss: 0.3459 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.2795 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1202 - acc: 0.0000e+00 - val_loss: 0.2404 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1318 - acc: 0.0000e+00 - val_loss: 0.2194 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1439 - acc: 0.0000e+00 - val_loss: 0.2125 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1483 - acc: 0.0000e+00 - val_loss: 0.2129 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1474 - acc: 0.0000e+00 - val_loss: 0.2182 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1430 - acc: 0.0000e+00 - val_loss: 0.2270 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1366 - acc: 0.0000e+00 - val_loss: 0.2341 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1320 - acc: 0.0000e+00 - val_loss: 0.2412 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1282 - acc: 0.0000e+00 - val_loss: 0.2508 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1242 - acc: 0.0000e+00 - val_loss: 0.2598 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1210 - acc: 0.0000e+00 - val_loss: 0.2651 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1191 - acc: 0.0000e+00 - val_loss: 0.2702 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.2732 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1169 - acc: 0.0000e+00 - val_loss: 0.2742 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1163 - acc: 0.0000e+00 - val_loss: 0.2794 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1151 - acc: 0.0000e+00 - val_loss: 0.2872 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.2951 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3016 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3097 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3183 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3226 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3279 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3348 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3341 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "1074/1074 [==============================] - 0s 216us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3315 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3351 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3404 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3314 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3235 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3114 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.2997 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2951 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.2971 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3161 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3300 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3353 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3389 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3447 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3490 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3479 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3443 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3381 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3265 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3205 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3190 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3177 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3155 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3227 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3398 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3493 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3455 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3428 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3445 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3606 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3675 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3801 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3884 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3830 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3773 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3718 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3596 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3406 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3525 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3469 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3361 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3057 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2985 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.2993 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2979 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3014 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3079 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3125 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3191 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3282 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3493 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3607 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3722 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3733 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3456 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3255 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3172 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3040 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3039 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3139 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3186 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3171 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3164 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3209 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3273 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3465 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3589 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3556 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3316 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3182 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3110 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3031 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3074 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3176 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3346 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3346 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3215 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3098 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3161 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3363 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3610 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3721 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3760 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3759 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3511 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3295 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3100 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2927 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2910 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3873 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3569 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3063 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3014 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.2975 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.2944 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.2902 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2913 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.3032 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3688 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3665 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3611 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3498 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3492 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3349 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3366 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3431 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3535 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3609 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3513 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3358 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3426 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3432 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3478 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3502 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "1074/1074 [==============================] - 0s 190us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3276 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3270 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3642 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3882 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3745 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3493 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3372 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3423 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3604 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3687 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3674 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3583 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "1074/1074 [==============================] - 0s 192us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3494 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3283 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3046 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2854 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1153 - acc: 0.0000e+00 - val_loss: 0.2760 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1153 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3095 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "1074/1074 [==============================] - 0s 191us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3914 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "1074/1074 [==============================] - 0s 196us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3997 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.4029 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "1074/1074 [==============================] - 0s 195us/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.4032 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "1074/1074 [==============================] - 0s 194us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.3973 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "1074/1074 [==============================] - 0s 193us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3858 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3644 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3241 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3243 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3464 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3656 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3807 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3761 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3694 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3744 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "1074/1074 [==============================] - 0s 215us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3595 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3545 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3707 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3641 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3309 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3031 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2877 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.2849 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2929 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.2999 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3055 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3231 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3576 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3486 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3369 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3198 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.2947 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.2751 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1178 - acc: 0.0000e+00 - val_loss: 0.2712 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1167 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3261 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3700 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3947 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.4049 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3875 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3725 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3402 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3334 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3185 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3165 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3446 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3568 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "1074/1074 [==============================] - 0s 215us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3585 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3735 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3805 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3599 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3521 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3380 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3325 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "1074/1074 [==============================] - 0s 203us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3399 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3531 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3735 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3742 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3596 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3415 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3175 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3038 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.2989 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3140 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3597 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3704 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3624 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3669 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3636 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3523 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3309 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3417 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3619 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3910 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4114 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1135 - acc: 0.0000e+00 - val_loss: 0.4134 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.4191 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4175 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.3999 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3803 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3387 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3050 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "1074/1074 [==============================] - 0s 249us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.2907 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "1074/1074 [==============================] - 0s 248us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.2914 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "1074/1074 [==============================] - 0s 257us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3081 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3251 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "1074/1074 [==============================] - 0s 250us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "1074/1074 [==============================] - 0s 259us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3515 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "1074/1074 [==============================] - 0s 254us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "1074/1074 [==============================] - 0s 254us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3682 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "1074/1074 [==============================] - 0s 255us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3573 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3320 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "1074/1074 [==============================] - 0s 261us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3252 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "1074/1074 [==============================] - 0s 260us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3383 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "1074/1074 [==============================] - 0s 252us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3274 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "1074/1074 [==============================] - 0s 242us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3069 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2917 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.2730 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1171 - acc: 0.0000e+00 - val_loss: 0.2745 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1159 - acc: 0.0000e+00 - val_loss: 0.2961 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "1074/1074 [==============================] - 0s 219us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1072 - acc: 0.0000e+00 - val_loss: 0.3579 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3846 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3889 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "1074/1074 [==============================] - 0s 229us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3999 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3959 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3702 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3623 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "1074/1074 [==============================] - 0s 228us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3554 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3450 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3374 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3247 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3211 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3153 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3073 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3471 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3746 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3950 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3932 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3668 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3504 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3433 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3397 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3567 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3696 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3603 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3365 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.2841 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1148 - acc: 0.0000e+00 - val_loss: 0.2801 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2900 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3027 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3366 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3251 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3028 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3021 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3033 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3335 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3445 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3301 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3323 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "1074/1074 [==============================] - 0s 214us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3681 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3836 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3983 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "1074/1074 [==============================] - 0s 211us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.4154 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4293 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1162 - acc: 0.0000e+00 - val_loss: 0.4231 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "1074/1074 [==============================] - 0s 213us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3351 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3034 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1143 - acc: 0.0000e+00 - val_loss: 0.2823 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.2943 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3243 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3647 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3783 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3887 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "1074/1074 [==============================] - 0s 197us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3964 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3915 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3283 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.2868 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1168 - acc: 0.0000e+00 - val_loss: 0.2601 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "1074/1074 [==============================] - 0s 202us/step - loss: 3.1208 - acc: 0.0000e+00 - val_loss: 0.2656 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2783 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "1074/1074 [==============================] - 0s 198us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3614 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500\n",
      "1074/1074 [==============================] - 0s 199us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3714 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "1074/1074 [==============================] - 0s 200us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "1074/1074 [==============================] - 0s 201us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3401 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "1074/1074 [==============================] - 0s 204us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3382 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "1074/1074 [==============================] - 0s 208us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3547 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3585 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3621 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3763 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3510 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3394 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "1074/1074 [==============================] - 0s 212us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3306 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "1074/1074 [==============================] - 0s 209us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3414 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3407 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3480 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3721 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "1074/1074 [==============================] - 0s 210us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3803 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3880 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3881 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3327 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "1074/1074 [==============================] - 0s 257us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3075 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3027 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "1074/1074 [==============================] - 0s 261us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.3189 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "1074/1074 [==============================] - 0s 259us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "1074/1074 [==============================] - 0s 255us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3830 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "1074/1074 [==============================] - 0s 253us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.4024 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "1074/1074 [==============================] - 0s 256us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4048 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "1074/1074 [==============================] - 0s 257us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "1074/1074 [==============================] - 0s 258us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3678 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "1074/1074 [==============================] - 0s 269us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3179 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "1074/1074 [==============================] - 0s 251us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.2824 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "1074/1074 [==============================] - 0s 268us/step - loss: 3.1160 - acc: 0.0000e+00 - val_loss: 0.2677 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "1074/1074 [==============================] - 0s 259us/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.2878 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "1074/1074 [==============================] - 0s 226us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3287 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3835 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.4124 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.4159 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "1074/1074 [==============================] - 0s 231us/step - loss: 3.1140 - acc: 0.0000e+00 - val_loss: 0.3935 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "1074/1074 [==============================] - 0s 236us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "1074/1074 [==============================] - 0s 232us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "1074/1074 [==============================] - 0s 222us/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3084 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "1074/1074 [==============================] - 0s 227us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2831 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "1074/1074 [==============================] - 0s 220us/step - loss: 3.1155 - acc: 0.0000e+00 - val_loss: 0.2728 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "1074/1074 [==============================] - 0s 221us/step - loss: 3.1165 - acc: 0.0000e+00 - val_loss: 0.2874 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "1074/1074 [==============================] - 0s 233us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.3181 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3558 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "1074/1074 [==============================] - 0s 230us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "1074/1074 [==============================] - 0s 223us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "1074/1074 [==============================] - 0s 224us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.4136 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "1074/1074 [==============================] - 0s 225us/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.4318 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "1074/1074 [==============================] - 0s 207us/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.4282 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "1074/1074 [==============================] - 0s 206us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.3867 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "1074/1074 [==============================] - 0s 205us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 2.83 MSE (1.68 RMSE)\n",
      "Test Score: 1.28 MSE (1.13 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 8.068400 ---> Previsão: 7.170642 Diff: 0.897758 Racio: 0.125199\n",
      "valor: 8.214900 ---> Previsão: 7.170642 Diff: 1.044258 Racio: 0.145630\n",
      "valor: 8.268400 ---> Previsão: 7.170642 Diff: 1.097758 Racio: 0.153091\n",
      "valor: 8.216300 ---> Previsão: 7.170642 Diff: 1.045658 Racio: 0.145825\n",
      "valor: 8.240600 ---> Previsão: 7.170642 Diff: 1.069958 Racio: 0.149214\n",
      "valor: 8.357400 ---> Previsão: 7.170642 Diff: 1.186758 Racio: 0.165502\n",
      "valor: 8.285500 ---> Previsão: 7.170642 Diff: 1.114858 Racio: 0.155475\n",
      "valor: 8.221000 ---> Previsão: 7.170642 Diff: 1.050357 Racio: 0.146480\n",
      "valor: 8.173500 ---> Previsão: 7.170642 Diff: 1.002857 Racio: 0.139856\n",
      "valor: 8.195600 ---> Previsão: 7.170642 Diff: 1.024958 Racio: 0.142938\n",
      "valor: 8.099000 ---> Previsão: 7.170642 Diff: 0.928358 Racio: 0.129466\n",
      "valor: 8.054800 ---> Previsão: 7.170642 Diff: 0.884157 Racio: 0.123302\n",
      "valor: 7.884200 ---> Previsão: 7.170642 Diff: 0.713557 Racio: 0.099511\n",
      "valor: 7.821900 ---> Previsão: 7.170642 Diff: 0.651258 Racio: 0.090823\n",
      "valor: 7.811000 ---> Previsão: 7.170642 Diff: 0.640357 Racio: 0.089303\n",
      "valor: 8.020300 ---> Previsão: 7.170642 Diff: 0.849658 Racio: 0.118491\n",
      "valor: 8.119800 ---> Previsão: 7.170642 Diff: 0.949157 Racio: 0.132367\n",
      "valor: 8.055900 ---> Previsão: 7.170642 Diff: 0.885258 Racio: 0.123456\n",
      "valor: 7.802900 ---> Previsão: 7.170642 Diff: 0.632257 Racio: 0.088173\n",
      "valor: 7.717500 ---> Previsão: 7.170642 Diff: 0.546858 Racio: 0.076263\n",
      "valor: 7.532200 ---> Previsão: 7.170642 Diff: 0.361557 Racio: 0.050422\n",
      "valor: 7.751600 ---> Previsão: 7.170642 Diff: 0.580957 Racio: 0.081019\n",
      "valor: 7.799800 ---> Previsão: 7.170642 Diff: 0.629157 Racio: 0.087741\n",
      "valor: 7.861600 ---> Previsão: 7.170642 Diff: 0.690957 Racio: 0.096359\n",
      "valor: 7.759700 ---> Previsão: 7.170642 Diff: 0.589057 Racio: 0.082148\n",
      "valor: 7.848000 ---> Previsão: 7.170642 Diff: 0.677358 Racio: 0.094463\n",
      "valor: 7.850000 ---> Previsão: 7.170642 Diff: 0.679358 Racio: 0.094742\n",
      "valor: 7.790000 ---> Previsão: 7.170642 Diff: 0.619358 Racio: 0.086374\n",
      "valor: 7.802300 ---> Previsão: 7.170642 Diff: 0.631657 Racio: 0.088089\n",
      "valor: 7.857900 ---> Previsão: 7.170642 Diff: 0.687257 Racio: 0.095843\n",
      "valor: 7.894400 ---> Previsão: 7.170642 Diff: 0.723758 Racio: 0.100933\n",
      "valor: 7.758800 ---> Previsão: 7.170642 Diff: 0.588158 Racio: 0.082023\n",
      "valor: 7.643300 ---> Previsão: 7.170642 Diff: 0.472658 Racio: 0.065916\n",
      "valor: 7.644600 ---> Previsão: 7.170642 Diff: 0.473958 Racio: 0.066097\n",
      "valor: 7.782200 ---> Previsão: 7.170642 Diff: 0.611557 Racio: 0.085286\n",
      "valor: 7.761800 ---> Previsão: 7.170642 Diff: 0.591158 Racio: 0.082441\n",
      "valor: 7.914700 ---> Previsão: 7.170642 Diff: 0.744057 Racio: 0.103764\n",
      "valor: 7.951700 ---> Previsão: 7.170642 Diff: 0.781057 Racio: 0.108924\n",
      "valor: 8.094500 ---> Previsão: 7.170642 Diff: 0.923858 Racio: 0.128839\n",
      "valor: 8.079000 ---> Previsão: 7.170642 Diff: 0.908358 Racio: 0.126677\n",
      "valor: 8.153400 ---> Previsão: 7.170642 Diff: 0.982758 Racio: 0.137053\n",
      "valor: 8.178900 ---> Previsão: 7.170642 Diff: 1.008258 Racio: 0.140609\n",
      "valor: 8.156500 ---> Previsão: 7.170642 Diff: 0.985858 Racio: 0.137485\n",
      "valor: 8.098400 ---> Previsão: 7.170642 Diff: 0.927758 Racio: 0.129383\n",
      "valor: 8.125000 ---> Previsão: 7.170642 Diff: 0.954358 Racio: 0.133092\n",
      "valor: 8.152000 ---> Previsão: 7.170642 Diff: 0.981358 Racio: 0.136858\n",
      "valor: 8.122000 ---> Previsão: 7.170642 Diff: 0.951358 Racio: 0.132674\n",
      "valor: 8.096800 ---> Previsão: 7.170642 Diff: 0.926158 Racio: 0.129160\n",
      "valor: 8.078000 ---> Previsão: 7.170642 Diff: 0.907358 Racio: 0.126538\n",
      "valor: 8.099300 ---> Previsão: 7.170642 Diff: 0.928658 Racio: 0.129508\n",
      "valor: 8.045700 ---> Previsão: 7.170642 Diff: 0.875058 Racio: 0.122033\n",
      "valor: 8.028800 ---> Previsão: 7.170642 Diff: 0.858158 Racio: 0.119677\n",
      "valor: 7.924500 ---> Previsão: 7.170642 Diff: 0.753858 Racio: 0.105131\n",
      "valor: 8.080100 ---> Previsão: 7.170642 Diff: 0.909458 Racio: 0.126831\n",
      "valor: 8.077700 ---> Previsão: 7.170642 Diff: 0.907058 Racio: 0.126496\n",
      "valor: 8.130200 ---> Previsão: 7.170642 Diff: 0.959558 Racio: 0.133818\n",
      "valor: 8.252100 ---> Previsão: 7.170642 Diff: 1.081458 Racio: 0.150817\n",
      "valor: 8.271800 ---> Previsão: 7.170642 Diff: 1.101158 Racio: 0.153565\n",
      "valor: 8.260100 ---> Previsão: 7.170642 Diff: 1.089458 Racio: 0.151933\n",
      "valor: 8.298600 ---> Previsão: 7.170642 Diff: 1.127957 Racio: 0.157302\n",
      "valor: 8.295300 ---> Previsão: 7.170642 Diff: 1.124658 Racio: 0.156842\n",
      "valor: 8.309400 ---> Previsão: 7.170642 Diff: 1.138758 Racio: 0.158808\n",
      "valor: 8.274600 ---> Previsão: 7.170642 Diff: 1.103958 Racio: 0.153955\n",
      "valor: 8.290200 ---> Previsão: 7.170642 Diff: 1.119558 Racio: 0.156131\n",
      "valor: 8.243700 ---> Previsão: 7.170642 Diff: 1.073058 Racio: 0.149646\n",
      "valor: 8.281700 ---> Previsão: 7.170642 Diff: 1.111057 Racio: 0.154945\n",
      "valor: 8.444300 ---> Previsão: 7.170642 Diff: 1.273658 Racio: 0.177621\n",
      "valor: 8.495300 ---> Previsão: 7.170642 Diff: 1.324658 Racio: 0.184734\n",
      "valor: 8.584500 ---> Previsão: 7.170642 Diff: 1.413858 Racio: 0.197173\n",
      "valor: 8.569800 ---> Previsão: 7.170642 Diff: 1.399157 Racio: 0.195123\n",
      "valor: 8.450300 ---> Previsão: 7.170642 Diff: 1.279658 Racio: 0.178458\n",
      "valor: 8.238300 ---> Previsão: 7.170642 Diff: 1.067658 Racio: 0.148893\n",
      "valor: 8.201900 ---> Previsão: 7.170642 Diff: 1.031258 Racio: 0.143817\n",
      "valor: 8.152400 ---> Previsão: 7.170642 Diff: 0.981758 Racio: 0.136913\n",
      "valor: 8.182600 ---> Previsão: 7.170642 Diff: 1.011958 Racio: 0.141125\n",
      "valor: 8.201300 ---> Previsão: 7.170642 Diff: 1.030658 Racio: 0.143733\n",
      "valor: 8.216200 ---> Previsão: 7.170642 Diff: 1.045558 Racio: 0.145811\n",
      "valor: 8.292300 ---> Previsão: 7.170642 Diff: 1.121657 Racio: 0.156424\n",
      "valor: 8.298800 ---> Previsão: 7.170642 Diff: 1.128158 Racio: 0.157330\n",
      "valor: 8.300600 ---> Previsão: 7.170642 Diff: 1.129958 Racio: 0.157581\n",
      "valor: 8.348500 ---> Previsão: 7.170642 Diff: 1.177857 Racio: 0.164261\n",
      "valor: 8.389600 ---> Previsão: 7.170642 Diff: 1.218958 Racio: 0.169993\n",
      "valor: 8.400300 ---> Previsão: 7.170642 Diff: 1.229658 Racio: 0.171485\n",
      "valor: 8.373200 ---> Previsão: 7.170642 Diff: 1.202558 Racio: 0.167706\n",
      "valor: 8.421700 ---> Previsão: 7.170642 Diff: 1.251057 Racio: 0.174469\n",
      "valor: 8.465500 ---> Previsão: 7.170642 Diff: 1.294858 Racio: 0.180578\n",
      "valor: 8.492700 ---> Previsão: 7.170642 Diff: 1.322058 Racio: 0.184371\n",
      "valor: 8.513600 ---> Previsão: 7.170642 Diff: 1.342957 Racio: 0.187286\n",
      "valor: 8.510000 ---> Previsão: 7.170642 Diff: 1.339358 Racio: 0.186783\n",
      "valor: 8.478100 ---> Previsão: 7.170642 Diff: 1.307458 Racio: 0.182335\n",
      "valor: 8.496700 ---> Previsão: 7.170642 Diff: 1.326057 Racio: 0.184929\n",
      "valor: 8.449300 ---> Previsão: 7.170642 Diff: 1.278658 Racio: 0.178318\n",
      "valor: 8.567500 ---> Previsão: 7.170642 Diff: 1.396858 Racio: 0.194802\n",
      "valor: 8.498500 ---> Previsão: 7.170642 Diff: 1.327857 Racio: 0.185180\n",
      "valor: 8.490800 ---> Previsão: 7.170642 Diff: 1.320158 Racio: 0.184106\n",
      "valor: 8.472700 ---> Previsão: 7.170642 Diff: 1.302058 Racio: 0.181582\n",
      "valor: 8.511500 ---> Previsão: 7.170642 Diff: 1.340858 Racio: 0.186993\n",
      "valor: 8.536400 ---> Previsão: 7.170642 Diff: 1.365758 Racio: 0.190465\n",
      "valor: 8.578400 ---> Previsão: 7.170642 Diff: 1.407758 Racio: 0.196322\n",
      "valor: 8.614100 ---> Previsão: 7.170642 Diff: 1.443457 Racio: 0.201301\n",
      "valor: 8.645800 ---> Previsão: 7.170642 Diff: 1.475158 Racio: 0.205722\n",
      "valor: 8.659100 ---> Previsão: 7.170642 Diff: 1.488457 Racio: 0.207577\n",
      "valor: 8.683900 ---> Previsão: 7.170642 Diff: 1.513258 Racio: 0.211035\n",
      "valor: 8.700000 ---> Previsão: 7.170642 Diff: 1.529358 Racio: 0.213280\n",
      "valor: 8.723700 ---> Previsão: 7.170642 Diff: 1.553058 Racio: 0.216586\n",
      "valor: 8.679100 ---> Previsão: 7.170642 Diff: 1.508457 Racio: 0.210366\n",
      "valor: 8.501400 ---> Previsão: 7.170642 Diff: 1.330758 Racio: 0.185584\n",
      "valor: 8.498000 ---> Previsão: 7.170642 Diff: 1.327358 Racio: 0.185110\n",
      "valor: 8.396500 ---> Previsão: 7.170642 Diff: 1.225858 Racio: 0.170955\n",
      "valor: 8.351400 ---> Previsão: 7.170642 Diff: 1.180758 Racio: 0.164666\n",
      "valor: 8.385100 ---> Previsão: 7.170642 Diff: 1.214458 Racio: 0.169365\n",
      "valor: 8.406300 ---> Previsão: 7.170642 Diff: 1.235658 Racio: 0.172322\n",
      "valor: 8.498700 ---> Previsão: 7.170642 Diff: 1.328058 Racio: 0.185208\n",
      "valor: 8.494800 ---> Previsão: 7.170642 Diff: 1.324157 Racio: 0.184664\n",
      "valor: 8.478000 ---> Previsão: 7.170642 Diff: 1.307358 Racio: 0.182321\n",
      "valor: 8.567500 ---> Previsão: 7.170642 Diff: 1.396858 Racio: 0.194802\n",
      "valor: 8.525700 ---> Previsão: 7.170642 Diff: 1.355058 Racio: 0.188973\n",
      "valor: 8.489100 ---> Previsão: 7.170642 Diff: 1.318457 Racio: 0.183869\n",
      "valor: 8.451000 ---> Previsão: 7.170642 Diff: 1.280357 Racio: 0.178555\n",
      "valor: 8.421000 ---> Previsão: 7.170642 Diff: 1.250357 Racio: 0.174372\n",
      "valor: 8.417000 ---> Previsão: 7.170642 Diff: 1.246358 Racio: 0.173814\n",
      "valor: 8.398800 ---> Previsão: 7.170642 Diff: 1.228158 Racio: 0.171276\n",
      "valor: 8.414600 ---> Previsão: 7.170642 Diff: 1.243958 Racio: 0.173479\n",
      "valor: 8.401800 ---> Previsão: 7.170642 Diff: 1.231158 Racio: 0.171694\n",
      "valor: 8.551300 ---> Previsão: 7.170642 Diff: 1.380658 Racio: 0.192543\n",
      "valor: 8.539900 ---> Previsão: 7.170642 Diff: 1.369258 Racio: 0.190953\n",
      "valor: 8.565100 ---> Previsão: 7.170642 Diff: 1.394458 Racio: 0.194468\n",
      "valor: 8.600800 ---> Previsão: 7.170642 Diff: 1.430158 Racio: 0.199446\n",
      "valor: 8.589500 ---> Previsão: 7.170642 Diff: 1.418858 Racio: 0.197870\n",
      "valor: 8.789300 ---> Previsão: 7.170642 Diff: 1.618658 Racio: 0.225734\n",
      "valor: 8.888400 ---> Previsão: 7.170642 Diff: 1.717758 Racio: 0.239554\n",
      "valor: 8.891400 ---> Previsão: 7.170642 Diff: 1.720758 Racio: 0.239973\n",
      "valor: 8.914400 ---> Previsão: 7.170642 Diff: 1.743758 Racio: 0.243180\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4f6f905f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    visualize_GOOGL()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
