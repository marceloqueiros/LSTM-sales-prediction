{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "janela = 6 #tamanho da Janela deslizante\n",
    "\n",
    "problem_name = 'lstm_sales' #to save the models\n",
    "model_architecture = 'VGG_16'\n",
    "weights_path = None \n",
    "target_size = (224, 224) \n",
    "batch_size = 1\n",
    "\n",
    "epochs = 100 #após x épocas sem melhorar pará (a usar callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding####################\n",
    "from keras.preprocessing import sequence\n",
    "from keras.constraints import maxnorm \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "from keras import backend as K \n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import keras\n",
    "K.set_image_dim_ordering('tf') #ordem 'th' ou 'tf' \n",
    "from numpy import genfromtxt\n",
    "import math \n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import time as tick\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle \n",
    "from os import listdir\n",
    "from PIL import Image, ImageOps\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from scipy.misc\timport toimage \n",
    "from scipy import misc, ndimage\n",
    "import scipy.fftpack as pack\n",
    "import scipy.misc\n",
    "from scipy.ndimage import rotate\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pathlib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - janela): #numero de registos - tamanho da sequencia\n",
    "        res.append(mat_dados[i: i + tam_sequencia])\n",
    "    \n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes (janela deslizante ao longo da serie)\n",
    "\n",
    "    #qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    \n",
    "    qt_casos_treino = 24 # 2 anos\n",
    "    #qt_casos_test = 12 - janela\n",
    "    \n",
    "    x_train = res[:qt_casos_treino, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = res[:qt_casos_treino, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]- predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i], predic[i], diff[i], racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def print_model(model,fich):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file=fich, show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "def print_history_accuracy(history):\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def print_history_loss(history):\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "\n",
    "def load_batch(fpath, label_key='labels'): \n",
    " \n",
    "    f = open(fpath, 'rb') \n",
    "    d = pickle.load(f, encoding='bytes') \n",
    "    d_decoded = {}        # decode utf8 \n",
    "    for k, v in d.items(): \n",
    "        d_decoded[k.decode('utf8')] = v \n",
    "    d = d_decoded \n",
    "    f.close() \n",
    "    data = d['data'] \n",
    "    labels = d[label_key] \n",
    "    data = data.reshape(data.shape[0], 3, 32, 32) \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def rotate_resize(temp, tam_image):\n",
    "    #-------------------rodar se necessário e cortar em quadrado\n",
    "    if temp.shape[0] > temp.shape[1]:\n",
    "        temp = rotate(temp,90)\n",
    "    \n",
    "    #cortar em quadrado no centro da imagem e fazer resize para o tam_image\n",
    "    difShapes = temp.shape[1]-temp.shape[0]\n",
    "    return (255 * resize(temp[0:temp.shape[0],int(difShapes/2):int(difShapes/2)+temp.shape[0]],\n",
    "                            (tam_image, tam_image))).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_pre_process():\n",
    "    file_name = 'advertising-and-sales-data-36-co.csv'\n",
    "    col_names = 'date', 'pub', 'sales'\n",
    "    dataset = pd.read_csv(file_name, sep = ';', header=0, names=col_names) #3 colunas\n",
    "    df = pd.DataFrame(dataset)\n",
    "    date_split = df['date'].str.split('-').str\n",
    "    df['year'], df['month'] = date_split #acrescentar ano e mes separados\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True) #eliminar data original\n",
    "\n",
    "    df = df[:-1] #eliminar a ultima linha porque é uma frase informativa\n",
    "\n",
    "    #vamos passar ano e mes para para strings para não ser interpretado como valores\n",
    "\n",
    "    look_up = {'1': 'First', '2': 'Second', '3': 'Third'}\n",
    "    #df['year'] = df['year'].apply(lambda x: look_up[x])\n",
    "\n",
    "    look_up = {'01': 'Jan', '02': 'Feb', '03': 'Mar', '04': 'Apr', '05': 'May',\n",
    "                '06': 'Jun', '07': 'Jul', '08': 'Aug', '09': 'Sep', '10': 'Oct', '11': 'Nov', '12': 'Dec'}\n",
    "\n",
    "    #df['month'] = df['month'].apply(lambda x: look_up[x])\n",
    "\n",
    "    df = df[['year', 'month', 'pub', 'sales']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(janela, nmr_parametros):\n",
    "    #embedding_vecor_length = 32\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(janela, nmr_parametros)))\n",
    "    model.add(LSTM(256, input_shape=(janela, nmr_parametros), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(janela, nmr_parametros)))\n",
    "    model.add(LSTM(128, input_shape=(janela, nmr_parametros), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(janela, nmr_parametros)))\n",
    "    model.add(LSTM(64, input_shape=(janela, nmr_parametros), return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='sgd',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history_loss = LossHistory() #print(history.losses) to use      \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='min')    \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = 'checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False, mode='min', period=1)\n",
    "\n",
    "#reduce training rate when no improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "csv_logger = CSVLogger('training.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month   pub  sales\n",
      "0     1    01  12.0   15.0\n",
      "1     1    02  20.5   16.0\n",
      "2     1    03  21.0   18.0\n",
      "3     1    04  15.5   27.0\n",
      "4     1    05  15.3   21.0\n",
      "5     1    06  23.5   49.0\n",
      "6     1    07  24.5   21.0\n",
      "7     1    08  21.3   22.0\n",
      "8     1    09  23.5   28.0\n",
      "9     1    10  28.0   36.0\n",
      "10    1    11  24.0   40.0\n",
      "11    1    12  15.5    3.0\n",
      "12    2    01  17.3   21.0\n",
      "13    2    02  25.3   29.0\n",
      "14    2    03  25.0   62.0\n",
      "15    2    04  36.5   65.0\n",
      "16    2    05  36.5   46.0\n",
      "17    2    06  29.6   44.0\n",
      "18    2    07  30.5   33.0\n",
      "19    2    08  28.0   62.0\n",
      "20    2    09  26.0   22.0\n",
      "21    2    10  21.5   12.0\n",
      "22    2    11  19.7   24.0\n",
      "23    2    12  19.0    3.0\n",
      "24    3    01  16.0    5.0\n",
      "25    3    02  20.7   14.0\n",
      "26    3    03  26.5   36.0\n",
      "27    3    04  30.6   40.0\n",
      "28    3    05  32.3   49.0\n",
      "29    3    06  29.5    7.0\n",
      "30    3    07  28.3   52.0\n",
      "31    3    08  31.3   65.0\n",
      "32    3    09  32.2   17.0\n",
      "33    3    10  26.4    5.0\n",
      "34    3    11  23.4   17.0\n",
      "35    3    12  16.4    1.0\n",
      "X_train (24, 6, 4)\n",
      "y_train (24,)\n",
      "X_test (6, 6, 4)\n",
      "y_test (6,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_26 (Batc (None, 6, 4)              16        \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 6, 256)            267264    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 6, 256)            1024      \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 6, 128)            197120    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 6, 128)            512       \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 516,401\n",
      "Trainable params: 515,625\n",
      "Non-trainable params: 776\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21 samples, validate on 3 samples\n",
      "Epoch 1/1000\n",
      "21/21 [==============================] - 5s 231ms/step - loss: 1223.2429 - acc: 0.0000e+00 - val_loss: 1311.3829 - val_acc: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1186.4554 - acc: 0.0000e+00 - val_loss: 1273.5847 - val_acc: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1150.8900 - acc: 0.0000e+00 - val_loss: 1235.1161 - val_acc: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1112.8018 - acc: 0.0000e+00 - val_loss: 1188.4398 - val_acc: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1065.4885 - acc: 0.0476 - val_loss: 1103.6382 - val_acc: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 982.3908 - acc: 0.0476 - val_loss: 874.3495 - val_acc: 0.3333\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 743.9650 - acc: 0.0000e+00 - val_loss: 224.6149 - val_acc: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 391.5266 - acc: 0.0000e+00 - val_loss: 770.2188 - val_acc: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 849.8290 - acc: 0.0000e+00 - val_loss: 529.9178 - val_acc: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 445.4660 - acc: 0.0952 - val_loss: 570.6291 - val_acc: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 619.4194 - acc: 0.0000e+00 - val_loss: 1097.0601 - val_acc: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 987.1703 - acc: 0.0000e+00 - val_loss: 1013.8729 - val_acc: 0.3333\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 931.5436 - acc: 0.0476 - val_loss: 818.8338 - val_acc: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 720.0042 - acc: 0.0000e+00 - val_loss: 271.4480 - val_acc: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 294.9740 - acc: 0.0000e+00 - val_loss: 385.0297 - val_acc: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 339.0475 - acc: 0.0000e+00 - val_loss: 1018.5500 - val_acc: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 380.6706 - acc: 0.0000e+00 - val_loss: 964.2830 - val_acc: 0.3333\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 847.7775 - acc: 0.0000e+00 - val_loss: 578.9189 - val_acc: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 549.1646 - acc: 0.0000e+00 - val_loss: 651.0164 - val_acc: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 348.0255 - acc: 0.0952 - val_loss: 420.3515 - val_acc: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 445.3189 - acc: 0.0000e+00 - val_loss: 318.2295 - val_acc: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 276.3288 - acc: 0.0000e+00 - val_loss: 364.9849 - val_acc: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 303.4863 - acc: 0.0000e+00 - val_loss: 287.8231 - val_acc: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 209.1565 - acc: 0.0476 - val_loss: 443.0670 - val_acc: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 282.8837 - acc: 0.0476 - val_loss: 515.2195 - val_acc: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 539.9601 - acc: 0.0000e+00 - val_loss: 452.0066 - val_acc: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 198.6851 - acc: 0.0476 - val_loss: 379.6355 - val_acc: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 349.5456 - acc: 0.0000e+00 - val_loss: 1336.2941 - val_acc: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 168.8449 - acc: 0.0000e+00 - val_loss: 335.2269 - val_acc: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 372.4022 - acc: 0.0476 - val_loss: 635.6696 - val_acc: 0.3333\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 122.1334 - acc: 0.0476 - val_loss: 498.7447 - val_acc: 0.3333\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 122.5634 - acc: 0.0000e+00 - val_loss: 1304.6022 - val_acc: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 152.1105 - acc: 0.0000e+00 - val_loss: 387.1585 - val_acc: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 217.8694 - acc: 0.0000e+00 - val_loss: 1176.2291 - val_acc: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 117.0333 - acc: 0.0476 - val_loss: 265.7550 - val_acc: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 254.7702 - acc: 0.0000e+00 - val_loss: 703.9138 - val_acc: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 200.2170 - acc: 0.0000e+00 - val_loss: 717.9261 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 488.6796 - acc: 0.0000e+00 - val_loss: 216.7589 - val_acc: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 229.6195 - acc: 0.0000e+00 - val_loss: 297.8550 - val_acc: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 149.2555 - acc: 0.0476 - val_loss: 406.3057 - val_acc: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 130.9140 - acc: 0.0000e+00 - val_loss: 706.9758 - val_acc: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 148.7506 - acc: 0.0476 - val_loss: 367.5097 - val_acc: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 360.2708 - acc: 0.0476 - val_loss: 273.1328 - val_acc: 0.3333\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 148.7009 - acc: 0.0952 - val_loss: 305.6259 - val_acc: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 167.6095 - acc: 0.0000e+00 - val_loss: 279.7927 - val_acc: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 134.2477 - acc: 0.1429 - val_loss: 579.2922 - val_acc: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 130.6948 - acc: 0.0952 - val_loss: 169.4333 - val_acc: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 153.7304 - acc: 0.0000e+00 - val_loss: 829.7537 - val_acc: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 160.0146 - acc: 0.0000e+00 - val_loss: 475.3473 - val_acc: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 252.0305 - acc: 0.0000e+00 - val_loss: 358.2803 - val_acc: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 99.7088 - acc: 0.0952 - val_loss: 147.5841 - val_acc: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 150.8586 - acc: 0.0476 - val_loss: 656.2221 - val_acc: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 122.5967 - acc: 0.1429 - val_loss: 228.9120 - val_acc: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 138.9553 - acc: 0.0476 - val_loss: 624.8046 - val_acc: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 41.9887 - acc: 0.0476 - val_loss: 629.5021 - val_acc: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.1545 - acc: 0.0952 - val_loss: 485.6740 - val_acc: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 46.4870 - acc: 0.0000e+00 - val_loss: 343.9655 - val_acc: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 56.4160 - acc: 0.0000e+00 - val_loss: 478.5990 - val_acc: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 77.6545 - acc: 0.1429 - val_loss: 339.7257 - val_acc: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 48.0300 - acc: 0.0000e+00 - val_loss: 481.6102 - val_acc: 0.3333\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 38.7653 - acc: 0.0000e+00 - val_loss: 697.2297 - val_acc: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 55.1965 - acc: 0.0476 - val_loss: 287.9041 - val_acc: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 71.7743 - acc: 0.0000e+00 - val_loss: 542.7083 - val_acc: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.2783 - acc: 0.0000e+00 - val_loss: 439.2897 - val_acc: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 70.9469 - acc: 0.0952 - val_loss: 319.5836 - val_acc: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 90.0304 - acc: 0.0476 - val_loss: 636.3398 - val_acc: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 90.7943 - acc: 0.0000e+00 - val_loss: 424.0696 - val_acc: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 41.9589 - acc: 0.0000e+00 - val_loss: 623.6971 - val_acc: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 69.3858 - acc: 0.0476 - val_loss: 350.2625 - val_acc: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 116.2658 - acc: 0.0952 - val_loss: 461.8510 - val_acc: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 106.3300 - acc: 0.1429 - val_loss: 394.0020 - val_acc: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 96.4173 - acc: 0.0476 - val_loss: 843.4974 - val_acc: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 118.3147 - acc: 0.0476 - val_loss: 291.4155 - val_acc: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 141.1786 - acc: 0.0000e+00 - val_loss: 535.6911 - val_acc: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 76.4849 - acc: 0.0476 - val_loss: 288.6241 - val_acc: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 71.7251 - acc: 0.1905 - val_loss: 1108.4425 - val_acc: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 149.0280 - acc: 0.0476 - val_loss: 242.2172 - val_acc: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 137.2522 - acc: 0.0000e+00 - val_loss: 1146.7054 - val_acc: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 67.0436 - acc: 0.1905 - val_loss: 299.1455 - val_acc: 0.3333\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 165.5288 - acc: 0.0476 - val_loss: 1112.7974 - val_acc: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 122.8738 - acc: 0.0000e+00 - val_loss: 245.6199 - val_acc: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 306.0219 - acc: 0.0952 - val_loss: 822.4222 - val_acc: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 136.3133 - acc: 0.0000e+00 - val_loss: 690.9541 - val_acc: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 117.4531 - acc: 0.0000e+00 - val_loss: 382.0718 - val_acc: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 137.3127 - acc: 0.0000e+00 - val_loss: 716.1716 - val_acc: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 67.9037 - acc: 0.0476 - val_loss: 350.1592 - val_acc: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 91.1011 - acc: 0.0000e+00 - val_loss: 503.0973 - val_acc: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 86.8842 - acc: 0.0952 - val_loss: 430.9929 - val_acc: 0.3333\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 81.9016 - acc: 0.0000e+00 - val_loss: 696.4517 - val_acc: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 73.2255 - acc: 0.0476 - val_loss: 233.8689 - val_acc: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 106.2283 - acc: 0.0000e+00 - val_loss: 616.6365 - val_acc: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 56.6183 - acc: 0.0476 - val_loss: 303.6978 - val_acc: 0.3333\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 108.9993 - acc: 0.0000e+00 - val_loss: 889.9483 - val_acc: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 60.2075 - acc: 0.0952 - val_loss: 442.1541 - val_acc: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 46.0519 - acc: 0.0952 - val_loss: 444.0168 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 48.3462 - acc: 0.0952 - val_loss: 646.1553 - val_acc: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 58.7285 - acc: 0.0476 - val_loss: 528.4681 - val_acc: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 53.1364 - acc: 0.0000e+00 - val_loss: 727.8428 - val_acc: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 49.0847 - acc: 0.0952 - val_loss: 676.5756 - val_acc: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 43.2640 - acc: 0.0952 - val_loss: 583.9142 - val_acc: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 35.5911 - acc: 0.0000e+00 - val_loss: 489.6906 - val_acc: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 31.2594 - acc: 0.0476 - val_loss: 937.0561 - val_acc: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 67.1797 - acc: 0.0000e+00 - val_loss: 440.9019 - val_acc: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.6987 - acc: 0.0000e+00 - val_loss: 636.2231 - val_acc: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.9792 - acc: 0.1905 - val_loss: 625.8610 - val_acc: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.0585 - acc: 0.0476 - val_loss: 429.7431 - val_acc: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 56.6739 - acc: 0.0476 - val_loss: 689.0836 - val_acc: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 88.7516 - acc: 0.0952 - val_loss: 387.6137 - val_acc: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 204.9415 - acc: 0.0476 - val_loss: 751.3671 - val_acc: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 69.3475 - acc: 0.0000e+00 - val_loss: 254.4104 - val_acc: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 98.9867 - acc: 0.0000e+00 - val_loss: 918.8132 - val_acc: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 73.8853 - acc: 0.0476 - val_loss: 381.4336 - val_acc: 0.3333\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 90.3047 - acc: 0.0000e+00 - val_loss: 705.6917 - val_acc: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 147.8241 - acc: 0.0000e+00 - val_loss: 304.8827 - val_acc: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 218.3747 - acc: 0.0000e+00 - val_loss: 546.3188 - val_acc: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 42.9649 - acc: 0.0476 - val_loss: 627.5837 - val_acc: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 44.9328 - acc: 0.0952 - val_loss: 667.9306 - val_acc: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 43.9332 - acc: 0.0476 - val_loss: 745.8186 - val_acc: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 46.4493 - acc: 0.0000e+00 - val_loss: 550.2943 - val_acc: 0.3333\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 61.2744 - acc: 0.0952 - val_loss: 775.8641 - val_acc: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 32.3815 - acc: 0.0476 - val_loss: 456.2583 - val_acc: 0.3333\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 48.6688 - acc: 0.0476 - val_loss: 624.9197 - val_acc: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 35.5734 - acc: 0.0000e+00 - val_loss: 454.9753 - val_acc: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 38.0009 - acc: 0.0000e+00 - val_loss: 754.0508 - val_acc: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 102.3594 - acc: 0.1429 - val_loss: 324.0767 - val_acc: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 227.5140 - acc: 0.0952 - val_loss: 358.9893 - val_acc: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 53.5051 - acc: 0.0476 - val_loss: 442.9914 - val_acc: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.1680 - acc: 0.0952 - val_loss: 531.8855 - val_acc: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.9459 - acc: 0.1429 - val_loss: 344.8302 - val_acc: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.4655 - acc: 0.0952 - val_loss: 472.7863 - val_acc: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.1408 - acc: 0.1429 - val_loss: 443.6644 - val_acc: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.6737 - acc: 0.1429 - val_loss: 370.3050 - val_acc: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.8280 - acc: 0.0476 - val_loss: 395.0579 - val_acc: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.4206 - acc: 0.1429 - val_loss: 617.9095 - val_acc: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.4419 - acc: 0.1429 - val_loss: 333.6872 - val_acc: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 68.9316 - acc: 0.0000e+00 - val_loss: 696.8972 - val_acc: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.2008 - acc: 0.0000e+00 - val_loss: 554.7003 - val_acc: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.5656 - acc: 0.0952 - val_loss: 538.7324 - val_acc: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.2163 - acc: 0.0476 - val_loss: 417.7943 - val_acc: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.0932 - acc: 0.0952 - val_loss: 501.4019 - val_acc: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.7572 - acc: 0.0476 - val_loss: 385.4049 - val_acc: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.6942 - acc: 0.0952 - val_loss: 459.3144 - val_acc: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.3285 - acc: 0.0952 - val_loss: 396.9630 - val_acc: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.3362 - acc: 0.1429 - val_loss: 506.8955 - val_acc: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.0597 - acc: 0.0476 - val_loss: 429.9785 - val_acc: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 16.4683 - acc: 0.0476 - val_loss: 421.5201 - val_acc: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.8478 - acc: 0.0476 - val_loss: 352.3597 - val_acc: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 34.9658 - acc: 0.0952 - val_loss: 361.1347 - val_acc: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.5021 - acc: 0.1429 - val_loss: 421.1703 - val_acc: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.6465 - acc: 0.0476 - val_loss: 664.1912 - val_acc: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.0455 - acc: 0.0952 - val_loss: 379.9613 - val_acc: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 35.3605 - acc: 0.1429 - val_loss: 685.2001 - val_acc: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6948 - acc: 0.0952 - val_loss: 443.8884 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 22.7375 - acc: 0.1429 - val_loss: 482.3870 - val_acc: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 26.4914 - acc: 0.0000e+00 - val_loss: 372.2405 - val_acc: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.7174 - acc: 0.0476 - val_loss: 473.0151 - val_acc: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 16.4804 - acc: 0.0000e+00 - val_loss: 500.6544 - val_acc: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.2229 - acc: 0.0952 - val_loss: 483.3228 - val_acc: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.0082 - acc: 0.0952 - val_loss: 406.9790 - val_acc: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 20.4759 - acc: 0.2381 - val_loss: 514.8697 - val_acc: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.7308 - acc: 0.1429 - val_loss: 400.2545 - val_acc: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.2800 - acc: 0.1429 - val_loss: 468.8968 - val_acc: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.6736 - acc: 0.1429 - val_loss: 486.7310 - val_acc: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.0144 - acc: 0.0476 - val_loss: 321.8873 - val_acc: 0.3333\n",
      "Epoch 165/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 43.0101 - acc: 0.0000e+00 - val_loss: 570.8974 - val_acc: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.4304 - acc: 0.0952 - val_loss: 501.4652 - val_acc: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 27.4691 - acc: 0.0952 - val_loss: 482.5655 - val_acc: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 31.8557 - acc: 0.0952 - val_loss: 696.3580 - val_acc: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 44.2957 - acc: 0.0476 - val_loss: 307.2844 - val_acc: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.4939 - acc: 0.0952 - val_loss: 489.4696 - val_acc: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.8995 - acc: 0.0952 - val_loss: 492.5047 - val_acc: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4001 - acc: 0.1429 - val_loss: 546.4878 - val_acc: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.1091 - acc: 0.0952 - val_loss: 565.3125 - val_acc: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.6412 - acc: 0.2381 - val_loss: 412.7598 - val_acc: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.2769 - acc: 0.0000e+00 - val_loss: 468.9911 - val_acc: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.5199 - acc: 0.0476 - val_loss: 557.8839 - val_acc: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.4694 - acc: 0.1429 - val_loss: 511.4553 - val_acc: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.0646 - acc: 0.0476 - val_loss: 516.2422 - val_acc: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.4558 - acc: 0.0476 - val_loss: 487.8708 - val_acc: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.3376 - acc: 0.1429 - val_loss: 437.9992 - val_acc: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.9308 - acc: 0.1905 - val_loss: 566.2342 - val_acc: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.7278 - acc: 0.0476 - val_loss: 520.1068 - val_acc: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.6140 - acc: 0.1905 - val_loss: 558.3818 - val_acc: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8189 - acc: 0.1429 - val_loss: 432.3834 - val_acc: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.4298 - acc: 0.1429 - val_loss: 510.9014 - val_acc: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.1652 - acc: 0.1905 - val_loss: 525.9028 - val_acc: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.7540 - acc: 0.0476 - val_loss: 682.2246 - val_acc: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.3438 - acc: 0.0476 - val_loss: 421.8255 - val_acc: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 31.3956 - acc: 0.0476 - val_loss: 566.1154 - val_acc: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.2316 - acc: 0.0952 - val_loss: 536.7088 - val_acc: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.5285 - acc: 0.1429 - val_loss: 359.0454 - val_acc: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.7829 - acc: 0.0476 - val_loss: 719.5232 - val_acc: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 23.4732 - acc: 0.1429 - val_loss: 435.1699 - val_acc: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.2426 - acc: 0.1429 - val_loss: 502.9179 - val_acc: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.2836 - acc: 0.0476 - val_loss: 397.1869 - val_acc: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 39.8597 - acc: 0.0952 - val_loss: 680.1554 - val_acc: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6862 - acc: 0.0952 - val_loss: 387.4336 - val_acc: 0.3333\n",
      "Epoch 198/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 27.2910 - acc: 0.0476 - val_loss: 669.4984 - val_acc: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7629 - acc: 0.1429 - val_loss: 635.2235 - val_acc: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.2166 - acc: 0.0952 - val_loss: 764.8997 - val_acc: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.0063 - acc: 0.0952 - val_loss: 724.4626 - val_acc: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 39.8450 - acc: 0.2381 - val_loss: 311.8153 - val_acc: 0.3333\n",
      "Epoch 203/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 45.9429 - acc: 0.0000e+00 - val_loss: 662.0440 - val_acc: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.0300 - acc: 0.1905 - val_loss: 383.9793 - val_acc: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7675 - acc: 0.0952 - val_loss: 517.6260 - val_acc: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.3479 - acc: 0.1429 - val_loss: 523.4222 - val_acc: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.4284 - acc: 0.0476 - val_loss: 687.3118 - val_acc: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6621 - acc: 0.0952 - val_loss: 398.5519 - val_acc: 0.3333\n",
      "Epoch 209/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6338 - acc: 0.0476 - val_loss: 552.7374 - val_acc: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.8927 - acc: 0.0952 - val_loss: 415.6316 - val_acc: 0.3333\n",
      "Epoch 211/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.1014 - acc: 0.2381 - val_loss: 573.1198 - val_acc: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1905 - acc: 0.0952 - val_loss: 548.3752 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4091 - acc: 0.0952 - val_loss: 477.3826 - val_acc: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9698 - acc: 0.0952 - val_loss: 555.7303 - val_acc: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.8637 - acc: 0.0476 - val_loss: 490.8631 - val_acc: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.7810 - acc: 0.0952 - val_loss: 522.4395 - val_acc: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.6402 - acc: 0.1429 - val_loss: 395.0357 - val_acc: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.0495 - acc: 0.0476 - val_loss: 586.7087 - val_acc: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.4424 - acc: 0.0952 - val_loss: 548.5190 - val_acc: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7484 - acc: 0.1429 - val_loss: 439.5224 - val_acc: 0.3333\n",
      "Epoch 221/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.9798 - acc: 0.0476 - val_loss: 635.3392 - val_acc: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.0776 - acc: 0.0952 - val_loss: 385.6945 - val_acc: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 50.2881 - acc: 0.0952 - val_loss: 779.7374 - val_acc: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.0458 - acc: 0.0000e+00 - val_loss: 425.9283 - val_acc: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9199 - acc: 0.1429 - val_loss: 477.5814 - val_acc: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.7808 - acc: 0.0952 - val_loss: 621.4297 - val_acc: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9020 - acc: 0.0952 - val_loss: 614.7611 - val_acc: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.3084 - acc: 0.2381 - val_loss: 383.6042 - val_acc: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.2846 - acc: 0.0476 - val_loss: 592.5262 - val_acc: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.6450 - acc: 0.0952 - val_loss: 770.1426 - val_acc: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.0300 - acc: 0.0952 - val_loss: 485.3217 - val_acc: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.4964 - acc: 0.0952 - val_loss: 618.2661 - val_acc: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7285 - acc: 0.1429 - val_loss: 584.7372 - val_acc: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.0273 - acc: 0.1429 - val_loss: 751.1408 - val_acc: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.2385 - acc: 0.2857 - val_loss: 583.9801 - val_acc: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.5226 - acc: 0.0476 - val_loss: 577.8300 - val_acc: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.1492 - acc: 0.1905 - val_loss: 579.6103 - val_acc: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.5001 - acc: 0.0476 - val_loss: 716.6437 - val_acc: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.5168 - acc: 0.1905 - val_loss: 589.3268 - val_acc: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.8038 - acc: 0.1905 - val_loss: 480.2625 - val_acc: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.9331 - acc: 0.3333 - val_loss: 542.6639 - val_acc: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6350 - acc: 0.2381 - val_loss: 547.0782 - val_acc: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3897 - acc: 0.0000e+00 - val_loss: 463.4121 - val_acc: 0.3333\n",
      "Epoch 244/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.5353 - acc: 0.0476 - val_loss: 522.2767 - val_acc: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.0711 - acc: 0.0000e+00 - val_loss: 559.6030 - val_acc: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5576 - acc: 0.2381 - val_loss: 573.6450 - val_acc: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 26.5291 - acc: 0.0476 - val_loss: 747.5219 - val_acc: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 40.8178 - acc: 0.0952 - val_loss: 376.4907 - val_acc: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.2964 - acc: 0.0000e+00 - val_loss: 637.9830 - val_acc: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8862 - acc: 0.0476 - val_loss: 489.1422 - val_acc: 0.3333\n",
      "Epoch 251/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.3853 - acc: 0.1429 - val_loss: 765.8062 - val_acc: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.6017 - acc: 0.0952 - val_loss: 459.7108 - val_acc: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.3392 - acc: 0.2381 - val_loss: 582.1611 - val_acc: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.8586 - acc: 0.1429 - val_loss: 533.3600 - val_acc: 0.3333\n",
      "Epoch 255/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.9786 - acc: 0.0476 - val_loss: 612.4669 - val_acc: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.4845 - acc: 0.0476 - val_loss: 340.8873 - val_acc: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 59.2134 - acc: 0.0952 - val_loss: 629.3452 - val_acc: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.0488 - acc: 0.0952 - val_loss: 480.5729 - val_acc: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.2151 - acc: 0.0952 - val_loss: 663.3207 - val_acc: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.9816 - acc: 0.1429 - val_loss: 555.8977 - val_acc: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.0571 - acc: 0.0952 - val_loss: 445.5715 - val_acc: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2358 - acc: 0.0000e+00 - val_loss: 494.1509 - val_acc: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 15.4343 - acc: 0.1905 - val_loss: 552.4361 - val_acc: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.1451 - acc: 0.0952 - val_loss: 468.5172 - val_acc: 0.3333\n",
      "Epoch 265/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.3761 - acc: 0.0476 - val_loss: 728.5598 - val_acc: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.0834 - acc: 0.0000e+00 - val_loss: 371.1560 - val_acc: 0.3333\n",
      "Epoch 267/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.2034 - acc: 0.0952 - val_loss: 779.3701 - val_acc: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.4242 - acc: 0.0952 - val_loss: 458.1235 - val_acc: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0310 - acc: 0.2381 - val_loss: 573.4946 - val_acc: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.0541 - acc: 0.0952 - val_loss: 613.9841 - val_acc: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.3828 - acc: 0.1905 - val_loss: 672.6270 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.9516 - acc: 0.0476 - val_loss: 390.8189 - val_acc: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6559 - acc: 0.0476 - val_loss: 488.3930 - val_acc: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.1899 - acc: 0.1429 - val_loss: 376.7402 - val_acc: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.5215 - acc: 0.0476 - val_loss: 535.5629 - val_acc: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.8188 - acc: 0.0476 - val_loss: 414.7258 - val_acc: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 26.5605 - acc: 0.0476 - val_loss: 501.1220 - val_acc: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.3414 - acc: 0.1429 - val_loss: 739.3647 - val_acc: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 35.6108 - acc: 0.0952 - val_loss: 338.5104 - val_acc: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.4464 - acc: 0.0476 - val_loss: 587.5249 - val_acc: 0.0000e+00\n",
      "Epoch 281/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.2631 - acc: 0.0000e+00 - val_loss: 522.0340 - val_acc: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.1780 - acc: 0.0476 - val_loss: 658.0987 - val_acc: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.5959 - acc: 0.0952 - val_loss: 447.0522 - val_acc: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.5574 - acc: 0.0000e+00 - val_loss: 613.2628 - val_acc: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1468 - acc: 0.0952 - val_loss: 622.5862 - val_acc: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5630 - acc: 0.1429 - val_loss: 493.7549 - val_acc: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.7376 - acc: 0.0952 - val_loss: 668.1569 - val_acc: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.2813 - acc: 0.1429 - val_loss: 581.1002 - val_acc: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.7777 - acc: 0.0952 - val_loss: 695.7801 - val_acc: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.7252 - acc: 0.0000e+00 - val_loss: 485.7310 - val_acc: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 18.2410 - acc: 0.0952 - val_loss: 581.7929 - val_acc: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.4918 - acc: 0.0952 - val_loss: 552.4590 - val_acc: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.3537 - acc: 0.0952 - val_loss: 543.6565 - val_acc: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4476 - acc: 0.1905 - val_loss: 556.7357 - val_acc: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7439 - acc: 0.0000e+00 - val_loss: 487.5917 - val_acc: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.4059 - acc: 0.0952 - val_loss: 478.8764 - val_acc: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.6556 - acc: 0.0000e+00 - val_loss: 523.4177 - val_acc: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0482 - acc: 0.0476 - val_loss: 537.6711 - val_acc: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7810 - acc: 0.1905 - val_loss: 494.1197 - val_acc: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9209 - acc: 0.2381 - val_loss: 515.0750 - val_acc: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.3697 - acc: 0.1905 - val_loss: 589.7986 - val_acc: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.0809 - acc: 0.2381 - val_loss: 711.5859 - val_acc: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.8402 - acc: 0.0952 - val_loss: 504.2589 - val_acc: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.8529 - acc: 0.0000e+00 - val_loss: 691.9465 - val_acc: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.4908 - acc: 0.0476 - val_loss: 375.6929 - val_acc: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.9841 - acc: 0.0000e+00 - val_loss: 621.5480 - val_acc: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.0331 - acc: 0.0476 - val_loss: 319.5110 - val_acc: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 42.6693 - acc: 0.0000e+00 - val_loss: 567.4153 - val_acc: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 31.7836 - acc: 0.0000e+00 - val_loss: 352.5822 - val_acc: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 34.1789 - acc: 0.0476 - val_loss: 586.7967 - val_acc: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8330 - acc: 0.1429 - val_loss: 530.3954 - val_acc: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.1701 - acc: 0.0952 - val_loss: 506.0590 - val_acc: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.4564 - acc: 0.1429 - val_loss: 461.2385 - val_acc: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3639 - acc: 0.0000e+00 - val_loss: 436.5314 - val_acc: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.3169 - acc: 0.0952 - val_loss: 636.0309 - val_acc: 0.0000e+00\n",
      "Epoch 316/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4880 - acc: 0.0000e+00 - val_loss: 444.4300 - val_acc: 0.3333\n",
      "Epoch 317/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.6194 - acc: 0.0476 - val_loss: 693.0731 - val_acc: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.9901 - acc: 0.0952 - val_loss: 416.0526 - val_acc: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.8793 - acc: 0.1905 - val_loss: 460.1920 - val_acc: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6784 - acc: 0.1429 - val_loss: 531.5288 - val_acc: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.1248 - acc: 0.2381 - val_loss: 497.3579 - val_acc: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7408 - acc: 0.0476 - val_loss: 518.4601 - val_acc: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2161 - acc: 0.2381 - val_loss: 429.8151 - val_acc: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8909 - acc: 0.0952 - val_loss: 501.1505 - val_acc: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5582 - acc: 0.0952 - val_loss: 498.2650 - val_acc: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.3533 - acc: 0.0952 - val_loss: 561.7744 - val_acc: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.4520 - acc: 0.0952 - val_loss: 503.2379 - val_acc: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.0635 - acc: 0.1429 - val_loss: 581.4478 - val_acc: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6980 - acc: 0.1905 - val_loss: 587.7830 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.9025 - acc: 0.1429 - val_loss: 485.4082 - val_acc: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.0853 - acc: 0.0952 - val_loss: 392.9059 - val_acc: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.0447 - acc: 0.0000e+00 - val_loss: 629.9429 - val_acc: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6179 - acc: 0.0952 - val_loss: 458.7501 - val_acc: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.2721 - acc: 0.1905 - val_loss: 577.6089 - val_acc: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.9384 - acc: 0.1429 - val_loss: 439.5832 - val_acc: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8016 - acc: 0.0952 - val_loss: 520.7167 - val_acc: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4709 - acc: 0.2857 - val_loss: 421.7849 - val_acc: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.8115 - acc: 0.0000e+00 - val_loss: 579.3036 - val_acc: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.9878 - acc: 0.1429 - val_loss: 633.7859 - val_acc: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 77.1362 - acc: 0.2381 - val_loss: 399.4834 - val_acc: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.5490 - acc: 0.0000e+00 - val_loss: 654.1573 - val_acc: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.6075 - acc: 0.0476 - val_loss: 484.6271 - val_acc: 0.3333\n",
      "Epoch 343/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.9411 - acc: 0.2381 - val_loss: 661.4180 - val_acc: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4445 - acc: 0.0952 - val_loss: 804.5211 - val_acc: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 37.2393 - acc: 0.0476 - val_loss: 472.9092 - val_acc: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.0714 - acc: 0.0476 - val_loss: 608.4686 - val_acc: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3399 - acc: 0.0476 - val_loss: 386.7700 - val_acc: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.3912 - acc: 0.0952 - val_loss: 682.7484 - val_acc: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.6206 - acc: 0.0476 - val_loss: 461.2393 - val_acc: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.7440 - acc: 0.0476 - val_loss: 715.1011 - val_acc: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.0434 - acc: 0.0476 - val_loss: 526.9473 - val_acc: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.0745 - acc: 0.0476 - val_loss: 570.1074 - val_acc: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.4223 - acc: 0.0952 - val_loss: 528.0416 - val_acc: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.9351 - acc: 0.1905 - val_loss: 614.1618 - val_acc: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.1754 - acc: 0.1429 - val_loss: 488.3664 - val_acc: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.7449 - acc: 0.0952 - val_loss: 622.1802 - val_acc: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.2719 - acc: 0.2857 - val_loss: 756.4224 - val_acc: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8760 - acc: 0.0476 - val_loss: 471.8562 - val_acc: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9703 - acc: 0.1429 - val_loss: 409.4487 - val_acc: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4847 - acc: 0.0476 - val_loss: 506.4122 - val_acc: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.4131 - acc: 0.0476 - val_loss: 696.3376 - val_acc: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6699 - acc: 0.0476 - val_loss: 500.2049 - val_acc: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 35.8403 - acc: 0.1429 - val_loss: 379.3832 - val_acc: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 31.9058 - acc: 0.0476 - val_loss: 636.1448 - val_acc: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.5934 - acc: 0.0476 - val_loss: 539.1105 - val_acc: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.6599 - acc: 0.1905 - val_loss: 434.4355 - val_acc: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.2942 - acc: 0.2381 - val_loss: 719.9978 - val_acc: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.1734 - acc: 0.0952 - val_loss: 518.8535 - val_acc: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.2354 - acc: 0.0000e+00 - val_loss: 626.9692 - val_acc: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.7264 - acc: 0.1905 - val_loss: 440.3196 - val_acc: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.3711 - acc: 0.1429 - val_loss: 610.4538 - val_acc: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7803 - acc: 0.0476 - val_loss: 492.0051 - val_acc: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5048 - acc: 0.0952 - val_loss: 615.1694 - val_acc: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7499 - acc: 0.1429 - val_loss: 611.2010 - val_acc: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4538 - acc: 0.1429 - val_loss: 685.7519 - val_acc: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5214 - acc: 0.1429 - val_loss: 452.6426 - val_acc: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.2461 - acc: 0.0476 - val_loss: 588.3340 - val_acc: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.8175 - acc: 0.0952 - val_loss: 405.3899 - val_acc: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.1623 - acc: 0.0952 - val_loss: 534.7916 - val_acc: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6103 - acc: 0.2381 - val_loss: 440.8491 - val_acc: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.9426 - acc: 0.0476 - val_loss: 607.6501 - val_acc: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.5181 - acc: 0.1905 - val_loss: 502.4084 - val_acc: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0882 - acc: 0.0952 - val_loss: 494.9347 - val_acc: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.0429 - acc: 0.1905 - val_loss: 572.3270 - val_acc: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0631 - acc: 0.1905 - val_loss: 500.0041 - val_acc: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.0840 - acc: 0.1905 - val_loss: 489.5734 - val_acc: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.3464 - acc: 0.0476 - val_loss: 478.9019 - val_acc: 0.0000e+00\n",
      "Epoch 388/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6626 - acc: 0.1429 - val_loss: 718.8441 - val_acc: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.7187 - acc: 0.0952 - val_loss: 514.6403 - val_acc: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9753 - acc: 0.0476 - val_loss: 694.8322 - val_acc: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1612 - acc: 0.1905 - val_loss: 489.6065 - val_acc: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9773 - acc: 0.0000e+00 - val_loss: 606.0635 - val_acc: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8788 - acc: 0.2857 - val_loss: 518.0110 - val_acc: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6241 - acc: 0.0952 - val_loss: 500.3562 - val_acc: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.3086 - acc: 0.1905 - val_loss: 483.6909 - val_acc: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.4617 - acc: 0.1905 - val_loss: 587.4894 - val_acc: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.5120 - acc: 0.0476 - val_loss: 796.5098 - val_acc: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.7012 - acc: 0.0952 - val_loss: 567.2751 - val_acc: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.1945 - acc: 0.1905 - val_loss: 634.6578 - val_acc: 0.0000e+00\n",
      "Epoch 400/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.0532 - acc: 0.1905 - val_loss: 647.9124 - val_acc: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5599 - acc: 0.0476 - val_loss: 783.0754 - val_acc: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.7127 - acc: 0.1429 - val_loss: 446.2650 - val_acc: 0.3333\n",
      "Epoch 403/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.0756 - acc: 0.0476 - val_loss: 693.9541 - val_acc: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5822 - acc: 0.0952 - val_loss: 580.9094 - val_acc: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.6081 - acc: 0.2381 - val_loss: 747.4637 - val_acc: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 23.6812 - acc: 0.1429 - val_loss: 485.9583 - val_acc: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9136 - acc: 0.2857 - val_loss: 633.1348 - val_acc: 0.0000e+00\n",
      "Epoch 408/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7352 - acc: 0.0952 - val_loss: 650.5187 - val_acc: 0.0000e+00\n",
      "Epoch 409/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7836 - acc: 0.0000e+00 - val_loss: 609.7711 - val_acc: 0.0000e+00\n",
      "Epoch 410/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6339 - acc: 0.0952 - val_loss: 605.7047 - val_acc: 0.0000e+00\n",
      "Epoch 411/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.5644 - acc: 0.0952 - val_loss: 591.7829 - val_acc: 0.0000e+00\n",
      "Epoch 412/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.6501 - acc: 0.1429 - val_loss: 619.4202 - val_acc: 0.0000e+00\n",
      "Epoch 413/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 29.2497 - acc: 0.1429 - val_loss: 795.4102 - val_acc: 0.0000e+00\n",
      "Epoch 414/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.0481 - acc: 0.1429 - val_loss: 530.9165 - val_acc: 0.0000e+00\n",
      "Epoch 415/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.6192 - acc: 0.2381 - val_loss: 815.5964 - val_acc: 0.0000e+00\n",
      "Epoch 416/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.5257 - acc: 0.0476 - val_loss: 429.0188 - val_acc: 0.0000e+00\n",
      "Epoch 417/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.5340 - acc: 0.0952 - val_loss: 783.2614 - val_acc: 0.0000e+00\n",
      "Epoch 418/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.7845 - acc: 0.0000e+00 - val_loss: 492.0747 - val_acc: 0.0000e+00\n",
      "Epoch 419/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.7868 - acc: 0.0952 - val_loss: 713.4349 - val_acc: 0.0000e+00\n",
      "Epoch 420/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8670 - acc: 0.1429 - val_loss: 685.7137 - val_acc: 0.0000e+00\n",
      "Epoch 421/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.2544 - acc: 0.1905 - val_loss: 617.2822 - val_acc: 0.0000e+00\n",
      "Epoch 422/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 14.8566 - acc: 0.0952 - val_loss: 683.6189 - val_acc: 0.0000e+00\n",
      "Epoch 423/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.2366 - acc: 0.2381 - val_loss: 908.7961 - val_acc: 0.0000e+00\n",
      "Epoch 424/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.5785 - acc: 0.0476 - val_loss: 566.7389 - val_acc: 0.0000e+00\n",
      "Epoch 425/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9499 - acc: 0.0476 - val_loss: 806.1768 - val_acc: 0.0000e+00\n",
      "Epoch 426/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.9507 - acc: 0.0952 - val_loss: 971.3071 - val_acc: 0.0000e+00\n",
      "Epoch 427/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.3249 - acc: 0.0952 - val_loss: 566.7244 - val_acc: 0.0000e+00\n",
      "Epoch 428/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.2247 - acc: 0.0952 - val_loss: 781.4634 - val_acc: 0.0000e+00\n",
      "Epoch 429/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9479 - acc: 0.0952 - val_loss: 611.6592 - val_acc: 0.0000e+00\n",
      "Epoch 430/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.6265 - acc: 0.0000e+00 - val_loss: 597.4429 - val_acc: 0.0000e+00\n",
      "Epoch 431/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.3063 - acc: 0.0476 - val_loss: 785.9822 - val_acc: 0.0000e+00\n",
      "Epoch 432/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5599 - acc: 0.1429 - val_loss: 498.4043 - val_acc: 0.0000e+00\n",
      "Epoch 433/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.6953 - acc: 0.0476 - val_loss: 691.3698 - val_acc: 0.0000e+00\n",
      "Epoch 434/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6645 - acc: 0.0952 - val_loss: 717.0520 - val_acc: 0.0000e+00\n",
      "Epoch 435/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.7888 - acc: 0.1429 - val_loss: 688.0679 - val_acc: 0.0000e+00\n",
      "Epoch 436/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9831 - acc: 0.0952 - val_loss: 618.1603 - val_acc: 0.0000e+00\n",
      "Epoch 437/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1977 - acc: 0.2381 - val_loss: 698.8210 - val_acc: 0.0000e+00\n",
      "Epoch 438/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7120 - acc: 0.1905 - val_loss: 548.3749 - val_acc: 0.0000e+00\n",
      "Epoch 439/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4378 - acc: 0.0476 - val_loss: 633.8867 - val_acc: 0.0000e+00\n",
      "Epoch 440/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6958 - acc: 0.0952 - val_loss: 589.8394 - val_acc: 0.0000e+00\n",
      "Epoch 441/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9527 - acc: 0.1429 - val_loss: 753.3010 - val_acc: 0.0000e+00\n",
      "Epoch 442/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9856 - acc: 0.1905 - val_loss: 672.1702 - val_acc: 0.0000e+00\n",
      "Epoch 443/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.3783 - acc: 0.0952 - val_loss: 735.0362 - val_acc: 0.0000e+00\n",
      "Epoch 444/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.5315 - acc: 0.0952 - val_loss: 536.2155 - val_acc: 0.0000e+00\n",
      "Epoch 445/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.3088 - acc: 0.2381 - val_loss: 648.5759 - val_acc: 0.0000e+00\n",
      "Epoch 446/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1886 - acc: 0.0952 - val_loss: 728.0998 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1434 - acc: 0.1905 - val_loss: 611.9600 - val_acc: 0.0000e+00\n",
      "Epoch 448/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.9803 - acc: 0.0952 - val_loss: 774.9409 - val_acc: 0.0000e+00\n",
      "Epoch 449/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.1903 - acc: 0.0476 - val_loss: 675.7714 - val_acc: 0.0000e+00\n",
      "Epoch 450/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.4283 - acc: 0.0476 - val_loss: 509.1378 - val_acc: 0.0000e+00\n",
      "Epoch 451/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.3551 - acc: 0.1429 - val_loss: 724.2729 - val_acc: 0.0000e+00\n",
      "Epoch 452/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.3841 - acc: 0.1429 - val_loss: 498.4700 - val_acc: 0.0000e+00\n",
      "Epoch 453/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4660 - acc: 0.0476 - val_loss: 627.0950 - val_acc: 0.0000e+00\n",
      "Epoch 454/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9734 - acc: 0.0000e+00 - val_loss: 745.7705 - val_acc: 0.0000e+00\n",
      "Epoch 455/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.3695 - acc: 0.2857 - val_loss: 553.4523 - val_acc: 0.0000e+00\n",
      "Epoch 456/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.7498 - acc: 0.0952 - val_loss: 566.9741 - val_acc: 0.0000e+00\n",
      "Epoch 457/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0828 - acc: 0.0952 - val_loss: 547.1694 - val_acc: 0.0000e+00\n",
      "Epoch 458/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.6487 - acc: 0.0000e+00 - val_loss: 701.8171 - val_acc: 0.0000e+00\n",
      "Epoch 459/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7698 - acc: 0.0952 - val_loss: 567.8019 - val_acc: 0.0000e+00\n",
      "Epoch 460/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1307 - acc: 0.1429 - val_loss: 684.0904 - val_acc: 0.0000e+00\n",
      "Epoch 461/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5202 - acc: 0.0952 - val_loss: 598.2393 - val_acc: 0.0000e+00\n",
      "Epoch 462/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8277 - acc: 0.1905 - val_loss: 669.4341 - val_acc: 0.0000e+00\n",
      "Epoch 463/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.3556 - acc: 0.0952 - val_loss: 737.3011 - val_acc: 0.0000e+00\n",
      "Epoch 464/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6433 - acc: 0.1429 - val_loss: 511.2885 - val_acc: 0.0000e+00\n",
      "Epoch 465/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.7828 - acc: 0.0952 - val_loss: 720.7036 - val_acc: 0.0000e+00\n",
      "Epoch 466/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 14.3527 - acc: 0.1429 - val_loss: 875.2437 - val_acc: 0.0000e+00\n",
      "Epoch 467/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 41.6831 - acc: 0.0000e+00 - val_loss: 384.5108 - val_acc: 0.0000e+00\n",
      "Epoch 468/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 40.3833 - acc: 0.1429 - val_loss: 683.8948 - val_acc: 0.0000e+00\n",
      "Epoch 469/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.8712 - acc: 0.1429 - val_loss: 659.6718 - val_acc: 0.0000e+00\n",
      "Epoch 470/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.4615 - acc: 0.0476 - val_loss: 521.0601 - val_acc: 0.0000e+00\n",
      "Epoch 471/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.5706 - acc: 0.1429 - val_loss: 564.2271 - val_acc: 0.0000e+00\n",
      "Epoch 472/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.7936 - acc: 0.1429 - val_loss: 629.3755 - val_acc: 0.0000e+00\n",
      "Epoch 473/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 31.0723 - acc: 0.0952 - val_loss: 690.3770 - val_acc: 0.0000e+00\n",
      "Epoch 474/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9153 - acc: 0.2857 - val_loss: 595.4178 - val_acc: 0.0000e+00\n",
      "Epoch 475/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.0537 - acc: 0.2381 - val_loss: 483.8739 - val_acc: 0.0000e+00\n",
      "Epoch 476/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.1767 - acc: 0.0000e+00 - val_loss: 565.7019 - val_acc: 0.0000e+00\n",
      "Epoch 477/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.4802 - acc: 0.0476 - val_loss: 794.9131 - val_acc: 0.0000e+00\n",
      "Epoch 478/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.7237 - acc: 0.0000e+00 - val_loss: 568.8867 - val_acc: 0.0000e+00\n",
      "Epoch 479/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.3446 - acc: 0.0476 - val_loss: 643.9979 - val_acc: 0.0000e+00\n",
      "Epoch 480/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.6912 - acc: 0.1429 - val_loss: 798.1521 - val_acc: 0.0000e+00\n",
      "Epoch 481/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.8049 - acc: 0.1429 - val_loss: 652.1198 - val_acc: 0.0000e+00\n",
      "Epoch 482/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6380 - acc: 0.0952 - val_loss: 760.0778 - val_acc: 0.0000e+00\n",
      "Epoch 483/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6656 - acc: 0.1905 - val_loss: 477.7947 - val_acc: 0.0000e+00\n",
      "Epoch 484/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.6567 - acc: 0.0476 - val_loss: 673.1281 - val_acc: 0.0000e+00\n",
      "Epoch 485/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.8033 - acc: 0.0952 - val_loss: 599.3405 - val_acc: 0.0000e+00\n",
      "Epoch 486/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.3297 - acc: 0.0000e+00 - val_loss: 778.3003 - val_acc: 0.0000e+00\n",
      "Epoch 487/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4260 - acc: 0.0952 - val_loss: 733.4814 - val_acc: 0.0000e+00\n",
      "Epoch 488/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4950 - acc: 0.0952 - val_loss: 888.4630 - val_acc: 0.0000e+00\n",
      "Epoch 489/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.4712 - acc: 0.1905 - val_loss: 651.2422 - val_acc: 0.0000e+00\n",
      "Epoch 490/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.1275 - acc: 0.0952 - val_loss: 650.4070 - val_acc: 0.0000e+00\n",
      "Epoch 491/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2889 - acc: 0.0952 - val_loss: 711.9584 - val_acc: 0.0000e+00\n",
      "Epoch 492/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.8126 - acc: 0.0952 - val_loss: 768.1559 - val_acc: 0.0000e+00\n",
      "Epoch 493/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.7022 - acc: 0.0476 - val_loss: 523.5950 - val_acc: 0.0000e+00\n",
      "Epoch 494/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.2421 - acc: 0.2381 - val_loss: 660.4830 - val_acc: 0.0000e+00\n",
      "Epoch 495/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4175 - acc: 0.0952 - val_loss: 714.4805 - val_acc: 0.0000e+00\n",
      "Epoch 496/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5361 - acc: 0.1429 - val_loss: 747.6794 - val_acc: 0.0000e+00\n",
      "Epoch 497/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.8762 - acc: 0.1429 - val_loss: 602.7938 - val_acc: 0.0000e+00\n",
      "Epoch 498/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.7583 - acc: 0.0000e+00 - val_loss: 630.1425 - val_acc: 0.0000e+00\n",
      "Epoch 499/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7118 - acc: 0.0476 - val_loss: 673.7652 - val_acc: 0.0000e+00\n",
      "Epoch 500/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9122 - acc: 0.0952 - val_loss: 729.5633 - val_acc: 0.0000e+00\n",
      "Epoch 501/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4938 - acc: 0.1905 - val_loss: 869.8066 - val_acc: 0.0000e+00\n",
      "Epoch 502/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.3633 - acc: 0.0952 - val_loss: 589.6410 - val_acc: 0.3333\n",
      "Epoch 503/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9366 - acc: 0.0952 - val_loss: 751.9248 - val_acc: 0.0000e+00\n",
      "Epoch 504/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.3822 - acc: 0.0952 - val_loss: 822.8637 - val_acc: 0.0000e+00\n",
      "Epoch 505/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1730 - acc: 0.0000e+00 - val_loss: 792.8956 - val_acc: 0.0000e+00\n",
      "Epoch 506/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9326 - acc: 0.1905 - val_loss: 651.0042 - val_acc: 0.3333\n",
      "Epoch 507/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6979 - acc: 0.2381 - val_loss: 746.0884 - val_acc: 0.0000e+00\n",
      "Epoch 508/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.6920 - acc: 0.2381 - val_loss: 945.4445 - val_acc: 0.0000e+00\n",
      "Epoch 509/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.0202 - acc: 0.1905 - val_loss: 704.0560 - val_acc: 0.0000e+00\n",
      "Epoch 510/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.3196 - acc: 0.2381 - val_loss: 766.8321 - val_acc: 0.0000e+00\n",
      "Epoch 511/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.8944 - acc: 0.1429 - val_loss: 461.9452 - val_acc: 0.3333\n",
      "Epoch 512/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.8225 - acc: 0.0952 - val_loss: 779.2723 - val_acc: 0.0000e+00\n",
      "Epoch 513/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9497 - acc: 0.0952 - val_loss: 957.1981 - val_acc: 0.0000e+00\n",
      "Epoch 514/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.2907 - acc: 0.0952 - val_loss: 530.3552 - val_acc: 0.0000e+00\n",
      "Epoch 515/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6822 - acc: 0.0476 - val_loss: 718.8622 - val_acc: 0.0000e+00\n",
      "Epoch 516/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3947 - acc: 0.2857 - val_loss: 723.7945 - val_acc: 0.0000e+00\n",
      "Epoch 517/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.3832 - acc: 0.0952 - val_loss: 556.4663 - val_acc: 0.0000e+00\n",
      "Epoch 518/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.6887 - acc: 0.1429 - val_loss: 915.7064 - val_acc: 0.0000e+00\n",
      "Epoch 519/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.5470 - acc: 0.0952 - val_loss: 743.7630 - val_acc: 0.0000e+00\n",
      "Epoch 520/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.0680 - acc: 0.2857 - val_loss: 856.2396 - val_acc: 0.0000e+00\n",
      "Epoch 521/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2733 - acc: 0.1905 - val_loss: 869.3867 - val_acc: 0.0000e+00\n",
      "Epoch 522/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.1984 - acc: 0.0952 - val_loss: 729.8096 - val_acc: 0.0000e+00\n",
      "Epoch 523/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.4256 - acc: 0.0952 - val_loss: 692.3330 - val_acc: 0.0000e+00\n",
      "Epoch 524/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.2160 - acc: 0.2381 - val_loss: 827.0479 - val_acc: 0.0000e+00\n",
      "Epoch 525/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0825 - acc: 0.0476 - val_loss: 860.1248 - val_acc: 0.0000e+00\n",
      "Epoch 526/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9419 - acc: 0.1905 - val_loss: 680.4443 - val_acc: 0.0000e+00\n",
      "Epoch 527/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9209 - acc: 0.0476 - val_loss: 660.0272 - val_acc: 0.0000e+00\n",
      "Epoch 528/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.6042 - acc: 0.0476 - val_loss: 781.5777 - val_acc: 0.0000e+00\n",
      "Epoch 529/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.6643 - acc: 0.2857 - val_loss: 600.4382 - val_acc: 0.0000e+00\n",
      "Epoch 530/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.3973 - acc: 0.0476 - val_loss: 870.8364 - val_acc: 0.0000e+00\n",
      "Epoch 531/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9144 - acc: 0.0476 - val_loss: 650.8499 - val_acc: 0.0000e+00\n",
      "Epoch 532/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.0322 - acc: 0.0952 - val_loss: 781.8535 - val_acc: 0.0000e+00\n",
      "Epoch 533/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8889 - acc: 0.0000e+00 - val_loss: 690.7070 - val_acc: 0.0000e+00\n",
      "Epoch 534/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2138 - acc: 0.0952 - val_loss: 678.1321 - val_acc: 0.0000e+00\n",
      "Epoch 535/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.3995 - acc: 0.1429 - val_loss: 735.0500 - val_acc: 0.0000e+00\n",
      "Epoch 536/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.2860 - acc: 0.2857 - val_loss: 972.0347 - val_acc: 0.0000e+00\n",
      "Epoch 537/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4067 - acc: 0.1429 - val_loss: 770.9706 - val_acc: 0.0000e+00\n",
      "Epoch 538/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.0300 - acc: 0.1429 - val_loss: 918.7863 - val_acc: 0.0000e+00\n",
      "Epoch 539/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 32.2998 - acc: 0.0952 - val_loss: 575.9959 - val_acc: 0.0000e+00\n",
      "Epoch 540/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.7184 - acc: 0.0952 - val_loss: 854.1716 - val_acc: 0.0000e+00\n",
      "Epoch 541/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1236 - acc: 0.1429 - val_loss: 857.0753 - val_acc: 0.0000e+00\n",
      "Epoch 542/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.6898 - acc: 0.1905 - val_loss: 726.2138 - val_acc: 0.0000e+00\n",
      "Epoch 543/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1311 - acc: 0.0952 - val_loss: 737.2678 - val_acc: 0.0000e+00\n",
      "Epoch 544/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0982 - acc: 0.0476 - val_loss: 669.5650 - val_acc: 0.0000e+00\n",
      "Epoch 545/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1608 - acc: 0.0952 - val_loss: 820.7045 - val_acc: 0.0000e+00\n",
      "Epoch 546/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.3201 - acc: 0.2381 - val_loss: 571.6216 - val_acc: 0.0000e+00\n",
      "Epoch 547/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5233 - acc: 0.1429 - val_loss: 696.6799 - val_acc: 0.0000e+00\n",
      "Epoch 548/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.1692 - acc: 0.0952 - val_loss: 716.6190 - val_acc: 0.0000e+00\n",
      "Epoch 549/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9100 - acc: 0.0952 - val_loss: 910.1619 - val_acc: 0.0000e+00\n",
      "Epoch 550/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1010 - acc: 0.0952 - val_loss: 785.5903 - val_acc: 0.0000e+00\n",
      "Epoch 551/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0484 - acc: 0.0952 - val_loss: 729.2742 - val_acc: 0.0000e+00\n",
      "Epoch 552/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.2442 - acc: 0.0476 - val_loss: 805.4050 - val_acc: 0.0000e+00\n",
      "Epoch 553/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.0758 - acc: 0.0000e+00 - val_loss: 592.7957 - val_acc: 0.0000e+00\n",
      "Epoch 554/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1288 - acc: 0.0000e+00 - val_loss: 815.1367 - val_acc: 0.0000e+00\n",
      "Epoch 555/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9977 - acc: 0.2857 - val_loss: 852.4011 - val_acc: 0.0000e+00\n",
      "Epoch 556/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7889 - acc: 0.1429 - val_loss: 783.9788 - val_acc: 0.0000e+00\n",
      "Epoch 557/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.3016 - acc: 0.1905 - val_loss: 801.5038 - val_acc: 0.0000e+00\n",
      "Epoch 558/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0786 - acc: 0.1905 - val_loss: 858.8986 - val_acc: 0.0000e+00\n",
      "Epoch 559/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.6588 - acc: 0.3333 - val_loss: 744.9147 - val_acc: 0.3333\n",
      "Epoch 560/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.3994 - acc: 0.2381 - val_loss: 745.3726 - val_acc: 0.0000e+00\n",
      "Epoch 561/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.6253 - acc: 0.1905 - val_loss: 958.7824 - val_acc: 0.0000e+00\n",
      "Epoch 562/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8796 - acc: 0.0476 - val_loss: 830.5108 - val_acc: 0.0000e+00\n",
      "Epoch 563/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.9542 - acc: 0.0952 - val_loss: 960.8586 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 564/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6490 - acc: 0.1429 - val_loss: 696.8702 - val_acc: 0.0000e+00\n",
      "Epoch 565/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4392 - acc: 0.2381 - val_loss: 801.9418 - val_acc: 0.0000e+00\n",
      "Epoch 566/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6984 - acc: 0.0952 - val_loss: 826.4162 - val_acc: 0.0000e+00\n",
      "Epoch 567/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2472 - acc: 0.1429 - val_loss: 860.3500 - val_acc: 0.0000e+00\n",
      "Epoch 568/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.3531 - acc: 0.1429 - val_loss: 766.6693 - val_acc: 0.0000e+00\n",
      "Epoch 569/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.4378 - acc: 0.1905 - val_loss: 948.3691 - val_acc: 0.0000e+00\n",
      "Epoch 570/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.9687 - acc: 0.2381 - val_loss: 672.3568 - val_acc: 0.0000e+00\n",
      "Epoch 571/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.8975 - acc: 0.1429 - val_loss: 966.1354 - val_acc: 0.0000e+00\n",
      "Epoch 572/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9994 - acc: 0.0952 - val_loss: 913.9849 - val_acc: 0.0000e+00\n",
      "Epoch 573/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4708 - acc: 0.1905 - val_loss: 933.5153 - val_acc: 0.0000e+00\n",
      "Epoch 574/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.0569 - acc: 0.1905 - val_loss: 836.3912 - val_acc: 0.0000e+00\n",
      "Epoch 575/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.4618 - acc: 0.0476 - val_loss: 884.7473 - val_acc: 0.0000e+00\n",
      "Epoch 576/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6718 - acc: 0.1905 - val_loss: 766.0807 - val_acc: 0.0000e+00\n",
      "Epoch 577/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1652 - acc: 0.1429 - val_loss: 734.5040 - val_acc: 0.0000e+00\n",
      "Epoch 578/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.0009 - acc: 0.1905 - val_loss: 841.2303 - val_acc: 0.0000e+00\n",
      "Epoch 579/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.2005 - acc: 0.2857 - val_loss: 643.0687 - val_acc: 0.0000e+00\n",
      "Epoch 580/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 15.9112 - acc: 0.0952 - val_loss: 868.3499 - val_acc: 0.0000e+00\n",
      "Epoch 581/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.0445 - acc: 0.0952 - val_loss: 697.3875 - val_acc: 0.0000e+00\n",
      "Epoch 582/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.6437 - acc: 0.0476 - val_loss: 835.3667 - val_acc: 0.0000e+00\n",
      "Epoch 583/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.7312 - acc: 0.1429 - val_loss: 707.4133 - val_acc: 0.0000e+00\n",
      "Epoch 584/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9895 - acc: 0.0952 - val_loss: 719.3382 - val_acc: 0.0000e+00\n",
      "Epoch 585/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1751 - acc: 0.0952 - val_loss: 875.6431 - val_acc: 0.0000e+00\n",
      "Epoch 586/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0878 - acc: 0.1429 - val_loss: 773.4089 - val_acc: 0.0000e+00\n",
      "Epoch 587/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.3835 - acc: 0.0476 - val_loss: 916.4258 - val_acc: 0.0000e+00\n",
      "Epoch 588/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.6328 - acc: 0.1429 - val_loss: 804.8184 - val_acc: 0.0000e+00\n",
      "Epoch 589/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.7144 - acc: 0.1429 - val_loss: 784.2549 - val_acc: 0.0000e+00\n",
      "Epoch 590/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.7595 - acc: 0.0952 - val_loss: 804.3198 - val_acc: 0.0000e+00\n",
      "Epoch 591/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.3139 - acc: 0.2381 - val_loss: 822.2922 - val_acc: 0.0000e+00\n",
      "Epoch 592/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8933 - acc: 0.2857 - val_loss: 658.2204 - val_acc: 0.0000e+00\n",
      "Epoch 593/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.0091 - acc: 0.1429 - val_loss: 969.2427 - val_acc: 0.0000e+00\n",
      "Epoch 594/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.4540 - acc: 0.2857 - val_loss: 841.0093 - val_acc: 0.0000e+00\n",
      "Epoch 595/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.0422 - acc: 0.1905 - val_loss: 722.7076 - val_acc: 0.0000e+00\n",
      "Epoch 596/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.0353 - acc: 0.0000e+00 - val_loss: 871.7420 - val_acc: 0.0000e+00\n",
      "Epoch 597/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.4683 - acc: 0.0952 - val_loss: 609.1378 - val_acc: 0.0000e+00\n",
      "Epoch 598/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.9853 - acc: 0.0952 - val_loss: 922.3246 - val_acc: 0.0000e+00\n",
      "Epoch 599/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1256 - acc: 0.0000e+00 - val_loss: 791.1689 - val_acc: 0.0000e+00\n",
      "Epoch 600/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9058 - acc: 0.1905 - val_loss: 738.8977 - val_acc: 0.0000e+00\n",
      "Epoch 601/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.1927 - acc: 0.0952 - val_loss: 983.7792 - val_acc: 0.0000e+00\n",
      "Epoch 602/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0507 - acc: 0.1429 - val_loss: 789.5286 - val_acc: 0.0000e+00\n",
      "Epoch 603/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7307 - acc: 0.1905 - val_loss: 836.3520 - val_acc: 0.0000e+00\n",
      "Epoch 604/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1206 - acc: 0.3810 - val_loss: 1042.4913 - val_acc: 0.0000e+00\n",
      "Epoch 605/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4742 - acc: 0.0476 - val_loss: 692.4783 - val_acc: 0.0000e+00\n",
      "Epoch 606/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2678 - acc: 0.0000e+00 - val_loss: 688.6704 - val_acc: 0.0000e+00\n",
      "Epoch 607/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4956 - acc: 0.1429 - val_loss: 739.7380 - val_acc: 0.0000e+00\n",
      "Epoch 608/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9237 - acc: 0.2381 - val_loss: 955.7446 - val_acc: 0.0000e+00\n",
      "Epoch 609/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1523 - acc: 0.1429 - val_loss: 793.4861 - val_acc: 0.0000e+00\n",
      "Epoch 610/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5562 - acc: 0.1905 - val_loss: 643.2600 - val_acc: 0.0000e+00\n",
      "Epoch 611/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9976 - acc: 0.0000e+00 - val_loss: 735.1171 - val_acc: 0.0000e+00\n",
      "Epoch 612/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9493 - acc: 0.2381 - val_loss: 884.6199 - val_acc: 0.0000e+00\n",
      "Epoch 613/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5053 - acc: 0.0952 - val_loss: 770.2866 - val_acc: 0.0000e+00\n",
      "Epoch 614/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0938 - acc: 0.0952 - val_loss: 921.1060 - val_acc: 0.0000e+00\n",
      "Epoch 615/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.4393 - acc: 0.0476 - val_loss: 736.8043 - val_acc: 0.0000e+00\n",
      "Epoch 616/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.1783 - acc: 0.1429 - val_loss: 906.6498 - val_acc: 0.0000e+00\n",
      "Epoch 617/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.6605 - acc: 0.1905 - val_loss: 799.2050 - val_acc: 0.0000e+00\n",
      "Epoch 618/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.5387 - acc: 0.0476 - val_loss: 888.1534 - val_acc: 0.0000e+00\n",
      "Epoch 619/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4116 - acc: 0.1429 - val_loss: 642.0648 - val_acc: 0.0000e+00\n",
      "Epoch 620/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1039 - acc: 0.0476 - val_loss: 858.3774 - val_acc: 0.0000e+00\n",
      "Epoch 621/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7487 - acc: 0.1905 - val_loss: 856.4390 - val_acc: 0.0000e+00\n",
      "Epoch 622/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5517 - acc: 0.1905 - val_loss: 684.2884 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4398 - acc: 0.1905 - val_loss: 864.7180 - val_acc: 0.0000e+00\n",
      "Epoch 624/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.3534 - acc: 0.2381 - val_loss: 902.7828 - val_acc: 0.0000e+00\n",
      "Epoch 625/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6773 - acc: 0.0952 - val_loss: 1029.6787 - val_acc: 0.0000e+00\n",
      "Epoch 626/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1188 - acc: 0.1429 - val_loss: 806.1896 - val_acc: 0.0000e+00\n",
      "Epoch 627/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9082 - acc: 0.1429 - val_loss: 848.5357 - val_acc: 0.0000e+00\n",
      "Epoch 628/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.4911 - acc: 0.0952 - val_loss: 775.8633 - val_acc: 0.0000e+00\n",
      "Epoch 629/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.8208 - acc: 0.1429 - val_loss: 748.1912 - val_acc: 0.0000e+00\n",
      "Epoch 630/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 21.4487 - acc: 0.0476 - val_loss: 1013.5124 - val_acc: 0.0000e+00\n",
      "Epoch 631/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.8391 - acc: 0.0476 - val_loss: 471.2266 - val_acc: 0.0000e+00\n",
      "Epoch 632/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 32.0085 - acc: 0.0952 - val_loss: 837.7686 - val_acc: 0.0000e+00\n",
      "Epoch 633/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.2766 - acc: 0.0476 - val_loss: 629.9333 - val_acc: 0.0000e+00\n",
      "Epoch 634/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.6998 - acc: 0.0000e+00 - val_loss: 941.3325 - val_acc: 0.0000e+00\n",
      "Epoch 635/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.9220 - acc: 0.0952 - val_loss: 642.8120 - val_acc: 0.0000e+00\n",
      "Epoch 636/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.5965 - acc: 0.0476 - val_loss: 986.9200 - val_acc: 0.0000e+00\n",
      "Epoch 637/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4935 - acc: 0.0952 - val_loss: 691.4936 - val_acc: 0.0000e+00\n",
      "Epoch 638/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1678 - acc: 0.1429 - val_loss: 603.1359 - val_acc: 0.0000e+00\n",
      "Epoch 639/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.1245 - acc: 0.0952 - val_loss: 881.3714 - val_acc: 0.0000e+00\n",
      "Epoch 640/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.7015 - acc: 0.0952 - val_loss: 687.5168 - val_acc: 0.0000e+00\n",
      "Epoch 641/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.2220 - acc: 0.0476 - val_loss: 890.1689 - val_acc: 0.0000e+00\n",
      "Epoch 642/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6227 - acc: 0.1905 - val_loss: 820.5605 - val_acc: 0.0000e+00\n",
      "Epoch 643/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9962 - acc: 0.1429 - val_loss: 836.3710 - val_acc: 0.0000e+00\n",
      "Epoch 644/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.8605 - acc: 0.1429 - val_loss: 588.7820 - val_acc: 0.0000e+00\n",
      "Epoch 645/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2132 - acc: 0.1429 - val_loss: 863.6236 - val_acc: 0.0000e+00\n",
      "Epoch 646/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.8121 - acc: 0.0952 - val_loss: 636.3209 - val_acc: 0.0000e+00\n",
      "Epoch 647/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4278 - acc: 0.0000e+00 - val_loss: 714.0833 - val_acc: 0.0000e+00\n",
      "Epoch 648/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9874 - acc: 0.1905 - val_loss: 742.5596 - val_acc: 0.0000e+00\n",
      "Epoch 649/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7937 - acc: 0.1429 - val_loss: 785.7108 - val_acc: 0.0000e+00\n",
      "Epoch 650/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.4405 - acc: 0.1429 - val_loss: 720.2842 - val_acc: 0.0000e+00\n",
      "Epoch 651/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2588 - acc: 0.1429 - val_loss: 896.2388 - val_acc: 0.0000e+00\n",
      "Epoch 652/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6629 - acc: 0.1905 - val_loss: 976.5689 - val_acc: 0.0000e+00\n",
      "Epoch 653/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4396 - acc: 0.0000e+00 - val_loss: 836.6154 - val_acc: 0.0000e+00\n",
      "Epoch 654/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8972 - acc: 0.0952 - val_loss: 674.7474 - val_acc: 0.0000e+00\n",
      "Epoch 655/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.5075 - acc: 0.1429 - val_loss: 688.9710 - val_acc: 0.0000e+00\n",
      "Epoch 656/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.9829 - acc: 0.1905 - val_loss: 942.3758 - val_acc: 0.0000e+00\n",
      "Epoch 657/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7132 - acc: 0.2381 - val_loss: 846.1079 - val_acc: 0.0000e+00\n",
      "Epoch 658/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1299 - acc: 0.1905 - val_loss: 849.3113 - val_acc: 0.0000e+00\n",
      "Epoch 659/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.9954 - acc: 0.0476 - val_loss: 679.4229 - val_acc: 0.0000e+00\n",
      "Epoch 660/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.8511 - acc: 0.0476 - val_loss: 1021.0079 - val_acc: 0.0000e+00\n",
      "Epoch 661/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4223 - acc: 0.0000e+00 - val_loss: 899.1733 - val_acc: 0.0000e+00\n",
      "Epoch 662/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.2466 - acc: 0.0476 - val_loss: 877.3032 - val_acc: 0.0000e+00\n",
      "Epoch 663/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4822 - acc: 0.1905 - val_loss: 739.6938 - val_acc: 0.0000e+00\n",
      "Epoch 664/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2034 - acc: 0.2381 - val_loss: 805.2982 - val_acc: 0.0000e+00\n",
      "Epoch 665/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.5809 - acc: 0.1905 - val_loss: 575.7118 - val_acc: 0.3333\n",
      "Epoch 666/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.0697 - acc: 0.3333 - val_loss: 788.3635 - val_acc: 0.0000e+00\n",
      "Epoch 667/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.9701 - acc: 0.1429 - val_loss: 784.2109 - val_acc: 0.0000e+00\n",
      "Epoch 668/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.8182 - acc: 0.3333 - val_loss: 820.0769 - val_acc: 0.0000e+00\n",
      "Epoch 669/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.9245 - acc: 0.1905 - val_loss: 790.5821 - val_acc: 0.0000e+00\n",
      "Epoch 670/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8213 - acc: 0.0476 - val_loss: 720.5036 - val_acc: 0.0000e+00\n",
      "Epoch 671/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.3951 - acc: 0.0952 - val_loss: 850.4434 - val_acc: 0.0000e+00\n",
      "Epoch 672/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.8133 - acc: 0.1429 - val_loss: 864.5486 - val_acc: 0.0000e+00\n",
      "Epoch 673/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5714 - acc: 0.2857 - val_loss: 700.5978 - val_acc: 0.0000e+00\n",
      "Epoch 674/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.9374 - acc: 0.1905 - val_loss: 1032.3961 - val_acc: 0.0000e+00\n",
      "Epoch 675/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.3365 - acc: 0.0952 - val_loss: 557.0391 - val_acc: 0.0000e+00\n",
      "Epoch 676/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 43.4693 - acc: 0.0476 - val_loss: 1040.4807 - val_acc: 0.0000e+00\n",
      "Epoch 677/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.7351 - acc: 0.1429 - val_loss: 721.4009 - val_acc: 0.0000e+00\n",
      "Epoch 678/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7529 - acc: 0.0952 - val_loss: 730.0327 - val_acc: 0.0000e+00\n",
      "Epoch 679/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5253 - acc: 0.0952 - val_loss: 899.2380 - val_acc: 0.0000e+00\n",
      "Epoch 680/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7040 - acc: 0.0952 - val_loss: 573.4426 - val_acc: 0.0000e+00\n",
      "Epoch 681/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5047 - acc: 0.1429 - val_loss: 814.5527 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 682/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.2967 - acc: 0.2381 - val_loss: 685.7575 - val_acc: 0.0000e+00\n",
      "Epoch 683/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5565 - acc: 0.1429 - val_loss: 818.3499 - val_acc: 0.0000e+00\n",
      "Epoch 684/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8142 - acc: 0.1905 - val_loss: 766.8690 - val_acc: 0.0000e+00\n",
      "Epoch 685/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9484 - acc: 0.2381 - val_loss: 793.8642 - val_acc: 0.0000e+00\n",
      "Epoch 686/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7542 - acc: 0.1905 - val_loss: 762.2686 - val_acc: 0.0000e+00\n",
      "Epoch 687/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8267 - acc: 0.1429 - val_loss: 804.8640 - val_acc: 0.0000e+00\n",
      "Epoch 688/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7918 - acc: 0.0952 - val_loss: 796.4543 - val_acc: 0.0000e+00\n",
      "Epoch 689/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.5928 - acc: 0.0476 - val_loss: 712.9514 - val_acc: 0.0000e+00\n",
      "Epoch 690/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2435 - acc: 0.1429 - val_loss: 733.7795 - val_acc: 0.0000e+00\n",
      "Epoch 691/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0407 - acc: 0.1429 - val_loss: 681.9487 - val_acc: 0.0000e+00\n",
      "Epoch 692/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7299 - acc: 0.1429 - val_loss: 757.9370 - val_acc: 0.0000e+00\n",
      "Epoch 693/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0242 - acc: 0.2381 - val_loss: 858.0919 - val_acc: 0.0000e+00\n",
      "Epoch 694/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7097 - acc: 0.1905 - val_loss: 941.1531 - val_acc: 0.0000e+00\n",
      "Epoch 695/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9429 - acc: 0.1429 - val_loss: 683.2000 - val_acc: 0.0000e+00\n",
      "Epoch 696/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4757 - acc: 0.0952 - val_loss: 790.2964 - val_acc: 0.0000e+00\n",
      "Epoch 697/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1008 - acc: 0.1429 - val_loss: 689.1484 - val_acc: 0.0000e+00\n",
      "Epoch 698/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7870 - acc: 0.1905 - val_loss: 780.2139 - val_acc: 0.0000e+00\n",
      "Epoch 699/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8052 - acc: 0.2381 - val_loss: 902.5998 - val_acc: 0.0000e+00\n",
      "Epoch 700/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4184 - acc: 0.2857 - val_loss: 923.9411 - val_acc: 0.0000e+00\n",
      "Epoch 701/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.4135 - acc: 0.1429 - val_loss: 650.1489 - val_acc: 0.0000e+00\n",
      "Epoch 702/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.8696 - acc: 0.1429 - val_loss: 939.6282 - val_acc: 0.0000e+00\n",
      "Epoch 703/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.0794 - acc: 0.1429 - val_loss: 535.7354 - val_acc: 0.0000e+00\n",
      "Epoch 704/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.6699 - acc: 0.1429 - val_loss: 650.3834 - val_acc: 0.0000e+00\n",
      "Epoch 705/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.0570 - acc: 0.3333 - val_loss: 855.6011 - val_acc: 0.0000e+00\n",
      "Epoch 706/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5811 - acc: 0.0952 - val_loss: 599.1503 - val_acc: 0.0000e+00\n",
      "Epoch 707/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.0633 - acc: 0.1429 - val_loss: 765.7978 - val_acc: 0.0000e+00\n",
      "Epoch 708/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.6318 - acc: 0.2857 - val_loss: 749.3124 - val_acc: 0.0000e+00\n",
      "Epoch 709/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8902 - acc: 0.1429 - val_loss: 818.0552 - val_acc: 0.0000e+00\n",
      "Epoch 710/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6067 - acc: 0.0952 - val_loss: 560.3293 - val_acc: 0.0000e+00\n",
      "Epoch 711/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7276 - acc: 0.1905 - val_loss: 803.2123 - val_acc: 0.0000e+00\n",
      "Epoch 712/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4842 - acc: 0.0476 - val_loss: 692.2317 - val_acc: 0.0000e+00\n",
      "Epoch 713/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9851 - acc: 0.1429 - val_loss: 569.7486 - val_acc: 0.0000e+00\n",
      "Epoch 714/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.5065 - acc: 0.0952 - val_loss: 858.4820 - val_acc: 0.0000e+00\n",
      "Epoch 715/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.1250 - acc: 0.0952 - val_loss: 733.3453 - val_acc: 0.0000e+00\n",
      "Epoch 716/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.2694 - acc: 0.1905 - val_loss: 843.4123 - val_acc: 0.0000e+00\n",
      "Epoch 717/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.9265 - acc: 0.1905 - val_loss: 574.5596 - val_acc: 0.0000e+00\n",
      "Epoch 718/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.8617 - acc: 0.0952 - val_loss: 775.0037 - val_acc: 0.0000e+00\n",
      "Epoch 719/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4010 - acc: 0.1905 - val_loss: 663.0166 - val_acc: 0.0000e+00\n",
      "Epoch 720/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7618 - acc: 0.1905 - val_loss: 745.1336 - val_acc: 0.0000e+00\n",
      "Epoch 721/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.9649 - acc: 0.1905 - val_loss: 598.8393 - val_acc: 0.0000e+00\n",
      "Epoch 722/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8300 - acc: 0.0952 - val_loss: 846.6050 - val_acc: 0.0000e+00\n",
      "Epoch 723/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5688 - acc: 0.0952 - val_loss: 651.2365 - val_acc: 0.0000e+00\n",
      "Epoch 724/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7840 - acc: 0.2381 - val_loss: 766.7407 - val_acc: 0.0000e+00\n",
      "Epoch 725/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9076 - acc: 0.2857 - val_loss: 691.8335 - val_acc: 0.0000e+00\n",
      "Epoch 726/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5694 - acc: 0.1429 - val_loss: 810.5447 - val_acc: 0.0000e+00\n",
      "Epoch 727/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7351 - acc: 0.1429 - val_loss: 699.1824 - val_acc: 0.0000e+00\n",
      "Epoch 728/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9219 - acc: 0.1429 - val_loss: 795.0995 - val_acc: 0.0000e+00\n",
      "Epoch 729/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.8093 - acc: 0.1429 - val_loss: 1027.2018 - val_acc: 0.0000e+00\n",
      "Epoch 730/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.2790 - acc: 0.1905 - val_loss: 661.8873 - val_acc: 0.0000e+00\n",
      "Epoch 731/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.7334 - acc: 0.1429 - val_loss: 688.5135 - val_acc: 0.0000e+00\n",
      "Epoch 732/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.9932 - acc: 0.2857 - val_loss: 893.4579 - val_acc: 0.0000e+00\n",
      "Epoch 733/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1782 - acc: 0.0952 - val_loss: 710.9035 - val_acc: 0.0000e+00\n",
      "Epoch 734/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.8077 - acc: 0.3333 - val_loss: 802.7822 - val_acc: 0.0000e+00\n",
      "Epoch 735/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3117 - acc: 0.1429 - val_loss: 764.7149 - val_acc: 0.0000e+00\n",
      "Epoch 736/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9666 - acc: 0.1905 - val_loss: 869.6682 - val_acc: 0.0000e+00\n",
      "Epoch 737/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1443 - acc: 0.1429 - val_loss: 789.4603 - val_acc: 0.0000e+00\n",
      "Epoch 738/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0085 - acc: 0.0952 - val_loss: 895.7332 - val_acc: 0.0000e+00\n",
      "Epoch 739/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0210 - acc: 0.0952 - val_loss: 665.2452 - val_acc: 0.0000e+00\n",
      "Epoch 740/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9491 - acc: 0.1905 - val_loss: 747.1384 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.5399 - acc: 0.1905 - val_loss: 788.5436 - val_acc: 0.0000e+00\n",
      "Epoch 742/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.2454 - acc: 0.1905 - val_loss: 780.4966 - val_acc: 0.0000e+00\n",
      "Epoch 743/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.4672 - acc: 0.0000e+00 - val_loss: 752.5256 - val_acc: 0.0000e+00\n",
      "Epoch 744/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2718 - acc: 0.0952 - val_loss: 607.2582 - val_acc: 0.0000e+00\n",
      "Epoch 745/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9169 - acc: 0.1429 - val_loss: 636.8013 - val_acc: 0.0000e+00\n",
      "Epoch 746/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2801 - acc: 0.3333 - val_loss: 718.1904 - val_acc: 0.0000e+00\n",
      "Epoch 747/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2542 - acc: 0.0952 - val_loss: 693.8076 - val_acc: 0.0000e+00\n",
      "Epoch 748/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.8098 - acc: 0.2857 - val_loss: 845.0257 - val_acc: 0.0000e+00\n",
      "Epoch 749/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7468 - acc: 0.1429 - val_loss: 688.1418 - val_acc: 0.0000e+00\n",
      "Epoch 750/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7266 - acc: 0.1429 - val_loss: 875.5083 - val_acc: 0.0000e+00\n",
      "Epoch 751/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4702 - acc: 0.1905 - val_loss: 912.6639 - val_acc: 0.0000e+00\n",
      "Epoch 752/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1125 - acc: 0.0952 - val_loss: 785.1581 - val_acc: 0.0000e+00\n",
      "Epoch 753/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7259 - acc: 0.1905 - val_loss: 665.1423 - val_acc: 0.0000e+00\n",
      "Epoch 754/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.4413 - acc: 0.1905 - val_loss: 711.5357 - val_acc: 0.0000e+00\n",
      "Epoch 755/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6143 - acc: 0.2381 - val_loss: 852.1967 - val_acc: 0.0000e+00\n",
      "Epoch 756/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4635 - acc: 0.1429 - val_loss: 765.1807 - val_acc: 0.0000e+00\n",
      "Epoch 757/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9738 - acc: 0.1905 - val_loss: 904.8255 - val_acc: 0.0000e+00\n",
      "Epoch 758/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.4714 - acc: 0.2857 - val_loss: 810.0789 - val_acc: 0.0000e+00\n",
      "Epoch 759/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2927 - acc: 0.1429 - val_loss: 694.8273 - val_acc: 0.0000e+00\n",
      "Epoch 760/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0787 - acc: 0.2857 - val_loss: 810.8513 - val_acc: 0.0000e+00\n",
      "Epoch 761/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8905 - acc: 0.2381 - val_loss: 925.0499 - val_acc: 0.0000e+00\n",
      "Epoch 762/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6243 - acc: 0.1905 - val_loss: 745.3277 - val_acc: 0.0000e+00\n",
      "Epoch 763/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7311 - acc: 0.2381 - val_loss: 690.5656 - val_acc: 0.0000e+00\n",
      "Epoch 764/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.8905 - acc: 0.2381 - val_loss: 701.7990 - val_acc: 0.0000e+00\n",
      "Epoch 765/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7285 - acc: 0.0952 - val_loss: 877.1423 - val_acc: 0.0000e+00\n",
      "Epoch 766/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8975 - acc: 0.3333 - val_loss: 623.7724 - val_acc: 0.0000e+00\n",
      "Epoch 767/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2147 - acc: 0.0000e+00 - val_loss: 767.2458 - val_acc: 0.0000e+00\n",
      "Epoch 768/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.5762 - acc: 0.1429 - val_loss: 891.0913 - val_acc: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.9322 - acc: 0.1905 - val_loss: 506.4608 - val_acc: 0.0000e+00\n",
      "Epoch 770/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.3483 - acc: 0.0000e+00 - val_loss: 986.5020 - val_acc: 0.0000e+00\n",
      "Epoch 771/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.4880 - acc: 0.0000e+00 - val_loss: 867.0974 - val_acc: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7813 - acc: 0.2381 - val_loss: 624.3982 - val_acc: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 37.6024 - acc: 0.1429 - val_loss: 1045.2611 - val_acc: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.2590 - acc: 0.0952 - val_loss: 701.1848 - val_acc: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9040 - acc: 0.1905 - val_loss: 875.3702 - val_acc: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.9065 - acc: 0.2857 - val_loss: 562.1458 - val_acc: 0.0000e+00\n",
      "Epoch 777/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.9219 - acc: 0.1429 - val_loss: 824.5305 - val_acc: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6831 - acc: 0.0476 - val_loss: 650.6727 - val_acc: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.9199 - acc: 0.1429 - val_loss: 984.4691 - val_acc: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.5252 - acc: 0.1429 - val_loss: 560.6962 - val_acc: 0.0000e+00\n",
      "Epoch 781/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.6722 - acc: 0.1429 - val_loss: 765.2841 - val_acc: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7361 - acc: 0.0952 - val_loss: 766.3220 - val_acc: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0504 - acc: 0.1429 - val_loss: 852.6868 - val_acc: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.9208 - acc: 0.2381 - val_loss: 640.3654 - val_acc: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 18.5275 - acc: 0.0952 - val_loss: 841.8794 - val_acc: 0.0000e+00\n",
      "Epoch 786/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3935 - acc: 0.0952 - val_loss: 729.7825 - val_acc: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.2665 - acc: 0.1429 - val_loss: 469.7928 - val_acc: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.4395 - acc: 0.0952 - val_loss: 794.0466 - val_acc: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5840 - acc: 0.1905 - val_loss: 940.8408 - val_acc: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.7212 - acc: 0.1429 - val_loss: 515.6845 - val_acc: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.2259 - acc: 0.1429 - val_loss: 779.0108 - val_acc: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.5058 - acc: 0.1905 - val_loss: 601.0348 - val_acc: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.0060 - acc: 0.0476 - val_loss: 934.2617 - val_acc: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7372 - acc: 0.1429 - val_loss: 814.8864 - val_acc: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 26.3545 - acc: 0.0952 - val_loss: 553.7151 - val_acc: 0.3333\n",
      "Epoch 796/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.0259 - acc: 0.0952 - val_loss: 753.3351 - val_acc: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9542 - acc: 0.2857 - val_loss: 732.5783 - val_acc: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1140 - acc: 0.1429 - val_loss: 817.5313 - val_acc: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.5459 - acc: 0.1429 - val_loss: 745.8262 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3407 - acc: 0.1429 - val_loss: 626.6334 - val_acc: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5757 - acc: 0.1429 - val_loss: 780.7079 - val_acc: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7215 - acc: 0.1429 - val_loss: 702.8904 - val_acc: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.9246 - acc: 0.0952 - val_loss: 731.1157 - val_acc: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.0256 - acc: 0.2857 - val_loss: 960.0567 - val_acc: 0.0000e+00\n",
      "Epoch 805/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 36.5383 - acc: 0.0476 - val_loss: 430.9035 - val_acc: 0.0000e+00\n",
      "Epoch 806/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 30.0219 - acc: 0.0476 - val_loss: 754.9868 - val_acc: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1378 - acc: 0.0952 - val_loss: 755.0696 - val_acc: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1089 - acc: 0.0952 - val_loss: 702.9028 - val_acc: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7245 - acc: 0.1429 - val_loss: 789.8481 - val_acc: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.3858 - acc: 0.0952 - val_loss: 585.5421 - val_acc: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.9840 - acc: 0.0476 - val_loss: 813.2689 - val_acc: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.3824 - acc: 0.3810 - val_loss: 565.6867 - val_acc: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.0438 - acc: 0.1905 - val_loss: 708.3210 - val_acc: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9375 - acc: 0.1905 - val_loss: 655.4603 - val_acc: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3968 - acc: 0.1429 - val_loss: 651.9346 - val_acc: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5854 - acc: 0.0476 - val_loss: 693.2405 - val_acc: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5094 - acc: 0.2381 - val_loss: 886.6837 - val_acc: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8928 - acc: 0.1429 - val_loss: 652.2509 - val_acc: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6734 - acc: 0.1429 - val_loss: 656.0273 - val_acc: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8808 - acc: 0.1429 - val_loss: 540.4767 - val_acc: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.0675 - acc: 0.1905 - val_loss: 677.1336 - val_acc: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.4807 - acc: 0.1905 - val_loss: 750.0247 - val_acc: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7899 - acc: 0.1429 - val_loss: 687.6577 - val_acc: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1696 - acc: 0.1429 - val_loss: 746.2418 - val_acc: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5365 - acc: 0.0952 - val_loss: 586.5887 - val_acc: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.7837 - acc: 0.1905 - val_loss: 787.3355 - val_acc: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3627 - acc: 0.1905 - val_loss: 600.7704 - val_acc: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4160 - acc: 0.1429 - val_loss: 837.6063 - val_acc: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7395 - acc: 0.2857 - val_loss: 637.0898 - val_acc: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.2558 - acc: 0.1429 - val_loss: 657.1840 - val_acc: 0.0000e+00\n",
      "Epoch 831/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.1079 - acc: 0.2857 - val_loss: 682.0582 - val_acc: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7416 - acc: 0.1905 - val_loss: 666.5319 - val_acc: 0.0000e+00\n",
      "Epoch 833/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3438 - acc: 0.1429 - val_loss: 669.3770 - val_acc: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4518 - acc: 0.4286 - val_loss: 754.2993 - val_acc: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.0069 - acc: 0.1905 - val_loss: 719.8184 - val_acc: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1072 - acc: 0.2857 - val_loss: 727.2693 - val_acc: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2909 - acc: 0.2381 - val_loss: 561.3318 - val_acc: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.2878 - acc: 0.0476 - val_loss: 797.5464 - val_acc: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0944 - acc: 0.2381 - val_loss: 673.1822 - val_acc: 0.0000e+00\n",
      "Epoch 840/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.6864 - acc: 0.1905 - val_loss: 577.9578 - val_acc: 0.0000e+00\n",
      "Epoch 841/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.0932 - acc: 0.2381 - val_loss: 638.2914 - val_acc: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4209 - acc: 0.2857 - val_loss: 855.0266 - val_acc: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.9429 - acc: 0.1905 - val_loss: 680.2742 - val_acc: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6100 - acc: 0.0476 - val_loss: 574.9035 - val_acc: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4454 - acc: 0.0952 - val_loss: 672.5040 - val_acc: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.8870 - acc: 0.2381 - val_loss: 668.9446 - val_acc: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1165 - acc: 0.2381 - val_loss: 697.1562 - val_acc: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.5449 - acc: 0.1905 - val_loss: 664.7983 - val_acc: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4696 - acc: 0.0952 - val_loss: 564.7122 - val_acc: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2983 - acc: 0.2381 - val_loss: 768.0996 - val_acc: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1409 - acc: 0.2381 - val_loss: 764.9490 - val_acc: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1192 - acc: 0.1905 - val_loss: 584.0746 - val_acc: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6017 - acc: 0.1429 - val_loss: 628.8735 - val_acc: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7405 - acc: 0.1429 - val_loss: 694.2026 - val_acc: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6951 - acc: 0.1905 - val_loss: 872.5312 - val_acc: 0.0000e+00\n",
      "Epoch 856/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.0514 - acc: 0.0952 - val_loss: 603.1381 - val_acc: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.3406 - acc: 0.1429 - val_loss: 720.9879 - val_acc: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9820 - acc: 0.2381 - val_loss: 741.3366 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 859/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0380 - acc: 0.2381 - val_loss: 762.6860 - val_acc: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.0055 - acc: 0.1905 - val_loss: 530.0685 - val_acc: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0584 - acc: 0.1905 - val_loss: 685.9787 - val_acc: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6959 - acc: 0.1905 - val_loss: 799.9545 - val_acc: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1989 - acc: 0.1429 - val_loss: 631.2228 - val_acc: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6850 - acc: 0.3333 - val_loss: 795.5505 - val_acc: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.6570 - acc: 0.2381 - val_loss: 893.4119 - val_acc: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0888 - acc: 0.1905 - val_loss: 783.5751 - val_acc: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3026 - acc: 0.0952 - val_loss: 824.8443 - val_acc: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5808 - acc: 0.0952 - val_loss: 739.3940 - val_acc: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.0893 - acc: 0.2381 - val_loss: 788.3025 - val_acc: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.4110 - acc: 0.1429 - val_loss: 828.1245 - val_acc: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.9808 - acc: 0.1429 - val_loss: 830.5388 - val_acc: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.8479 - acc: 0.2857 - val_loss: 921.2444 - val_acc: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.0197 - acc: 0.1905 - val_loss: 729.6415 - val_acc: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9596 - acc: 0.1429 - val_loss: 784.2690 - val_acc: 0.0000e+00\n",
      "Epoch 875/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6118 - acc: 0.1905 - val_loss: 692.4973 - val_acc: 0.0000e+00\n",
      "Epoch 876/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6726 - acc: 0.0476 - val_loss: 700.3945 - val_acc: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3743 - acc: 0.3810 - val_loss: 867.7557 - val_acc: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.1800 - acc: 0.1429 - val_loss: 839.6808 - val_acc: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.5234 - acc: 0.1429 - val_loss: 915.5854 - val_acc: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.1297 - acc: 0.1905 - val_loss: 806.0244 - val_acc: 0.0000e+00\n",
      "Epoch 881/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0151 - acc: 0.2381 - val_loss: 690.0796 - val_acc: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8069 - acc: 0.0952 - val_loss: 926.6568 - val_acc: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1886 - acc: 0.0952 - val_loss: 613.5347 - val_acc: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.0379 - acc: 0.0952 - val_loss: 793.5305 - val_acc: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4844 - acc: 0.0476 - val_loss: 668.3658 - val_acc: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.3655 - acc: 0.1905 - val_loss: 981.7464 - val_acc: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.3169 - acc: 0.1905 - val_loss: 927.5276 - val_acc: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.1313 - acc: 0.0952 - val_loss: 783.5461 - val_acc: 0.0000e+00\n",
      "Epoch 889/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3957 - acc: 0.0952 - val_loss: 691.6756 - val_acc: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7973 - acc: 0.0952 - val_loss: 780.4715 - val_acc: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4755 - acc: 0.2857 - val_loss: 623.7970 - val_acc: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.3538 - acc: 0.0952 - val_loss: 904.9229 - val_acc: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.2589 - acc: 0.1429 - val_loss: 624.9216 - val_acc: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.5956 - acc: 0.1429 - val_loss: 925.7388 - val_acc: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.0005 - acc: 0.1429 - val_loss: 572.7270 - val_acc: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.8861 - acc: 0.1905 - val_loss: 795.7985 - val_acc: 0.0000e+00\n",
      "Epoch 897/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.2065 - acc: 0.3810 - val_loss: 867.6121 - val_acc: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.8694 - acc: 0.3810 - val_loss: 554.7932 - val_acc: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8027 - acc: 0.0952 - val_loss: 775.6223 - val_acc: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7325 - acc: 0.3333 - val_loss: 712.6954 - val_acc: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.0142 - acc: 0.3810 - val_loss: 718.7912 - val_acc: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.0661 - acc: 0.1905 - val_loss: 756.5568 - val_acc: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3238 - acc: 0.0476 - val_loss: 529.2582 - val_acc: 0.6667\n",
      "Epoch 904/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.4979 - acc: 0.0952 - val_loss: 825.4240 - val_acc: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7208 - acc: 0.1905 - val_loss: 689.4825 - val_acc: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.8887 - acc: 0.2381 - val_loss: 870.4822 - val_acc: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2209 - acc: 0.0952 - val_loss: 641.5320 - val_acc: 0.3333\n",
      "Epoch 908/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.9563 - acc: 0.0952 - val_loss: 851.2625 - val_acc: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5484 - acc: 0.3333 - val_loss: 589.5090 - val_acc: 0.3333\n",
      "Epoch 910/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.4286 - acc: 0.0476 - val_loss: 790.2510 - val_acc: 0.0000e+00\n",
      "Epoch 911/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9363 - acc: 0.2857 - val_loss: 746.9895 - val_acc: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4101 - acc: 0.3333 - val_loss: 880.0186 - val_acc: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8200 - acc: 0.2381 - val_loss: 682.2499 - val_acc: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.8464 - acc: 0.2381 - val_loss: 826.8168 - val_acc: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 32.7799 - acc: 0.1905 - val_loss: 378.1140 - val_acc: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 41.9754 - acc: 0.0952 - val_loss: 692.0462 - val_acc: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.6926 - acc: 0.1429 - val_loss: 651.3146 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 918/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1313 - acc: 0.3333 - val_loss: 634.7611 - val_acc: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.3886 - acc: 0.1429 - val_loss: 685.2521 - val_acc: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.9997 - acc: 0.1905 - val_loss: 904.2219 - val_acc: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5102 - acc: 0.0476 - val_loss: 638.4544 - val_acc: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8520 - acc: 0.0952 - val_loss: 844.4738 - val_acc: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.1754 - acc: 0.1905 - val_loss: 904.3862 - val_acc: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3025 - acc: 0.2381 - val_loss: 748.4518 - val_acc: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.5270 - acc: 0.1905 - val_loss: 614.6260 - val_acc: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.6744 - acc: 0.1905 - val_loss: 793.7464 - val_acc: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4561 - acc: 0.3333 - val_loss: 821.4157 - val_acc: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 6.3595 - acc: 0.2857 - val_loss: 769.7523 - val_acc: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7115 - acc: 0.2381 - val_loss: 656.9117 - val_acc: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.8472 - acc: 0.2857 - val_loss: 659.6298 - val_acc: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.4210 - acc: 0.0476 - val_loss: 786.0151 - val_acc: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.5343 - acc: 0.0952 - val_loss: 578.6118 - val_acc: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.9208 - acc: 0.1429 - val_loss: 772.1392 - val_acc: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8109 - acc: 0.1905 - val_loss: 678.0098 - val_acc: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.1054 - acc: 0.1905 - val_loss: 777.3926 - val_acc: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1199 - acc: 0.2381 - val_loss: 652.2740 - val_acc: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.9386 - acc: 0.1429 - val_loss: 740.1404 - val_acc: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1236 - acc: 0.1905 - val_loss: 613.7214 - val_acc: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.7474 - acc: 0.0952 - val_loss: 643.9434 - val_acc: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0470 - acc: 0.1429 - val_loss: 643.1910 - val_acc: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.6347 - acc: 0.0952 - val_loss: 813.8862 - val_acc: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5042 - acc: 0.0952 - val_loss: 585.8901 - val_acc: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.2979 - acc: 0.1905 - val_loss: 851.4595 - val_acc: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.9763 - acc: 0.2381 - val_loss: 570.8290 - val_acc: 0.0000e+00\n",
      "Epoch 945/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.2131 - acc: 0.0952 - val_loss: 720.2842 - val_acc: 0.0000e+00\n",
      "Epoch 946/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.0732 - acc: 0.2381 - val_loss: 607.9308 - val_acc: 0.3333\n",
      "Epoch 947/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.9470 - acc: 0.2381 - val_loss: 617.8809 - val_acc: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.1122 - acc: 0.1429 - val_loss: 669.6736 - val_acc: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.8290 - acc: 0.2381 - val_loss: 724.4978 - val_acc: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.8977 - acc: 0.3333 - val_loss: 693.0761 - val_acc: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.1814 - acc: 0.2857 - val_loss: 646.2104 - val_acc: 0.3333\n",
      "Epoch 952/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.1653 - acc: 0.1905 - val_loss: 592.3611 - val_acc: 0.3333\n",
      "Epoch 953/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.2648 - acc: 0.1429 - val_loss: 647.3668 - val_acc: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.7263 - acc: 0.2381 - val_loss: 606.4825 - val_acc: 0.3333\n",
      "Epoch 955/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.4787 - acc: 0.2381 - val_loss: 793.4827 - val_acc: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.6963 - acc: 0.0952 - val_loss: 722.8194 - val_acc: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.9565 - acc: 0.2381 - val_loss: 696.7175 - val_acc: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3979 - acc: 0.1429 - val_loss: 700.9590 - val_acc: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5100 - acc: 0.0952 - val_loss: 750.7463 - val_acc: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.9953 - acc: 0.1905 - val_loss: 795.9511 - val_acc: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.5524 - acc: 0.1429 - val_loss: 723.7256 - val_acc: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.2255 - acc: 0.1905 - val_loss: 623.5903 - val_acc: 0.3333\n",
      "Epoch 963/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.9835 - acc: 0.1429 - val_loss: 687.0662 - val_acc: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.9688 - acc: 0.0952 - val_loss: 734.5124 - val_acc: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9401 - acc: 0.1429 - val_loss: 910.3619 - val_acc: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.9278 - acc: 0.0476 - val_loss: 542.9825 - val_acc: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 25.9244 - acc: 0.2857 - val_loss: 761.6687 - val_acc: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1696 - acc: 0.1429 - val_loss: 587.4109 - val_acc: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2620 - acc: 0.1905 - val_loss: 582.7701 - val_acc: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.9797 - acc: 0.0952 - val_loss: 544.6257 - val_acc: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6166 - acc: 0.2857 - val_loss: 640.3405 - val_acc: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7415 - acc: 0.2381 - val_loss: 538.1418 - val_acc: 0.3333\n",
      "Epoch 973/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0142 - acc: 0.1429 - val_loss: 607.2669 - val_acc: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.7581 - acc: 0.1905 - val_loss: 591.1727 - val_acc: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8504 - acc: 0.2857 - val_loss: 492.0485 - val_acc: 0.3333\n",
      "Epoch 976/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.5392 - acc: 0.0952 - val_loss: 607.6284 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 977/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.1206 - acc: 0.1429 - val_loss: 624.2366 - val_acc: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.2245 - acc: 0.0952 - val_loss: 530.5480 - val_acc: 0.3333\n",
      "Epoch 979/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.5561 - acc: 0.1429 - val_loss: 684.6159 - val_acc: 0.0000e+00\n",
      "Epoch 980/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3473 - acc: 0.1905 - val_loss: 436.0782 - val_acc: 0.0000e+00\n",
      "Epoch 981/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 23.2880 - acc: 0.0476 - val_loss: 629.9961 - val_acc: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.1042 - acc: 0.2381 - val_loss: 496.2595 - val_acc: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8166 - acc: 0.1905 - val_loss: 589.8367 - val_acc: 0.0000e+00\n",
      "Epoch 984/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5024 - acc: 0.2381 - val_loss: 588.8537 - val_acc: 0.3333\n",
      "Epoch 985/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4598 - acc: 0.1429 - val_loss: 570.7508 - val_acc: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0413 - acc: 0.0952 - val_loss: 587.6273 - val_acc: 0.3333\n",
      "Epoch 987/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6901 - acc: 0.2857 - val_loss: 536.7039 - val_acc: 0.3333\n",
      "Epoch 988/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 4.5878 - acc: 0.1429 - val_loss: 531.5095 - val_acc: 0.3333\n",
      "Epoch 989/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.3653 - acc: 0.2381 - val_loss: 509.7400 - val_acc: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 3.9364 - acc: 0.2857 - val_loss: 590.7851 - val_acc: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0034 - acc: 0.0952 - val_loss: 495.1484 - val_acc: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.7806 - acc: 0.0476 - val_loss: 624.8475 - val_acc: 0.0000e+00\n",
      "Epoch 993/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.7551 - acc: 0.1429 - val_loss: 604.4864 - val_acc: 0.0000e+00\n",
      "Epoch 994/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.6676 - acc: 0.2381 - val_loss: 625.8455 - val_acc: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.5473 - acc: 0.2857 - val_loss: 408.8964 - val_acc: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.8882 - acc: 0.1429 - val_loss: 694.9653 - val_acc: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.1524 - acc: 0.0000e+00 - val_loss: 446.1622 - val_acc: 0.3333\n",
      "Epoch 998/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 28.6920 - acc: 0.0952 - val_loss: 691.5782 - val_acc: 0.0000e+00\n",
      "Epoch 999/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.5158 - acc: 0.0952 - val_loss: 497.9209 - val_acc: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.2292 - acc: 0.0476 - val_loss: 648.0316 - val_acc: 0.0000e+00\n",
      "Train Score: 83.60 MSE (9.14 RMSE)\n",
      "Test Score: 151.99 MSE (12.33 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 52.000000 ---> Previsão: 34.912109 Diff: 17.087891 Racio: 0.489455\n",
      "valor: 65.000000 ---> Previsão: 47.480595 Diff: 17.519405 Racio: 0.368980\n",
      "valor: 17.000000 ---> Previsão: 12.079090 Diff: 4.920910 Racio: 0.407391\n",
      "valor: 5.000000 ---> Previsão: 16.966026 Diff: 11.966026 Racio: -0.705293\n",
      "valor: 17.000000 ---> Previsão: 22.683250 Diff: 5.683250 Racio: -0.250548\n",
      "valor: 1.000000 ---> Previsão: 11.646164 Diff: 10.646164 Racio: -0.914135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK6WUn/h/x8OiyAboQmAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbd57cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = read_and_pre_process()\n",
    "    print(df)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, janela)# o df[::-1] é o df por ordem inversa\n",
    "\n",
    "    #max_review_length = 10\n",
    "    #x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "    #x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    \n",
    "    nmr_parametros = X_train.shape[2]\n",
    "    \n",
    "    model = build_model2(janela, nmr_parametros)\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=1000, validation_split=0.1, verbose=1)\n",
    "    \n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    \n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    \n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #transformar uma matriz de uma coluna e n linhas em um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "    o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-3daa58ce4126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Score: %.2f MSE (%.2f RMSE)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>pub</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>20.5</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>15.5</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>05</td>\n",
       "      <td>15.3</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>06</td>\n",
       "      <td>23.5</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>07</td>\n",
       "      <td>24.5</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>08</td>\n",
       "      <td>21.3</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>09</td>\n",
       "      <td>23.5</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>28.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>01</td>\n",
       "      <td>17.3</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>02</td>\n",
       "      <td>25.3</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>03</td>\n",
       "      <td>25.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>04</td>\n",
       "      <td>36.5</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>05</td>\n",
       "      <td>36.5</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>06</td>\n",
       "      <td>29.6</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>07</td>\n",
       "      <td>30.5</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>08</td>\n",
       "      <td>28.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>09</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>21.5</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>19.7</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>01</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>02</td>\n",
       "      <td>20.7</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>03</td>\n",
       "      <td>26.5</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>04</td>\n",
       "      <td>30.6</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>05</td>\n",
       "      <td>32.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>06</td>\n",
       "      <td>29.5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>07</td>\n",
       "      <td>28.3</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>08</td>\n",
       "      <td>31.3</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>09</td>\n",
       "      <td>32.2</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>26.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>23.4</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year month   pub  sales\n",
       "0     1    01  12.0   15.0\n",
       "1     1    02  20.5   16.0\n",
       "2     1    03  21.0   18.0\n",
       "3     1    04  15.5   27.0\n",
       "4     1    05  15.3   21.0\n",
       "5     1    06  23.5   49.0\n",
       "6     1    07  24.5   21.0\n",
       "7     1    08  21.3   22.0\n",
       "8     1    09  23.5   28.0\n",
       "9     1    10  28.0   36.0\n",
       "10    1    11  24.0   40.0\n",
       "11    1    12  15.5    3.0\n",
       "12    2    01  17.3   21.0\n",
       "13    2    02  25.3   29.0\n",
       "14    2    03  25.0   62.0\n",
       "15    2    04  36.5   65.0\n",
       "16    2    05  36.5   46.0\n",
       "17    2    06  29.6   44.0\n",
       "18    2    07  30.5   33.0\n",
       "19    2    08  28.0   62.0\n",
       "20    2    09  26.0   22.0\n",
       "21    2    10  21.5   12.0\n",
       "22    2    11  19.7   24.0\n",
       "23    2    12  19.0    3.0\n",
       "24    3    01  16.0    5.0\n",
       "25    3    02  20.7   14.0\n",
       "26    3    03  26.5   36.0\n",
       "27    3    04  30.6   40.0\n",
       "28    3    05  32.3   49.0\n",
       "29    3    06  29.5    7.0\n",
       "30    3    07  28.3   52.0\n",
       "31    3    08  31.3   65.0\n",
       "32    3    09  32.2   17.0\n",
       "33    3    10  26.4    5.0\n",
       "34    3    11  23.4   17.0\n",
       "35    3    12  16.4    1.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_atributos = len(df_dados.columns)\n",
    "mat_dados = df_dados.as_matrix()\n",
    "tam_sequencia = janela + 1\n",
    "res = []\n",
    "for i in range(len(mat_dados) - janela): #numero de registos - tamanho da sequencia\n",
    "    res.append(mat_dados[i: i + tam_sequencia])\n",
    "res = np.array(res)\n",
    "\n",
    "qt_casos_treino = 24 # 2 anos\n",
    "    \n",
    "x_train = res[:qt_casos_treino, :]\n",
    "    \n",
    "#x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "y_train = x_train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "x_test = res[qt_casos_treino:, :-1]\n",
    "y_test = x_test[:, -1][:,-1]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix + AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(validation_generator.classes, predicted_classes))\n",
    "\n",
    "# AUC for prediction on validation sample\n",
    "X_val_sample, val_labels = next(validation_generator)\n",
    "val_pred = model.predict_proba(X_val_sample)\n",
    "val_pred = np.reshape(val_pred, val_labels.shape)\n",
    "val_score_auc = roc_auc_score(val_labels, val_pred)\n",
    "print (\"AUC validation score: \",val_score_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
